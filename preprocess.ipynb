{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './../../../Seq2Seq/data_triple/'\n",
    "# files = os.listdir(path)\n",
    "# files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "# print(len(files_file))\n",
    "# print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自分へのリプライを削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "100041 91623 OK\n",
      "472551 429123 OK\n",
      "80607 73764 OK\n",
      "137673 122841 OK\n",
      "89361 80364 OK\n",
      "63621 55362 OK\n",
      "1626 1440 OK\n",
      "201612 181905 OK\n",
      "252069 226596 OK\n",
      "153867 138603 OK\n",
      "30582 28095 OK\n",
      "11307 10260 OK\n",
      "178443 161754 OK\n",
      "155967 139176 OK\n",
      "63324 57927 OK\n",
      "475503 420939 OK\n",
      "358191 321888 OK\n",
      "6297 5802 OK\n",
      "342648 301740 OK\n",
      "642 591 OK\n",
      "181878 164145 OK\n",
      "197790 178221 OK\n",
      "1221525 1095972 OK\n",
      "485319 435846 OK\n",
      "666 612 OK\n",
      "29880 26415 OK\n",
      "145695 131466 OK\n",
      "321912 286350 OK\n",
      "4539 4182 OK\n",
      "7086 6486 OK\n",
      "125820 110484 OK\n",
      "27417 25215 OK\n",
      "501 459 OK\n",
      "153498 136344 OK\n",
      "369786 333318 OK\n",
      "89106 80493 OK\n",
      "723 609 OK\n",
      "226785 202686 OK\n",
      "8931 8250 OK\n",
      "753135 671082 OK\n",
      "220404 196647 OK\n",
      "168375 152082 OK\n",
      "315801 282135 OK\n",
      "164727 149301 OK\n",
      "205836 185415 OK\n",
      "182184 162402 OK\n",
      "237204 209418 OK\n",
      "406305 367878 OK\n",
      "31923 29094 OK\n",
      "715464 647670 OK\n",
      "479919 429831 OK\n",
      "237609 212772 OK\n",
      "430791 392112 OK\n",
      "12984 11703 OK\n",
      "174153 156657 OK\n",
      "4215 3864 OK\n",
      "36750 33585 OK\n",
      "219504 195861 OK\n",
      "180027 160284 OK\n",
      "1524 1365 OK\n",
      "101718 91641 OK\n",
      "400230 354717 OK\n",
      "126093 111585 OK\n",
      "204510 186561 OK\n",
      "506487 450285 OK\n",
      "129489 113382 OK\n",
      "527442 468546 OK\n",
      "41322 37425 OK\n",
      "18903 16998 OK\n",
      "15477 13932 OK\n",
      "16089 13509 OK\n",
      "62580 57102 OK\n",
      "108954 99402 OK\n",
      "23133 21243 OK\n",
      "1572 1434 OK\n",
      "61950 55929 OK\n",
      "577290 523047 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/data_triple/'#元のフォルダーの位置\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    processed = delete_self_reply_pairs(rawdata)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "#     save_file = open('./data/deleted_self_reply/'+files_file[i],'w')\n",
    "#     save_file.writelines(processed)\n",
    "#     save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正規化してカッコの入った文章を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "55362 19134 OK\n",
      "80364 28953 OK\n",
      "122841 42915 OK\n",
      "73764 25686 OK\n",
      "91623 33810 OK\n",
      "429123 150849 OK\n",
      "161754 55797 OK\n",
      "10260 3351 OK\n",
      "139176 46290 OK\n",
      "57927 19527 OK\n",
      "28095 9909 OK\n",
      "138603 48549 OK\n",
      "226596 76050 OK\n",
      "1440 444 OK\n",
      "181905 63255 OK\n",
      "178221 63060 OK\n",
      "164145 59949 OK\n",
      "1095972 372810 OK\n",
      "5802 2022 OK\n",
      "591 195 OK\n",
      "301740 107808 OK\n",
      "420939 143565 OK\n",
      "321888 109662 OK\n",
      "136344 47079 OK\n",
      "333318 119688 OK\n",
      "286350 98880 OK\n",
      "459 156 OK\n",
      "6486 2400 OK\n",
      "4182 1440 OK\n",
      "25215 9303 OK\n",
      "110484 37593 OK\n",
      "26415 8715 OK\n",
      "131466 48324 OK\n",
      "612 180 OK\n",
      "435846 149082 OK\n",
      "212772 74175 OK\n",
      "209418 74454 OK\n",
      "367878 132039 OK\n",
      "647670 231441 OK\n",
      "429831 147210 OK\n",
      "29094 9906 OK\n",
      "282135 94695 OK\n",
      "152082 54519 OK\n",
      "149301 52854 OK\n",
      "185415 67020 OK\n",
      "162402 54558 OK\n",
      "202686 70848 OK\n",
      "8250 2667 OK\n",
      "80493 29811 OK\n",
      "609 240 OK\n",
      "196647 65715 OK\n",
      "671082 228462 OK\n",
      "160284 54480 OK\n",
      "1365 459 OK\n",
      "91641 31653 OK\n",
      "33585 11247 OK\n",
      "195861 70659 OK\n",
      "156657 53391 OK\n",
      "3864 1311 OK\n",
      "392112 132588 OK\n",
      "11703 4017 OK\n",
      "113382 40170 OK\n",
      "450285 150681 OK\n",
      "37425 13080 OK\n",
      "468546 166500 OK\n",
      "186561 63354 OK\n",
      "354717 124764 OK\n",
      "111585 36771 OK\n",
      "523047 183945 OK\n",
      "55929 20004 OK\n",
      "99402 36291 OK\n",
      "21243 6924 OK\n",
      "57102 19650 OK\n",
      "1434 489 OK\n",
      "13932 4986 OK\n",
      "16998 5742 OK\n",
      "13509 4734 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_self_reply/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    processed = normalize(rawdata)\n",
    "    processed = delete_brackets(processed)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "\n",
    "#     save_file = open('./data/deleted_brackets/'+files_file[i],'w')\n",
    "#     save_file.writelines(processed)\n",
    "#     save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# user_nameを削除する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "94695 94695 OK\n",
      "54519 54519 OK\n",
      "52854 52854 OK\n",
      "67020 67020 OK\n",
      "54558 54558 OK\n",
      "70848 70848 OK\n",
      "2667 2667 OK\n",
      "29811 29811 OK\n",
      "240 240 OK\n",
      "65715 65715 OK\n",
      "228462 228462 OK\n",
      "74175 74175 OK\n",
      "132039 132039 OK\n",
      "74454 74454 OK\n",
      "9906 9906 OK\n",
      "147210 147210 OK\n",
      "231441 231441 OK\n",
      "53391 53391 OK\n",
      "1311 1311 OK\n",
      "132588 132588 OK\n",
      "4017 4017 OK\n",
      "54480 54480 OK\n",
      "459 459 OK\n",
      "31653 31653 OK\n",
      "70659 70659 OK\n",
      "11247 11247 OK\n",
      "63354 63354 OK\n",
      "124764 124764 OK\n",
      "36771 36771 OK\n",
      "150681 150681 OK\n",
      "40170 40170 OK\n",
      "13080 13080 OK\n",
      "166500 166500 OK\n",
      "4986 4986 OK\n",
      "5742 5742 OK\n",
      "4734 4734 OK\n",
      "20004 20004 OK\n",
      "183945 183945 OK\n",
      "36291 36291 OK\n",
      "6924 6924 OK\n",
      "19650 19650 OK\n",
      "489 489 OK\n",
      "25686 25686 OK\n",
      "42915 42915 OK\n",
      "33810 33810 OK\n",
      "150849 150849 OK\n",
      "19134 19134 OK\n",
      "28953 28953 OK\n",
      "48549 48549 OK\n",
      "76050 76050 OK\n",
      "63255 63255 OK\n",
      "444 444 OK\n",
      "3351 3351 OK\n",
      "55797 55797 OK\n",
      "46290 46290 OK\n",
      "19527 19527 OK\n",
      "9909 9909 OK\n",
      "2022 2022 OK\n",
      "107808 107808 OK\n",
      "195 195 OK\n",
      "143565 143565 OK\n",
      "109662 109662 OK\n",
      "63060 63060 OK\n",
      "59949 59949 OK\n",
      "372810 372810 OK\n",
      "8715 8715 OK\n",
      "48324 48324 OK\n",
      "149082 149082 OK\n",
      "180 180 OK\n",
      "47079 47079 OK\n",
      "119688 119688 OK\n",
      "98880 98880 OK\n",
      "156 156 OK\n",
      "1440 1440 OK\n",
      "2400 2400 OK\n",
      "9303 9303 OK\n",
      "37593 37593 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_brackets/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    \n",
    "    processed = delete_inline_username(rawdata)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0 and len(rawdata)==len(processed)):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    save_file = open('./data/deleted_usrname/'+files_file[i],'w')\n",
    "    save_file.writelines(processed)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 絵文字を削除する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "107808 107808 OK\n",
      "195 195 OK\n",
      "2022 2022 OK\n",
      "109662 109662 OK\n",
      "143565 143565 OK\n",
      "372810 372810 OK\n",
      "63060 63060 OK\n",
      "59949 59949 OK\n",
      "48324 48324 OK\n",
      "8715 8715 OK\n",
      "149082 149082 OK\n",
      "180 180 OK\n",
      "119688 119688 OK\n",
      "47079 47079 OK\n",
      "156 156 OK\n",
      "1440 1440 OK\n",
      "2400 2400 OK\n",
      "37593 37593 OK\n",
      "9303 9303 OK\n",
      "98880 98880 OK\n",
      "25686 25686 OK\n",
      "42915 42915 OK\n",
      "150849 150849 OK\n",
      "33810 33810 OK\n",
      "19134 19134 OK\n",
      "28953 28953 OK\n",
      "48549 48549 OK\n",
      "76050 76050 OK\n",
      "63255 63255 OK\n",
      "444 444 OK\n",
      "19527 19527 OK\n",
      "3351 3351 OK\n",
      "55797 55797 OK\n",
      "46290 46290 OK\n",
      "9909 9909 OK\n",
      "63354 63354 OK\n",
      "124764 124764 OK\n",
      "36771 36771 OK\n",
      "13080 13080 OK\n",
      "166500 166500 OK\n",
      "150681 150681 OK\n",
      "40170 40170 OK\n",
      "4734 4734 OK\n",
      "4986 4986 OK\n",
      "5742 5742 OK\n",
      "20004 20004 OK\n",
      "183945 183945 OK\n",
      "489 489 OK\n",
      "36291 36291 OK\n",
      "6924 6924 OK\n",
      "19650 19650 OK\n",
      "67020 67020 OK\n",
      "52854 52854 OK\n",
      "54558 54558 OK\n",
      "94695 94695 OK\n",
      "54519 54519 OK\n",
      "65715 65715 OK\n",
      "228462 228462 OK\n",
      "70848 70848 OK\n",
      "2667 2667 OK\n",
      "29811 29811 OK\n",
      "240 240 OK\n",
      "74175 74175 OK\n",
      "9906 9906 OK\n",
      "147210 147210 OK\n",
      "231441 231441 OK\n",
      "132039 132039 OK\n",
      "74454 74454 OK\n",
      "1311 1311 OK\n",
      "53391 53391 OK\n",
      "132588 132588 OK\n",
      "4017 4017 OK\n",
      "31653 31653 OK\n",
      "54480 54480 OK\n",
      "459 459 OK\n",
      "70659 70659 OK\n",
      "11247 11247 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_usrname/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    \n",
    "    processed = delete_emoji(rawdata)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0 and len(rawdata)==len(processed)):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    save_file = open('./data/deleted_emoji/'+files_file[i],'w')\n",
    "    save_file.writelines(processed)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLを削除する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "4734 4734 OK\n",
      "4986 4986 OK\n",
      "5742 5742 OK\n",
      "20004 20004 OK\n",
      "183945 183945 OK\n",
      "489 489 OK\n",
      "6924 6924 OK\n",
      "36291 36291 OK\n",
      "19650 19650 OK\n",
      "63354 63354 OK\n",
      "36771 36771 OK\n",
      "124764 124764 OK\n",
      "13080 13080 OK\n",
      "166500 166500 OK\n",
      "40170 40170 OK\n",
      "150681 150681 OK\n",
      "1311 1311 OK\n",
      "53391 53391 OK\n",
      "4017 4017 OK\n",
      "132588 132588 OK\n",
      "31653 31653 OK\n",
      "459 459 OK\n",
      "54480 54480 OK\n",
      "11247 11247 OK\n",
      "70659 70659 OK\n",
      "54558 54558 OK\n",
      "52854 52854 OK\n",
      "67020 67020 OK\n",
      "94695 94695 OK\n",
      "54519 54519 OK\n",
      "65715 65715 OK\n",
      "228462 228462 OK\n",
      "2667 2667 OK\n",
      "70848 70848 OK\n",
      "240 240 OK\n",
      "29811 29811 OK\n",
      "74175 74175 OK\n",
      "231441 231441 OK\n",
      "147210 147210 OK\n",
      "9906 9906 OK\n",
      "74454 74454 OK\n",
      "132039 132039 OK\n",
      "48324 48324 OK\n",
      "8715 8715 OK\n",
      "180 180 OK\n",
      "149082 149082 OK\n",
      "119688 119688 OK\n",
      "47079 47079 OK\n",
      "156 156 OK\n",
      "37593 37593 OK\n",
      "9303 9303 OK\n",
      "2400 2400 OK\n",
      "1440 1440 OK\n",
      "98880 98880 OK\n",
      "195 195 OK\n",
      "107808 107808 OK\n",
      "2022 2022 OK\n",
      "109662 109662 OK\n",
      "143565 143565 OK\n",
      "372810 372810 OK\n",
      "63060 63060 OK\n",
      "59949 59949 OK\n",
      "48549 48549 OK\n",
      "76050 76050 OK\n",
      "444 444 OK\n",
      "63255 63255 OK\n",
      "19527 19527 OK\n",
      "46290 46290 OK\n",
      "55797 55797 OK\n",
      "3351 3351 OK\n",
      "9909 9909 OK\n",
      "42915 42915 OK\n",
      "25686 25686 OK\n",
      "150849 150849 OK\n",
      "33810 33810 OK\n",
      "19134 19134 OK\n",
      "28953 28953 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_emoji/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    \n",
    "    processed = delete_url(rawdata)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0 and len(rawdata)==len(processed)):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    save_file = open('./data/deleted_url/'+files_file[i],'w')\n",
    "    save_file.writelines(processed)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 連続した文字、記号を削除する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "48549 44808 OK\n",
      "76050 70386 OK\n",
      "63255 58398 OK\n",
      "444 411 OK\n",
      "19527 18060 OK\n",
      "46290 42744 OK\n",
      "3351 3108 OK\n",
      "55797 51432 OK\n",
      "9909 9081 OK\n",
      "25686 23706 OK\n",
      "42915 39573 OK\n",
      "150849 138696 OK\n",
      "33810 31131 OK\n",
      "19134 17718 OK\n",
      "28953 26499 OK\n",
      "48324 44490 OK\n",
      "8715 8115 OK\n",
      "149082 138024 OK\n",
      "180 165 OK\n",
      "119688 110736 OK\n",
      "47079 43413 OK\n",
      "156 144 OK\n",
      "9303 8634 OK\n",
      "37593 34611 OK\n",
      "1440 1323 OK\n",
      "2400 2241 OK\n",
      "98880 91245 OK\n",
      "107808 99819 OK\n",
      "195 183 OK\n",
      "2022 1866 OK\n",
      "109662 101280 OK\n",
      "143565 132609 OK\n",
      "372810 344523 OK\n",
      "63060 57942 OK\n",
      "59949 55023 OK\n",
      "1311 1230 OK\n",
      "53391 49347 OK\n",
      "4017 3693 OK\n",
      "132588 122232 OK\n",
      "31653 29421 OK\n",
      "459 423 OK\n",
      "54480 50238 OK\n",
      "70659 64989 OK\n",
      "11247 10371 OK\n",
      "54558 50376 OK\n",
      "67020 61872 OK\n",
      "52854 48882 OK\n",
      "94695 87618 OK\n",
      "54519 50277 OK\n",
      "65715 60762 OK\n",
      "228462 211365 OK\n",
      "2667 2481 OK\n",
      "70848 65709 OK\n",
      "240 225 OK\n",
      "29811 27411 OK\n",
      "74175 68421 OK\n",
      "9906 9138 OK\n",
      "147210 135885 OK\n",
      "231441 212670 OK\n",
      "132039 121770 OK\n",
      "74454 69006 OK\n",
      "4734 4371 OK\n",
      "4986 4695 OK\n",
      "5742 5355 OK\n",
      "183945 170139 OK\n",
      "20004 18336 OK\n",
      "489 438 OK\n",
      "6924 6393 OK\n",
      "36291 33360 OK\n",
      "19650 18054 OK\n",
      "63354 58608 OK\n",
      "36771 34044 OK\n",
      "124764 115467 OK\n",
      "13080 12069 OK\n",
      "166500 154041 OK\n",
      "150681 139473 OK\n",
      "40170 36951 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_url/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    \n",
    "    processed = delete_same_char_sequence(rawdata)\n",
    "    processed = delete_same_mark_sequence(processed)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    save_file = open('./data/deleted_char_seq/'+files_file[i],'w')\n",
    "    save_file.writelines(processed)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ハッシュタグを削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "211365 210987 OK\n",
      "60762 60654 OK\n",
      "225 225 OK\n",
      "27411 27390 OK\n",
      "2481 2475 OK\n",
      "65709 65559 OK\n",
      "50376 50292 OK\n",
      "61872 61785 OK\n",
      "48882 48813 OK\n",
      "50277 50199 OK\n",
      "87618 87471 OK\n",
      "135885 135699 OK\n",
      "212670 212295 OK\n",
      "9138 9120 OK\n",
      "121770 121572 OK\n",
      "69006 68907 OK\n",
      "68421 68286 OK\n",
      "3693 3684 OK\n",
      "122232 121941 OK\n",
      "1230 1224 OK\n",
      "49347 49212 OK\n",
      "64989 64878 OK\n",
      "10371 10359 OK\n",
      "29421 29388 OK\n",
      "423 423 OK\n",
      "50238 50154 OK\n",
      "34044 33987 OK\n",
      "115467 115206 OK\n",
      "58608 58467 OK\n",
      "154041 153783 OK\n",
      "12069 12051 OK\n",
      "36951 36909 OK\n",
      "139473 139212 OK\n",
      "4371 4359 OK\n",
      "5355 5346 OK\n",
      "4695 4686 OK\n",
      "438 438 OK\n",
      "18054 18024 OK\n",
      "6393 6375 OK\n",
      "33360 33309 OK\n",
      "18336 18309 OK\n",
      "170139 169860 OK\n",
      "138696 138495 OK\n",
      "31131 31101 OK\n",
      "39573 39519 OK\n",
      "23706 23667 OK\n",
      "26499 26454 OK\n",
      "17718 17673 OK\n",
      "58398 58314 OK\n",
      "411 411 OK\n",
      "70386 70275 OK\n",
      "44808 44712 OK\n",
      "9081 9051 OK\n",
      "18060 18030 OK\n",
      "42744 42684 OK\n",
      "51432 51360 OK\n",
      "3108 3105 OK\n",
      "101280 101049 OK\n",
      "132609 132390 OK\n",
      "183 180 OK\n",
      "99819 99624 OK\n",
      "1866 1863 OK\n",
      "344523 343923 OK\n",
      "55023 54885 OK\n",
      "57942 57858 OK\n",
      "165 165 OK\n",
      "138024 137742 OK\n",
      "44490 44388 OK\n",
      "8115 8097 OK\n",
      "8634 8622 OK\n",
      "34611 34545 OK\n",
      "2241 2241 OK\n",
      "1323 1320 OK\n",
      "144 144 OK\n",
      "91245 91104 OK\n",
      "110736 110553 OK\n",
      "43413 43371 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_char_seq/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    \n",
    "    processed = delete_hashtag(rawdata)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    save_file = open('./data/deleted_hashtag/'+files_file[i],'w')\n",
    "    save_file.writelines(processed)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英単語を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "70275 57828 OK\n",
      "44712 36783 OK\n",
      "58314 48684 OK\n",
      "411 339 OK\n",
      "18030 15054 OK\n",
      "51360 43014 OK\n",
      "3105 2571 OK\n",
      "42684 35304 OK\n",
      "9051 7659 OK\n",
      "39519 32910 OK\n",
      "23667 19950 OK\n",
      "138495 116028 OK\n",
      "31101 26433 OK\n",
      "17673 14526 OK\n",
      "26454 22134 OK\n",
      "44388 37443 OK\n",
      "8097 6852 OK\n",
      "165 129 OK\n",
      "137742 113757 OK\n",
      "110553 93375 OK\n",
      "43371 35955 OK\n",
      "2241 1920 OK\n",
      "1320 1101 OK\n",
      "8622 7350 OK\n",
      "34545 28716 OK\n",
      "144 123 OK\n",
      "91104 75927 OK\n",
      "180 147 OK\n",
      "99624 82347 OK\n",
      "1863 1584 OK\n",
      "101049 83424 OK\n",
      "132390 109455 OK\n",
      "343923 286029 OK\n",
      "54885 46416 OK\n",
      "57858 48324 OK\n",
      "1224 978 OK\n",
      "49212 40692 OK\n",
      "121941 101418 OK\n",
      "3684 2958 OK\n",
      "29388 24402 OK\n",
      "50154 41343 OK\n",
      "423 348 OK\n",
      "64878 54603 OK\n",
      "10359 8670 OK\n",
      "48813 41151 OK\n",
      "61785 52269 OK\n",
      "50292 41712 OK\n",
      "50199 41967 OK\n",
      "87471 72135 OK\n",
      "210987 175677 OK\n",
      "60654 50328 OK\n",
      "27390 23046 OK\n",
      "225 204 OK\n",
      "65559 54318 OK\n",
      "2475 2025 OK\n",
      "68286 57039 OK\n",
      "135699 112614 OK\n",
      "212295 179031 OK\n",
      "9120 7533 OK\n",
      "121572 101832 OK\n",
      "68907 57360 OK\n",
      "4359 3522 OK\n",
      "5346 4410 OK\n",
      "4686 3885 OK\n",
      "18309 15324 OK\n",
      "169860 141144 OK\n",
      "438 372 OK\n",
      "18024 15111 OK\n",
      "33309 28026 OK\n",
      "6375 5274 OK\n",
      "58467 48684 OK\n",
      "115206 96084 OK\n",
      "33987 28089 OK\n",
      "153783 129162 OK\n",
      "12051 9726 OK\n",
      "36909 30516 OK\n",
      "139212 114834 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_hashtag/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    \n",
    "    processed = delete_english_words(rawdata)\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    save_file = open('./data/deleted_english_words/'+files_file[i],'w')\n",
    "    save_file.writelines(processed)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トレード関連、空、日本語英語以外の変なのを削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 files\n",
      "40692 37371 OK\n",
      "978 894 OK\n",
      "2958 2727 OK\n",
      "101418 93405 OK\n",
      "348 309 OK\n",
      "41343 37995 OK\n",
      "24402 22434 OK\n",
      "54603 50583 OK\n",
      "8670 7887 OK\n",
      "41967 38571 OK\n",
      "72135 66024 OK\n",
      "41712 38556 OK\n",
      "52269 48351 OK\n",
      "41151 37878 OK\n",
      "204 183 OK\n",
      "23046 21384 OK\n",
      "2025 1854 OK\n",
      "54318 50064 OK\n",
      "175677 161997 OK\n",
      "50328 46335 OK\n",
      "57039 52677 OK\n",
      "101832 94422 OK\n",
      "57360 52902 OK\n",
      "112614 103950 OK\n",
      "179031 165195 OK\n",
      "7533 6942 OK\n",
      "4410 4056 OK\n",
      "3885 3624 OK\n",
      "3522 3171 OK\n",
      "15324 14121 OK\n",
      "141144 130677 OK\n",
      "15111 13899 OK\n",
      "5274 4851 OK\n",
      "28026 26067 OK\n",
      "372 348 OK\n",
      "48684 44709 OK\n",
      "28089 25794 OK\n",
      "96084 88446 OK\n",
      "30516 28056 OK\n",
      "114834 105591 OK\n",
      "129162 119415 OK\n",
      "9726 9012 OK\n",
      "57828 53292 OK\n",
      "36783 33780 OK\n",
      "48684 44883 OK\n",
      "339 309 OK\n",
      "35304 32379 OK\n",
      "43014 39618 OK\n",
      "2571 2394 OK\n",
      "15054 14055 OK\n",
      "7659 7053 OK\n",
      "32910 30213 OK\n",
      "19950 18471 OK\n",
      "26433 24582 OK\n",
      "116028 107046 OK\n",
      "14526 13194 OK\n",
      "22134 20304 OK\n",
      "6852 6351 OK\n",
      "37443 34659 OK\n",
      "129 129 OK\n",
      "113757 104376 OK\n",
      "35955 32961 OK\n",
      "93375 86385 OK\n",
      "75927 70344 OK\n",
      "28716 26151 OK\n",
      "7350 6792 OK\n",
      "1920 1797 OK\n",
      "1101 999 OK\n",
      "123 120 OK\n",
      "1584 1473 OK\n",
      "147 138 OK\n",
      "82347 76017 OK\n",
      "109455 100704 OK\n",
      "83424 76656 OK\n",
      "46416 42657 OK\n",
      "48324 44439 OK\n",
      "286029 263370 OK\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_english_words/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "print(len(files_file),'files')\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    rawdata = []\n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "\n",
    "    processed = delete_trade_tweets(rawdata)#トレード関連を削除\n",
    "    processed = delete_word_variety(processed)#すごい変な文字が含まれてる文を削除\n",
    "    processed = delete_non_JaEn(processed)#空とか変な文字だけの文を削除\n",
    "    processed = neolog_normalize(processed)#最後に正則化\n",
    "    \n",
    "    \n",
    "    if(len(processed)%3!=0):\n",
    "        print(files_file[i],'NG')\n",
    "    elif(len(processed)%3==0):\n",
    "        print(len(rawdata),len(processed),'OK')\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    save_file = open('./data/deleted_non_word/'+files_file[i],'w')\n",
    "    save_file.writelines(processed)\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280818"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/deleted_non_word/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "rawdata = []\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "len(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_english_words/'\n",
    "rawdata = []\n",
    "data = open(os.path.join(path,files_file[14]),'r',encoding='utf-8')\n",
    "data_lines = data.readlines()\n",
    "rawdata.extend(data_lines)\n",
    "data.close()\n",
    "print(len(rawdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "import emoji\n",
    "import neologdn\n",
    "\n",
    "def normalize(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    for i in range(len(twitter_data)):\n",
    "        s = \"\".join(c for c in twitter_data[i] if unicodedata.category(c) != \"Mn\")\n",
    "        s = unicodedata.normalize(\"NFKC\", s)#NFKDの方がいいかなって思うけどMeCabで分かち書きするならNFKC,NFKDだと濁点が分けられてしまう。\n",
    "        dialogue_tweets.append(s)\n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_self_reply_pairs(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    for i in range(1, len(twitter_data), 3):\n",
    "        if (re.match('@', twitter_data[i])!=None and \n",
    "            re.match('@', twitter_data[i+1])!=None):\n",
    "            dialogue_tweets.append(twitter_data[i-1])\n",
    "            dialogue_tweets.append(twitter_data[i])\n",
    "            dialogue_tweets.append(twitter_data[i+1])\n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_brackets2(twitter_data):###頑張って顔文字を消してる(どっちか)\n",
    "    brackets = []\n",
    "    brackets.append(r'【(.+)】')\n",
    "    brackets.append(r'《(.+)》')\n",
    "    brackets.append(r'\\((.+)\\)')\n",
    "    brackets.append(r'（(.+)）')\n",
    "    dialogue_tweets = []\n",
    "    for ele in twitter_data:\n",
    "        for bracket in brackets:\n",
    "            ele = re.sub(bracket,'',ele)\n",
    "        dialogue_tweets.append(ele)\n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_brackets(twitter_data):###面倒だからカッコは全部消す(どっちか)\n",
    "    #print(len(twitter_data))\n",
    "    brackets = r\"[【】《》()（）\\[\\]「」『』]\"\n",
    "    dialogue_tweets = []\n",
    "    for i in range(0,len(twitter_data),3):\n",
    "        if (re.search(brackets, twitter_data[i])==None and \n",
    "            re.search(brackets, twitter_data[i+1])==None and\n",
    "            re.search(brackets, twitter_data[i+2])==None):\n",
    "            dialogue_tweets.append(twitter_data[i])\n",
    "            dialogue_tweets.append(twitter_data[i+1])\n",
    "            dialogue_tweets.append(twitter_data[i+2])\n",
    "    #print(len(dialogue_tweets))\n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_emoji(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    for ele in twitter_data:\n",
    "        ele_emoji_deleted = ''.join(char for char in ele if char not in emoji.UNICODE_EMOJI)\n",
    "        dialogue_tweets.append(ele_emoji_deleted)\n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_inline_username(twitter_data):\n",
    "    username = r'@([a-zA-Z0-9_…]+)( ?)'\n",
    "    dialogue_tweets = []\n",
    "    for ele in twitter_data:\n",
    "        ele_username_deleted = re.sub(username,'',ele)\n",
    "        dialogue_tweets.append(ele_username_deleted)\n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_url(twitter_data):\n",
    "    url = r'( ?)(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+\\$,%#]+( )*)'\n",
    "    dialogue_tweets = []\n",
    "    for ele in twitter_data:\n",
    "        ele_url_deleted = re.sub(url,'',ele)\n",
    "        dialogue_tweets.append(ele_url_deleted)\n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_same_char_sequence(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    for i in range(0, len(twitter_data), 3):\n",
    "        \n",
    "        if (not re.search(r'[あアぁァ]{2,}(、?)',rawdata[i]) and \n",
    "            not re.search(r'[あアぁァ]{2,}(、?)',rawdata[i+1]) and \n",
    "            not re.search(r'[あアぁァ]{2,}(、?)',rawdata[i+2]) and\n",
    "            not re.search(r'([イぃィ]{2,}|い{3,})(、?)',rawdata[i]) and \n",
    "            not re.search(r'([イぃィ]{2,}|い{3,})(、?)',rawdata[i+1]) and \n",
    "            not re.search(r'([イぃィ]{2,}|い{3,})(、?)',rawdata[i+2]) and\n",
    "            not re.search(r'[うウぅゥ]{2,}(、?)',rawdata[i]) and \n",
    "            not re.search(r'[うウぅゥ]{2,}(、?)',rawdata[i+1]) and \n",
    "            not re.search(r'[うウぅゥ]{2,}(、?)',rawdata[i+2]) and\n",
    "            not re.search(r'[えエぇェ]{2,}(、?)',rawdata[i]) and \n",
    "            not re.search(r'[えエぇェ]{2,}(、?)',rawdata[i+1]) and \n",
    "            not re.search(r'[えエぇェ]{2,}(、?)',rawdata[i+2]) and\n",
    "            not re.search(r'[おオぉォ]{2,}(、?)',rawdata[i]) and \n",
    "            not re.search(r'[おオぉォ]{2,}(、?)',rawdata[i+1]) and \n",
    "            not re.search(r'[おオぉォ]{2,}(、?)',rawdata[i+2]) and\n",
    "            not re.search(r'[@＠]',rawdata[i]) and \n",
    "            not re.search(r'[@＠]',rawdata[i+1]) and \n",
    "            not re.search(r'[@＠]',rawdata[i+2])):\n",
    "            \n",
    "            dialogue_tweets.append(rawdata[i])\n",
    "            dialogue_tweets.append(rawdata[i+1])\n",
    "            dialogue_tweets.append(rawdata[i+2])\n",
    "        \n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_same_mark_sequence(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    for ele in twitter_data:\n",
    "        \n",
    "        deleted_ele = re.sub(r'[ωʬ]','w',ele)\n",
    "        deleted_ele = re.sub(r'ｗ','w',deleted_ele.lower())\n",
    "        deleted_ele = re.sub(r'w{2,}','w',deleted_ele)\n",
    "        deleted_ele = re.sub(r'[\\,\\.。、・]{2,}|…{1,}','',deleted_ele)\n",
    "        deleted_ele = re.sub(r'！','!',deleted_ele)\n",
    "        deleted_ele = re.sub(r'？','\\?',deleted_ele)\n",
    "        deleted_ele = re.sub(r'(!\\?){2,}','!?',deleted_ele)\n",
    "        deleted_ele = re.sub(r'(\\?!){2,}','!?',deleted_ele)\n",
    "        deleted_ele = re.sub(r'!{2,}','!',deleted_ele)\n",
    "        deleted_ele = re.sub(r'\\?{2,}','?',deleted_ele)\n",
    "        deleted_ele = re.sub(r'笑{2,}','笑',deleted_ele)\n",
    "        deleted_ele = re.sub(r'[ー〜ー~−–-➖]{1,}','ー',deleted_ele)\n",
    "        \n",
    "        dialogue_tweets.append(deleted_ele)\n",
    "        \n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_hashtag(twitter_data):\n",
    "    hashtag = r'( ?)[#＃]([\\w一-龠ぁ-んァ-ヴーａ-ｚ]+( |$))'\n",
    "\n",
    "    dialogue_tweets = []\n",
    "    for i in range(0, len(twitter_data), 3):\n",
    "        ele_hashtag_deleted1 = re.sub(hashtag,'',twitter_data[i])\n",
    "        ele_hashtag_deleted2 = re.sub(hashtag,'',twitter_data[i+1])\n",
    "        ele_hashtag_deleted3 = re.sub(hashtag,'',twitter_data[i+2])\n",
    "        \n",
    "        if (not re.search(r'[#＃]', ele_hashtag_deleted1) and\n",
    "            not re.search(r'[#＃]', ele_hashtag_deleted2) and\n",
    "            not re.search(r'[#＃]', ele_hashtag_deleted3)):\n",
    "            \n",
    "            dialogue_tweets.append(ele_hashtag_deleted1)\n",
    "            dialogue_tweets.append(ele_hashtag_deleted2)\n",
    "            dialogue_tweets.append(ele_hashtag_deleted3)\n",
    "            \n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_english_words(twitter_data):\n",
    "    eng_word = r'[a-zA-Z]{2,}'\n",
    "    dialogue_tweets = []\n",
    "    for i in range(0, len(twitter_data), 3):\n",
    "        if (not re.search(eng_word, twitter_data[i]) and\n",
    "            not re.search(eng_word, twitter_data[i+1]) and\n",
    "            not re.search(eng_word, twitter_data[i+2])):\n",
    "            \n",
    "            dialogue_tweets.append(twitter_data[i])\n",
    "            dialogue_tweets.append(twitter_data[i+1])\n",
    "            dialogue_tweets.append(twitter_data[i+2])\n",
    "            \n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_trade_tweets(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    for i in range(0, len(twitter_data), 3):\n",
    "        if (((re.search(r'求',twitter_data[i]) and \n",
    "              re.search(r'出',twitter_data[i]) and\n",
    "              re.search(r'交換',twitter_data[i])) or\n",
    "             (re.search(r'求',twitter_data[i]) and\n",
    "              re.search(r'譲',twitter_data[i]))) \n",
    "            or\n",
    "            ((re.search(r'求',twitter_data[i+1]) and \n",
    "              re.search(r'出',twitter_data[i+1]) and\n",
    "              re.search(r'交換',twitter_data[i+1])) or\n",
    "             (re.search(r'求',twitter_data[i+1]) and\n",
    "              re.search(r'譲',twitter_data[i+1]))) \n",
    "            or\n",
    "            ((re.search(r'求',twitter_data[i+2]) and \n",
    "              re.search(r'出',twitter_data[i+2]) and\n",
    "              re.search(r'交換',twitter_data[i+2])) or\n",
    "             (re.search(r'求',twitter_data[i+2]) and\n",
    "              re.search(r'譲',twitter_data[i+2])))):\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            dialogue_tweets.append(twitter_data[i])\n",
    "            dialogue_tweets.append(twitter_data[i+1])\n",
    "            dialogue_tweets.append(twitter_data[i+2])\n",
    "    \n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_word_variety(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    delete_char=r'[\\u200d./\\^;・\"]'\n",
    "    not_permitted_char = r'[^一-龠々ヶぁ-んァ-ヴーa-zA-Z0-9!?\\n\\s\\t、。:%+&]'\n",
    "    for i in range(0, len(twitter_data), 3):\n",
    "        ele_hashtag_deleted1 = re.sub(delete_char,'',twitter_data[i])\n",
    "        ele_hashtag_deleted2 = re.sub(delete_char,'',twitter_data[i+1])\n",
    "        ele_hashtag_deleted3 = re.sub(delete_char,'',twitter_data[i+2])\n",
    "        \n",
    "        if (not re.search(not_permitted_char, ele_hashtag_deleted1) and\n",
    "            not re.search(not_permitted_char, ele_hashtag_deleted2) and\n",
    "            not re.search(not_permitted_char, ele_hashtag_deleted3)):\n",
    "            \n",
    "            dialogue_tweets.append(ele_hashtag_deleted1)\n",
    "            dialogue_tweets.append(ele_hashtag_deleted2)\n",
    "            dialogue_tweets.append(ele_hashtag_deleted3)\n",
    "            \n",
    "    return dialogue_tweets\n",
    "\n",
    "def delete_non_JaEn(twitter_data):\n",
    "    moji = r'[一-龠ぁ-んァ-ヴーa-z0-9!\\?]'\n",
    "    dialogue_tweets = []\n",
    "\n",
    "    for i in range(0, len(twitter_data), 3):\n",
    "        if (not re.search(moji, twitter_data[i]) or\n",
    "            not re.search(moji, twitter_data[i+1]) or\n",
    "            not re.search(moji, twitter_data[i+2])):\n",
    "            continue\n",
    "        else:\n",
    "            dialogue_tweets.append(twitter_data[i])\n",
    "            dialogue_tweets.append(twitter_data[i+1])\n",
    "            dialogue_tweets.append(twitter_data[i+2])\n",
    "            \n",
    "    return dialogue_tweets\n",
    "\n",
    "def neolog_normalize(twitter_data):\n",
    "    dialogue_tweets = []\n",
    "    for ele in twitter_data:\n",
    "        normalized_ele = neologdn.normalize(ele)\n",
    "        dialogue_tweets.append(normalized_ele)\n",
    "    return dialogue_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3280818\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "path = './data/deleted_non_word/'\n",
    "files = os.listdir(path)\n",
    "files_file = [f for f in files if os.path.isfile(os.path.join(path,f))]\n",
    "rawdata = []\n",
    "for i in range(len(files_file)):\n",
    "    \n",
    "    data = open(os.path.join(path,files_file[i]),'r',encoding='utf-8')\n",
    "    data_lines = data.readlines()\n",
    "    rawdata.extend(data_lines)\n",
    "    data.close()\n",
    "print(len(rawdata))\n",
    "if len(rawdata)%3==0:\n",
    "    print('ok')\n",
    "\n",
    "# save_file = open('./data/processed_ja_file.txt','w')\n",
    "# save_file.writelines(rawdata)\n",
    "# save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3280818\n"
     ]
    }
   ],
   "source": [
    "data = open('./data/processed_ja_file.txt','r',encoding='utf-8')\n",
    "rawdata = data.readlines()\n",
    "data.close()\n",
    "print(len(rawdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spm.SentencePieceTrainer.Train('--input=data/processed_ja_file.txt --model_prefix=data/train_model --vocab_size=32000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=20\n",
    "spp.Load('data/train_model8000.model')\n",
    "print(len(spp.EncodeAsPieces(rawdata[i])),spp.EncodeAsPieces(rawdata[i]))\n",
    "st = ' '.join(spp.EncodeAsPieces(rawdata[i]))\n",
    "print(st,'最後の文字は',st[-1])\n",
    "print()\n",
    "spp.Load('data/train_model.model')\n",
    "print(len(spp.EncodeAsPieces(rawdata[i])),spp.EncodeAsPieces(rawdata[i]))\n",
    "st = ' '.join(spp.EncodeAsPieces(rawdata[i]))\n",
    "print(st,'最後の文字は',st[-1])\n",
    "print()\n",
    "print(len(rawdata[i]),rawdata[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence pieceで区切ったデータファイルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3280818\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "segmentation_model_position = './data'\n",
    "segmentation_model_name = 'train_model32000.model'\n",
    "\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "\n",
    "data = open('./data/processed_ja_file.txt','r',encoding='utf-8')\n",
    "rawdata = data.readlines()\n",
    "data.close()\n",
    "print(len(rawdata))\n",
    "pair_data = []\n",
    "for i in range(0,len(rawdata),3):\n",
    "    strin = ' '.join(spp.EncodeAsPieces(rawdata[i]))\n",
    "    strout = ' '.join(spp.EncodeAsPieces(rawdata[i+1]))\n",
    "    pair_data.append(strin+'\\t'+strout+'\\n')\n",
    "    \n",
    "    strin = ' '.join(spp.EncodeAsPieces(rawdata[i+1]))\n",
    "    strout = ' '.join(spp.EncodeAsPieces(rawdata[i+2]))\n",
    "    pair_data.append(strin+'\\t'+strout+'\\n')\n",
    "\n",
    "save_file = open('./data/'+segmentation_model_name[:-6]+'.txt','w')\n",
    "save_file.writelines(pair_data)\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中居正広の金曜日のスマイルたちへ', 'を', '西野カナ', 'と', '一緒', 'に', '見', 'たい', 'EOS', '']\n",
      "32000 ▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか\n",
      "20000 ▁わたし 安定の た っく んですよー 気づいたら た っ くん いない し 、 い つき あるし 誰 と交換 したのか\n",
      "32000 ▁ 連れ が 誰か 引いて た んですよ 、 きっと それ と交換 したんだ と思うんですけど 誰 だったのか w あ 、 たっくん は しれっと 買取 に 出しました ー\n",
      "20000 ▁ 連れ が 誰か 引いて た んですよ 、 きっと それ と交換 したんだ と思うんですけど 誰 だったのか w あ 、 た っ くんは しれっと 買取 に出 しましたー\n",
      "32000 ▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w\n",
      "20000 ▁今は 古 すぎて 廃 盤 のもの が多い ので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w\n",
      "32000 ▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な\n",
      "20000 ▁クソ じゃん 気にし ねー でいい んじゃね w まあ 俺は ファ イヤー 党 の人 なので もちろん あ の方に 入れた が な\n",
      "32000 ▁ 授業 始まる と同時に ゴム を 生徒 達に 配り 自己紹介 が 下着 の色 だ の t バック だ の の話 しかしない くそ キチガイ な 自称 先生 か 来た 。 きもい 。\n",
      "20000 ▁ 授業 始まる と同時に ゴム を 生徒 達に 配 り 自己紹介 が 下着 の色 だ の t バック だ の の話 しかしない くそ キチガイ な 自称 先生 か 来た 。 きもい 。\n",
      "32000 ▁私も 母性 で やった w 試験 管 に ゴム を 着け る 異 様な 光景 を みた\n",
      "20000 ▁私も 母 性 で やった w 試験 管 に ゴム を 着け る 異 様な 光景 を みた\n",
      "32000 ▁やば w それは じわる w ひろ み も 試験 管 に ゴム つけた の ? w\n",
      "20000 ▁やば w それは じわる w ひろ み も 試験 管 に ゴム つけた の ? w\n",
      "32000 ▁一応 仕事 です w 冒頭 の看板 中 程 見て見て 。 ガラス 展 しとる の よー\n",
      "20000 ▁一応 仕事 です w 冒 頭の 看板 中 程 見て 見て 。 ガラス 展 しとる の よー\n",
      "32000 ▁ 生理 的に 無理な ものは 、 無理 ! 嘘つき 、 嫌い 。 ひと の 善 意 を 前提 にしてる 奴 、 無理 ! 生理 的に w あー 、 この辺 にしとこ\n",
      "20000 ▁ 生理 的に 無理 な ものは 、 無理 ! 嘘つき 、 嫌い 。 ひと の 善 意 を 前提 にしてる 奴 、 無理 ! 生理 的に w あー 、 この辺 に しとこ\n",
      "32000 ▁その キャラ 設定 で 、 ファイナル アン サー ? 笑 そういえば 、 寝起き 画像 はまだ っすか ? 忘れずに 待ってますよ w\n",
      "20000 ▁その キャラ 設定 で 、 ファイナル アン サー ? 笑 そういえば 、 寝起き 画像 はまだ っすか ? 忘れずに 待ってます よ w\n",
      "32000 ▁ ほん っっっ と です w めちゃくちゃ 気 色 悪い そして これを ポイント の かからない 掲示板 で 言う ところ も ケチ くさ くて 気 色 悪い です\n",
      "20000 ▁ ほん っっっ と です w めちゃくちゃ 気 色 悪い そして これを ポイント の かからない 掲 示 板 で 言う ところ も ケチ くさ くて 気 色 悪い です\n"
     ]
    }
   ],
   "source": [
    "st = \"中居正広の金曜日のスマイルたちへを西野カナと一緒に見たい\"\n",
    "\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/\")\n",
    "print([ele.split(\"\\t\")[0] for ele in tagger.parse(st).split(\"\\n\")])\n",
    "\n",
    "# st2 = '苦労みせないのが私の良いとこ 。。。とか言ってみたいʬʬ (≧з≦)ﾌﾟﾌﾟﾌﾟ'\n",
    "# print(tagger.parse(st2))\n",
    "# print([ele.split(\"\\t\")[0] for ele in tagger.parse(st2).split(\"\\n\")])\n",
    "\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "segmentation_model_position = './data'\n",
    "segmentation_model_name = 'train_model32000.model'\n",
    "spp1 = spm.SentencePieceProcessor()\n",
    "spp1.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "\n",
    "segmentation_model_position = './data'\n",
    "segmentation_model_name = 'train_model20000.model'\n",
    "spp2 = spm.SentencePieceProcessor()\n",
    "spp2.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "\n",
    "with open('./data/processed_ja_file.txt','r') as data:\n",
    "    sts = data.readlines()\n",
    "\n",
    "n=0\n",
    "for i in range(len(sts)):\n",
    "    if len(spp2.EncodeAsPieces(sts[i]))>15:\n",
    "        print(32000,' '.join(spp1.EncodeAsPieces(sts[i])))\n",
    "        print(20000,' '.join(spp2.EncodeAsPieces(sts[i])))\n",
    "        n+=1\n",
    "    if n>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mecab neologdで区切ったデータファイルを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3280818\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import MeCab\n",
    "\n",
    "segmentation_model_name = 'mecab_tagged'\n",
    "\n",
    "tagger = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/\")\n",
    "#tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "\n",
    "\n",
    "data = open('./data/processed_ja_file.txt','r',encoding='utf-8')\n",
    "rawdata = data.readlines()\n",
    "data.close()\n",
    "print(len(rawdata))\n",
    "pair_data = []\n",
    "for i in range(0,len(rawdata),3):#\n",
    "    strin = ' '.join([ele.split(\"\\t\")[0] for ele in tagger.parse(rawdata[i]).split(\"\\n\")[:-2]])\n",
    "    strout = ' '.join([ele.split(\"\\t\")[0] for ele in tagger.parse(rawdata[i+1]).split(\"\\n\")[:-2]])\n",
    "    pair_data.append(strin+'\\t'+strout+'\\n')\n",
    "    \n",
    "    strin = ' '.join([ele.split(\"\\t\")[0] for ele in tagger.parse(rawdata[i+1]).split(\"\\n\")[:-2]])\n",
    "    strout = ' '.join([ele.split(\"\\t\")[0] for ele in tagger.parse(rawdata[i+2]).split(\"\\n\")[:-2]])\n",
    "    pair_data.append(strin+'\\t'+strout+'\\n')\n",
    "\n",
    "#print(pair_data)\n",
    "save_file = open('./data/'+segmentation_model_name+'.txt','w')\n",
    "save_file.writelines(pair_data)\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['たっくん', '名詞,固有名詞,人名,一般,*,*,たっくん,タックン,タックン']\n",
      "['たっくん', '名詞,固有名詞,人名,一般,*,*,たっくん,タックン,タックン']\n",
      "わたし安定のたっくんですよー気づいたらたっくんいないし、いつきあるし誰と交換したのかEOS\n",
      "わたし安定の[名前]ですよー気づいたら[名前]いないし、いつきあるし誰と交換したのかEOS\n",
      "\n",
      "['たっくん', '名詞,固有名詞,人名,一般,*,*,たっくん,タックン,タックン']\n",
      "連れが誰か引いてたんですよ、きっとそれと交換したんだと思うんですけど誰だったのかwあ、たっくんはしれっと買取に出しましたーEOS\n",
      "連れが誰か引いてたんですよ、きっとそれと交換したんだと思うんですけど誰だったのかwあ、[名前]はしれっと買取に出しましたーEOS\n",
      "\n",
      "['ひろみ', '名詞,固有名詞,人名,名,*,*,ひろみ,ヒロミ,ヒロミ']\n",
      "やばwそれはじわるwひろみも試験管にゴムつけたの?wEOS\n",
      "やばwそれはじわるw[名前]も試験管にゴムつけたの?wEOS\n",
      "\n",
      "ほー笑EOS\n",
      "ほー笑EOS\n",
      "\n",
      "['智也', '名詞,固有名詞,人名,名,*,*,智也,トモヤ,トモヤ']\n",
      "仕事だよー今日は休み取ってたけど智也さん準優乗ってないから行ってないしEOS\n",
      "仕事だよー今日は休み取ってたけど[名前]さん準優乗ってないから行ってないしEOS\n",
      "\n",
      "['あらら', '名詞,固有名詞,人名,一般,*,*,あらら,アララ,アララ']\n",
      "['ゆな', '名詞,固有名詞,人名,名,*,*,ゆな,ユナ,ユナ']\n",
      "あららん今日行ってたらゆなちゃん儲かってたねwEOS\n",
      "[名前]ん今日行ってたら[名前]ちゃん儲かってたねwEOS\n",
      "\n",
      "['みつき', '名詞,固有名詞,人名,一般,*,*,みつき,ミツキ,ミツキ']\n",
      "みつきさんいつも氷結飲んでません?wEOS\n",
      "[名前]さんいつも氷結飲んでません?wEOS\n",
      "\n",
      "['まいちゃん', '名詞,固有名詞,人名,一般,*,*,まいちゃん,マイチャン,マイチャン']\n",
      "まいちゃんとカンパイするために仕方なくEOS\n",
      "[名前]とカンパイするために仕方なくEOS\n",
      "\n",
      "['フエゴ', '名詞,固有名詞,人名,一般,*,*,フエゴ,フエゴ,フエゴ']\n",
      "ほれ!フエゴ殿どうした!?妾にゆうてまいれ!EOS\n",
      "ほれ![名前]殿どうした!?妾にゆうてまいれ!EOS\n",
      "\n",
      "そのキャラ設定で、ファイナルアンサー?笑そういえば、寝起き画像はまだっすか?忘れずに待ってますよwEOS\n",
      "そのキャラ設定で、ファイナルアンサー?笑そういえば、寝起き画像はまだっすか?忘れずに待ってますよwEOS\n",
      "\n",
      "['二宮', '名詞,固有名詞,人名,姓,*,*,二宮,ニノミヤ,ニノミヤ']\n",
      "何故だろう拭いきれぬ二宮みwEOS\n",
      "何故だろう拭いきれぬ[名前]みwEOS\n",
      "\n",
      "狙ってんのか天然なのかwEOS\n",
      "狙ってんのか天然なのかwEOS\n",
      "\n",
      "じゃあ辞めれば良いのにねw気色悪いですねー笑EOS\n",
      "じゃあ辞めれば良いのにねw気色悪いですねー笑EOS\n",
      "\n",
      "['なほ', '名詞,固有名詞,人名,名,*,*,なほ,なほ,ナホ']\n",
      "なほちゃん殺すよEOS\n",
      "[名前]ちゃん殺すよEOS\n",
      "\n",
      "['な!', '名詞,固有名詞,人名,一般,*,*,な!,ナ,ナ']\n",
      "はいな!初めてです2枚申し込んどく?やばそうwEOS\n",
      "はい[名前]初めてです2枚申し込んどく?やばそうwEOS\n",
      "\n",
      "['安田', '名詞,固有名詞,人名,姓,*,*,安田,ヤスダ,ヤスダ']\n",
      "['ピコ太郎', '名詞,固有名詞,人名,一般,*,*,ピコ太郎,ピコタロウ,ピコタロー']\n",
      "いやそれなwまいことも言ってて安田はピコ太郎って言ってる爆笑EOS\n",
      "いやそれなwまいことも言ってて[名前]は[名前]って言ってる爆笑EOS\n",
      "\n",
      "['よね', '名詞,固有名詞,人名,名,*,*,よね,ヨネ,ヨネ']\n",
      "首回り紐垂れてたよねw貴乃花親方かよってEOS\n",
      "首回り紐垂れてた[名前]w貴乃花親方かよってEOS\n",
      "\n",
      "['みく', '名詞,固有名詞,人名,名,*,*,みく,ミク,ミク']\n",
      "もえ限定とかなんかみくが照れるやんwEOS\n",
      "もえ限定とかなんか[名前]が照れるやんwEOS\n",
      "\n",
      "['宮川', '名詞,固有名詞,人名,姓,*,*,宮川,ミヤガワ,ミヤガワ']\n",
      "ちゃんと宮川症って書いとかなきゃ不審に思われるよ?wEOS\n",
      "ちゃんと[名前]症って書いとかなきゃ不審に思われるよ?wEOS\n",
      "\n",
      "['ピカチュウ', '名詞,固有名詞,人名,一般,*,*,ピカチュウ,ピカチュウ,ピカチュー']\n",
      "ピカチュウをドブネズミにさすなやEOS\n",
      "[名前]をドブネズミにさすなやEOS\n",
      "\n",
      "['ツイ', '名詞,固有名詞,人名,姓,*,*,ツイ,ツイ,ツイ']\n",
      "元ツイ見て描かずにはいられなかったEOS\n",
      "元[名前]見て描かずにはいられなかったEOS\n",
      "\n",
      "['矢吹奈子', '名詞,固有名詞,人名,一般,*,*,矢吹奈子,ヤブキナコ,ヤブキナコ']\n",
      "矢吹奈子ちゃんってこんなに小さいの!?かわいい!EOS\n",
      "[名前]ちゃんってこんなに小さいの!?かわいい!EOS\n",
      "\n",
      "['平手', '名詞,固有名詞,人名,姓,*,*,平手,ヒラテ,ヒラテ']\n",
      "平手さんEOS\n",
      "[名前]さんEOS\n",
      "\n",
      "['平手', '名詞,固有名詞,人名,姓,*,*,平手,ヒラテ,ヒラテ']\n",
      "['愛', '名詞,固有名詞,人名,名,*,*,愛,アイ,アイ']\n",
      "あかねの平手愛すごい。っていつも思ってますw00減ってきちゃってるけど消えないで!wEOS\n",
      "あかねの[名前][名前]すごい。っていつも思ってますw00減ってきちゃってるけど消えないで!wEOS\n",
      "\n",
      "何年か振りに見たな笑EOS\n",
      "何年か振りに見たな笑EOS\n",
      "\n",
      "['銀屋', '名詞,固有名詞,人名,姓,*,*,銀屋,ギンヤ,ギンヤ']\n",
      "よくわかったなw銀屋だよwEOS\n",
      "よくわかったなw[名前]だよwEOS\n",
      "\n",
      "['かもしれん', '名詞,固有名詞,人名,一般,*,*,かもしれん,カモシレン,カモシレン']\n",
      "たぶん違うかもしれんEOS\n",
      "たぶん違う[名前]EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = open('./data/processed_ja_file.txt','r',encoding='utf-8')\n",
    "bun = data.readlines()\n",
    "data.close()\n",
    "for i in range(200):\n",
    "    if '固有名詞,人名' in tagger.parse(bun[i]):\n",
    "        a = []\n",
    "        b = []\n",
    "        #print(tagger.parse(bun[i]))\n",
    "        for wordele in tagger.parse(bun[i]).split(\"\\n\"):\n",
    "            info = wordele.split(\"\\t\")\n",
    "            a.append(info[0])\n",
    "            b.append(info[0])\n",
    "            if len(info)>1:\n",
    "                if '固有名詞,人名' in info[1] and \"笑\"!=info[0] and \"なのか\"!=info[0]:\n",
    "                    print(info)\n",
    "                    b.pop()\n",
    "                    b.append(\"[名前]\")\n",
    "        print(\"\".join(a))\n",
    "        print(\"\".join(b))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train_model32000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"train_model32000.txt\"\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "MAX_LENGTH = 40  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return \"\".join(#########  NFKC, NFKD, NFC, NFDの中から選ぶ、多分NFKCか NFKDがいいと思う。あとでNFD!=NFKDになるような文全部削除しよう\n",
    "        c for c in unicodedata.normalize(\"NFKC\", s) if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) ####ここはあとで調べないとわからん\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)     ###.!?にスペースを入れる\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) ##日本語ならここを変更\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()   #スペースの連続を一つにする\n",
    "    return s\n",
    "\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and len(p[1].split(\" \")) < MAX_LENGTH\n",
    "\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name):#, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    pairs = readVocs(corpus, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "#     for pair in pairs:\n",
    "#         voc.addSentence(pair[0])\n",
    "#         voc.addSentence(pair[1])\n",
    "#     print(\"Counted words:\", voc.num_words)\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2187212"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = open(corpus, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2187212\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "nn=0\n",
    "for l in lines:\n",
    "    lsp = l.split('\\t')\n",
    "    nn+=1\n",
    "    for s in lsp:\n",
    "        ns = normalizeString(s)\n",
    "        if s != ns:\n",
    "            print(s)\n",
    "            print(ns)\n",
    "            n+=1\n",
    "    if n>50:\n",
    "        break\n",
    "print(nn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train_model32000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"train_model32000.txt\"\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for i in range(len(lines[:n])):\n",
    "        print(lines[i].split('\\t'),i)\n",
    "        \n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = len(self.index2word)  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print(\n",
    "            \"keep_words {} / {} = {:.4f}\".format(\n",
    "                len(keep_words),\n",
    "                len(self.word2index),\n",
    "                len(keep_words) / len(self.word2index),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "            \n",
    "            \n",
    "    # Remove words below a certain count threshold\n",
    "    def shortTrim(self, min_length):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for i in range(3,len(self.index2word),1):\n",
    "            if len(self.index2word[i]) >= min_length:\n",
    "                keep_words.append(self.index2word[i])\n",
    "\n",
    "        print(\n",
    "            \"keep_words {} / {} = {:.4f}\".format(\n",
    "                len(keep_words),\n",
    "                len(self.word2index),\n",
    "                len(keep_words) / len(self.word2index),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1834337 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 34296\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい']\n",
      "['▁ おち つけ よ', '▁みんな は げろ']\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい']\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真']\n",
      "['▁ は ?', '▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "MAX_LENGTH = 20  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split(\"\\t\")] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and len(p[1].split(\" \")) < MAX_LENGTH\n",
    "\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name):#, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(corpus, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 31995 / 34293 = 0.9330\n",
      "Trimmed from 1834337 pairs to 1822478, 0.9935 of total\n",
      "Counted words in voc 31998\n",
      "Counted words in voc2: 31998\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 28  # Minimum word count threshold for trimming\n",
    "\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\n",
    "        \"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
    "            len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
    "        )\n",
    "    )\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "voc2 = Voc(corpus_name)\n",
    "for pair in pairs:\n",
    "    voc2.addSentence(pair[0])\n",
    "    voc2.addSentence(pair[1])\n",
    "print(\"Counted words in voc\", voc.num_words)\n",
    "#vocはコーパスに対して十分な語彙の辞書、コーパスに無い単語も含まれてる。\n",
    "#(vocを作成後、出現頻度の低い単語を削除し、削除された単語を含む文章をコーパスから削除したため。)\n",
    "print(\"Counted words in voc2:\", voc2.num_words)   \n",
    "#voc2はコーパスに対して必要十分な語彙の辞書、コーパス無いの単語全てを含んでいて、辞書内の単語は必ずコーパスに登場する。\n",
    "#(vocのように削除された単語を含む文章をコーパスから削除した後、その新しいコーパスから作成したのがvoc2だから。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 同僚申請\n",
      "1 被災地\n",
      "2 存じます\n",
      "3 ますと幸いです\n",
      "4 夜分遅く\n",
      "5 検索よりお声かけ\n",
      "6 避難所\n",
      "7 话绝对\n",
      "8 ご検討の程\n",
      "9 ご提供\n",
      "10 沖縄県\n",
      "11 お引き取り\n",
      "12 マスコミ\n",
      "13 どうぞ宜しくお願い致します\n",
      "14 郵送にて\n",
      "15 お取引の為\n",
      "16 呵呵呵呵\n",
      "17 嘖嘖嘖嘖\n",
      "18 可能でございます\n",
      "19 当方所持の\n",
      "20 嬉しく思います\n",
      "21 ご入用\n",
      "22 下さりありがとうございます\n",
      "23 労働者\n",
      "24 難しいとは思いますが\n",
      "25 朝鮮人\n",
      "26 厭蛞蝓\n",
      "27 夜分遅くに失礼致します\n",
      "28 ご相談\n",
      "29 ならびに\n",
      "30 喵喵喵喵喵\n",
      "31 ご検討のほど\n",
      "32 再度ご検討\n",
      "33 朝から夜まで\n",
      "34 添付させていただきます\n",
      "35 賈詡賈詡賈詡\n",
      "36 ご交換\n",
      "37 安倍政権\n",
      "38 お返事が遅くなり\n",
      "39 么么哒\n",
      "40 郵送での交換\n",
      "41 ご返信ありがとうございます\n",
      "42 ユダヤ\n",
      "43 ご希望に添え\n",
      "44 交換して頂くことは可能でしょうか\n",
      "45 迪丽热\n",
      "46 轻飘飘\n",
      "47 添付させて頂きます\n",
      "48 嘻嘻嘻\n",
      "49 大変嬉しく思います\n",
      "50 させていただきたい\n",
      "51 プライベ\n",
      "52 奈央が\n",
      "53 自治体\n",
      "54 検索よりお声掛け\n",
      "55 画像をお借り\n",
      "56 慰安婦\n",
      "57 珊瑚礁\n",
      "58 被災者\n",
      "59 坐什么\n",
      "60 駱駝鈕\n",
      "61 御検討\n",
      "62 検索よりお声がけ\n",
      "63 冰淇淋\n",
      "64 繼續當醫\n",
      "65 検索より失礼いたします\n",
      "66 都内手渡し\n",
      "67 已经很\n",
      "68 嘿嘿嘿\n",
      "69 诶嘿嘿黑\n",
      "70 出先のため\n",
      "71 绫罗绸缎\n",
      "72 據說餐廳\n",
      "73 国会議員\n",
      "74 终结为\n",
      "75 懒做跑\n",
      "76 徠驪驤\n",
      "77 俄杂种\n",
      "78 码农节\n",
      "79 啊们啊们啊们\n",
      "80 ジャーナリスト\n",
      "81 お返事並びに\n",
      "82 这么狠\n",
      "83 喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵喵\n",
      "84 喵喵喵喵喵喵喵喵\n",
      "85 喵喵喵喵喵喵\n",
      "86 宋亚轩\n",
      "87 应该给\n",
      "88 樣穿吧\n",
      "89 蝙蝠侠\n",
      "90 膃肭臍\n",
      "91 呜呜呜呜呜\n",
      "92 为什么呢\n",
      "93 韩沉很\n",
      "94 转卖给\n",
      "95 箭蔓娜\n",
      "96 什麼聲\n",
      "97 彌勒菩\n",
      "98 瞎说什么\n",
      "99 很软吧\n",
      "100 咱终于\n",
      "101 什么呢\n",
      "102 婊嬸兒\n",
      "103 别找骂\n",
      "104 记录嘛\n",
      "105 很靈驗\n",
      "106 嘤嘤嘤\n",
      "107 个带籫儿\n",
      "108 啊啊啊\n",
      "109 已经烟雾缭绕\n",
      "110 啊呜啊呜\n",
      "111 什么梗\n",
      "112 槿桔梗\n",
      "113 怎麼拔\n",
      "114 嗶哩嗶哩\n",
      "115 为什么这个娃娃\n",
      "116 啊啊啊啊啊啊啊啊\n",
      "117 见惯莫\n",
      "118 很笨蛋\n",
      "119 趙匡胤\n",
      "120 圖嗎隨\n",
      "121 參賽喔\n",
      "122 娜澑舒\n",
      "123 怎麼說\n",
      "124 什麼樣\n",
      "125 嗯嗯嗯\n",
      "126 頽鮪黹贓\n",
      "127 懂傻傻\n",
      "128 怎样处\n",
      "129 这种质\n",
      "130 很虧欸\n",
      "131 拜拜啦\n",
      "132 什麼啊\n",
      "133 俾囡囡聽\n",
      "134 汪汪汪\n",
      "135 箭稔梛\n",
      "136 嘿嘿嘿嘿\n",
      "137 鶫鶫鶫鶫鶫鶫鶫鶫鶫鶫鶫鶫鶫鶫\n",
      "138 歇爾娃娃很\n",
      "139 奈央の\n",
      "140 连战后\n",
      "141 哭啾啾\n",
      "142 怎么样\n",
      "143 喵喵喵\n",
      "144 搞啥搞啥\n",
      "145 什么什么\n",
      "146 樂兔兔\n",
      "147 姊姊們\n",
      "148 为它撑\n",
      "149 褶皺錄\n",
      "150 吵闹闭嘴\n",
      "151 您倆怎麼\n",
      "152 菇凉请\n",
      "153 啊啊啊杂扒饭\n",
      "154 蘛嬭聾\n",
      "155 窬禰鶹\n",
      "156 为胖胖\n",
      "157 杨冰怡\n",
      "158 旁邊蹲\n",
      "159 怎么做\n",
      "160 什麼梗\n",
      "161 样枪毙\n",
      "162 已滿哦\n",
      "163 样垃圾\n",
      "164 为啥啊\n",
      "165 對喔估\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "for i in range(3,len(voc.index2word)):\n",
    "    if voc.word2count[voc.index2word[i]]<=28:\n",
    "        if len(voc.index2word[i])>=3:\n",
    "            print(n,voc.index2word[i])\n",
    "            n+=1\n",
    "    if n>200:\n",
    "        print(\"break\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 17796 / 21970 = 0.8100\n",
      "Trimmed from 1503290 pairs to 12221, 0.0081 of total\n",
      "Counted words in voc 17799\n",
      "Counted words in voc2: 9572\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 2\n",
    "#vocの辞書を作り直してから回す.\n",
    "def trimShortWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.shortTrim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\n",
    "        \"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
    "            len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
    "        )\n",
    "    )\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimShortWords(voc, pairs, MIN_COUNT)\n",
    "voc2 = Voc(corpus_name)\n",
    "for pair in pairs:\n",
    "    voc2.addSentence(pair[0])\n",
    "    voc2.addSentence(pair[1])\n",
    "print(\"Counted words in voc\", voc.num_words)\n",
    "#vocはコーパスに対して十分な語彙の辞書、コーパスに無い単語も含まれてる。\n",
    "#(vocを作成後、出現頻度の低い単語を削除し、削除された単語を含む文章をコーパスから削除したため。)\n",
    "print(\"Counted words in voc2:\", voc2.num_words)   \n",
    "#voc2はコーパスに対して必要十分な語彙の辞書、コーパス無いの単語全てを含んでいて、辞書内の単語は必ずコーパスに登場する。\n",
    "#(vocのように削除された単語を含む文章をコーパスから削除した後、その新しいコーパスから作成したのがvoc2だから。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3807\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "oneword=[]\n",
    "for i in range(3,len(voc.index2word),1):\n",
    "    if len(voc.index2word[i])<=1:\n",
    "        n+=1\n",
    "        oneword.append(voc.index2word[i])\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "▁\n",
      ":\n",
      "都\n",
      "を\n",
      "移\n",
      "と\n",
      "、\n",
      "り\n",
      "よ\n",
      "ろ\n",
      "っ\n",
      "パ\n",
      "ゲ\n",
      "w\n",
      "お\n",
      "目\n",
      "元\n",
      "は\n",
      "。\n",
      "の\n",
      "内\n",
      "部\n",
      "に\n",
      "ね\n",
      "ー\n",
      "?\n",
      "今\n",
      "る\n",
      "し\n",
      "腰\n",
      "命\n",
      "中\n",
      "率\n",
      "ソ\n",
      "マ\n",
      "笑\n",
      "事\n",
      "て\n",
      "ら\n",
      "1\n",
      "年\n",
      "が\n",
      "や\n",
      "何\n",
      "か\n",
      "ん\n",
      "我\n",
      "儘\n",
      "だ\n",
      "ク\n",
      "テ\n",
      "な\n",
      "君\n",
      "で\n",
      "食\n",
      "氷\n",
      "結\n",
      "拭\n",
      "い\n",
      "ぬ\n",
      "二\n",
      "宮\n",
      "み\n",
      "嵐\n",
      "色\n",
      "字\n",
      "面\n",
      "す\n",
      "ま\n",
      "害\n",
      "気\n",
      "わ\n",
      "濡\n",
      "魔\n",
      "界\n",
      "奴\n",
      "ほ\n",
      "先\n",
      "ず\n",
      "叱\n",
      "絶\n",
      "ッ\n",
      "対\n",
      "店\n",
      "実\n",
      "名\n",
      "ダ\n",
      "様\n",
      "k\n",
      "6\n",
      "万\n",
      "矢\n",
      "吹\n",
      "奈\n",
      "も\n",
      "歌\n",
      "暇\n",
      "即\n",
      "板\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(oneword[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@r_u__k_a_ 頑張ってください! クラブに通い続けたエロオヤジが見つけた必勝法らしいのでw 実際わたしはそれで小林由依ちゃんに勝てましたw\n",
      "\n",
      "@\t名詞,サ変接続,*,*,*,*,*\n",
      "r\t名詞,一般,*,*,*,*,*\n",
      "_\t名詞,サ変接続,*,*,*,*,*\n",
      "u\t名詞,一般,*,*,*,*,*\n",
      "__\t名詞,サ変接続,*,*,*,*,*\n",
      "k\t名詞,一般,*,*,*,*,*\n",
      "_\t名詞,サ変接続,*,*,*,*,*\n",
      "a\t名詞,一般,*,*,*,*,*\n",
      "_\t名詞,サ変接続,*,*,*,*,*\n",
      "頑張っ\t動詞,自立,*,*,五段・ラ行,連用タ接続,頑張る,ガンバッ,ガンバッ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "く\t動詞,非自立,*,*,カ変・クル,体言接続特殊２,くる,ク,ク\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "゙さい\t名詞,一般,*,*,*,*,*\n",
      "!\t名詞,サ変接続,*,*,*,*,*\n",
      "クラフ\t名詞,一般,*,*,*,*,*\n",
      "゙\t名詞,一般,*,*,*,*,*\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "通い\t動詞,自立,*,*,五段・ワ行促音便,連用形,通う,カヨイ,カヨイ\n",
      "続け\t動詞,非自立,*,*,一段,連用形,続ける,ツヅケ,ツズケ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "エロオヤシ\t名詞,一般,*,*,*,*,*\n",
      "゙が\t名詞,一般,*,*,*,*,*\n",
      "見つけ\t動詞,自立,*,*,一段,連用形,見つける,ミツケ,ミツケ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "必勝\t名詞,一般,*,*,*,*,必勝,ヒッショウ,ヒッショー\n",
      "法\t名詞,接尾,一般,*,*,*,法,ホウ,ホー\n",
      "らしい\t助動詞,*,*,*,形容詞・イ段,基本形,らしい,ラシイ,ラシイ\n",
      "の\t名詞,非自立,一般,*,*,*,の,ノ,ノ\n",
      "て\t助詞,格助詞,連語,*,*,*,て,テ,テ\n",
      "゙\t名詞,一般,*,*,*,*,*\n",
      "w\t名詞,一般,*,*,*,*,*\n",
      "実際\t副詞,助詞類接続,*,*,*,*,実際,ジッサイ,ジッサイ\n",
      "わたし\t名詞,代名詞,一般,*,*,*,わたし,ワタシ,ワタシ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "それ\t動詞,自立,*,*,一段,連用形,それる,ソレ,ソレ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "゙\t名詞,固有名詞,一般,*,*,*,*\n",
      "小林\t名詞,固有名詞,人名,姓,*,*,小林,コバヤシ,コバヤシ\n",
      "由依\t名詞,固有名詞,人名,名,*,*,由依,ユイ,ユイ\n",
      "ちゃん\t名詞,接尾,人名,*,*,*,ちゃん,チャン,チャン\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "勝て\t動詞,自立,*,*,一段,連用形,勝てる,カテ,カテ\n",
      "まし\t助動詞,*,*,*,特殊・マス,連用形,ます,マシ,マシ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "w\t名詞,固有名詞,組織,*,*,*,*\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import MeCab\n",
    "i=4\n",
    "print(deleted_bracket[i])\n",
    "tagger = MeCab.Tagger('/usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "for ele in tagger.parse(deleted_bracket[i]).split('\\n'):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='话'>\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[\\u4E00-\\u9FFF]\"\n",
    "print(re.search(pattern,'话绝对'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r'[一-龠々ヶぁ-んァ-ヴーa-zA-Z0-9!?\\n\\s\\t、。:%+&]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\\\u8bdd\\\\u7edd\\\\u5bf9'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'话绝对'.encode('unicode-escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-mikami-mercury",
   "language": "python",
   "name": "py37-mikami-mercury"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
