{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)\n",
    "#device=torch.device('cpu')########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train_model32000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"train_model32000.txt\"#\"mecab_tagged.txt\"#\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLines(file, n=10):\n",
    "    with open(file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for i in range(len(lines[:n])):\n",
    "        print(lines[i].split('\\t'),len(lines[i].split('\\t')[0].split()),len(lines[i].split('\\t')[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁なんと !', '▁なんと !\\n'] 2 2\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り\\n'] 2 12\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい\\n'] 13 4\n",
      "['▁まさかの 増えてる w 羨ましい', '▁ 連れ が 誰か 引いて た んですよ 、 きっと それ と交換 したんだ と思うんですけど 誰 だったのか w あ 、 たっくん は しれっと 買取 に 出しました ー\\n'] 4 25\n",
      "['▁ おち つけ よ', '▁みんな は げろ\\n'] 4 3\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !\\n'] 3 13\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w\\n'] 19 7\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w\\n'] 7 4\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい\\n'] 2 6\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真\\n'] 6 3\n"
     ]
    }
   ],
   "source": [
    "printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_name = \"train_model20000.txt\"\n",
    "# position = \"./data\"\n",
    "# corpus = os.path.join(position, corpus_name)\n",
    "# printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = len(self.index2word)  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print(\n",
    "            \"keep_words {} / {} = {:.4f}\".format(\n",
    "                len(keep_words),\n",
    "                len(self.word2index),\n",
    "                len(keep_words) / len(self.word2index),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1834337 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 34296\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい']\n",
      "['▁ おち つけ よ', '▁みんな は げろ']\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい']\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真']\n",
      "['▁ は ?', '▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "MAX_LENGTH = 20  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split(\"\\t\")] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and len(p[1].split(\" \")) < MAX_LENGTH\n",
    "\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name):#, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(corpus, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "咩\n",
      "乖哦\n",
      "胜\n",
      "鉴\n",
      "涅\n",
      "賈詡賈詡賈詡\n",
      "炮哥\n",
      "尤\n",
      "禕\n",
      "瑕\n",
      "躱\n",
      "湎\n",
      "軛\n",
      "皂\n",
      "另\n",
      "綽\n",
      "鷄\n",
      "嘻嘻嘻\n",
      "紧\n",
      "說實\n",
      "631\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "nn=40\n",
    "onew = []\n",
    "for i in range(3,len(voc.index2word)):\n",
    "    if voc.word2count[voc.index2word[i]]==1:\n",
    "        onew.append(voc.index2word[i])\n",
    "        if n<20+nn and nn<=n:\n",
    "            print(voc.index2word[i])\n",
    "        n+=1\n",
    "        \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 31995 / 34293 = 0.9330\n",
      "Trimmed from 1834337 pairs to 1822478, 0.9935 of total\n",
      "Counted words in voc 31998\n",
      "Counted words in voc2: 31998\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 28#22#3#  # Minimum word count threshold for trimming\n",
    "\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\n",
    "        \"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
    "            len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
    "        )\n",
    "    )\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "voc2 = Voc(corpus_name)\n",
    "for pair in pairs:\n",
    "    voc2.addSentence(pair[0])\n",
    "    voc2.addSentence(pair[1])\n",
    "print(\"Counted words in voc\", voc.num_words)\n",
    "#vocはコーパスに対して十分な語彙の辞書、コーパスに無い単語も含まれてる。\n",
    "#(vocを作成後、出現頻度の低い単語を削除し、削除された単語を含む文章をコーパスから削除したため。)\n",
    "print(\"Counted words in voc2:\", voc2.num_words)   \n",
    "#voc2はコーパスに対して必要十分な語彙の辞書、コーパス無いの単語全てを含んでいて、辞書内の単語は必ずコーパスに登場する。\n",
    "#(vocのように削除された単語を含む文章をコーパスから削除した後、その新しいコーパスから作成したのがvoc2だから。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(\" \")] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "\n",
    "####zip_longest(*[[あ,い,う,え,お],[か,き,く,け,こ,さ]])→ (あ,か)(い,き)(う,く)(え,け)(お,こ)(pad,さ)　にしてくれる\n",
    "\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)  # Byte?\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "# print(\"input_variable:\", input_variable)\n",
    "# print(\"lengths:\", lengths)\n",
    "# print(\"target_variable:\", target_variable)\n",
    "# print(\"mask:\", mask)\n",
    "# print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = ([1,2,3,4],[2,3,4,5])\n",
    "# bbb = aaa[0][:2]\n",
    "# print(bbb)\n",
    "# embedding = nn.Embedding(4, 5)\n",
    "# seq = torch.LongTensor([[2,2,2],[1,1,1],[2,2,2],[3,3,3],[1,1,0],[0,0,0]])\n",
    "# embedded = embedding(seq)\n",
    "# input_lengths = torch.LongTensor([5,5,4])\n",
    "# packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "# print(packed)\n",
    "\n",
    "# # rnn = nn.GRU(5, 4, 2)\n",
    "# # # input = torch.randn(6, 3, 5)\n",
    "# # h0 = torch.randn(2, 3, 4)\n",
    "# # output, hn = rnn(packed, h0)\n",
    "# # print(\"output\",output,\"\\n hn\",hn)\n",
    "# rnn = nn.LSTM(5, 4, 2)\n",
    "# # input = torch.randn(6, 3, 5)\n",
    "# h0 = None#torch.randn(2, 3, 4)\n",
    "# c0 = None#torch.randn(2, 3, 4)\n",
    "# output, hn = rnn(packed,(hn))\n",
    "# print(\"output\",output,\"\\nhn\",hn,)#\"\\ncn\",cn)\n",
    "# decoder_hidden = hn[:2]\n",
    "# print(decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "#         self.gru = nn.GRU(\n",
    "#             hidden_size,\n",
    "#             hidden_size,\n",
    "#             n_layers,\n",
    "#             dropout=(0 if n_layers == 1 else dropout),\n",
    "#             bidirectional=True,\n",
    "#         )\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None, cell=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.lstm(packed, hidden)#ほんとは(hidden,cell_state)の形にしたほうがいい\n",
    "\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, : self.hidden_size] + outputs[:, :, self.hidden_size :]\n",
    "        # [T x B x *]\n",
    "        # T is the length of the longest sequence\n",
    "        # B is the batch size\n",
    "        # * is any number of dimensions\n",
    "        \n",
    "        # hidden.size()=[(layer x numDirection) x B x *]\n",
    "        \n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method not in [\"dot\", \"general\", \"concat\"]:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "\n",
    "        elif self.method == \"general\":\n",
    "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"concat\":\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            print('self.vのパラメタNaNの数',torch.sum(torch.isnan(self.v)))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_outputs):\n",
    "        return torch.sum(encoder_outputs * hidden, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.attn(\n",
    "            torch.cat(\n",
    "                (hidden.expand(encoder_outputs.size(0), -1, -1), encoder_outputs), dim=2\n",
    "            )\n",
    "        ).tanh()\n",
    "        \n",
    "        return torch.sum(attn_energies * self.v, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_outputs):\n",
    "        energy = self.attn(encoder_outputs)\n",
    "        return torch.sum(energy * hidden, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == \"general\":\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "\n",
    "        elif self.method == \"dot\":\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('attn_energies',torch.sum(torch.isnan(attn_energies)),attn_energies)####2\n",
    "#         a = attn_energies * self.v\n",
    "#         print('a',a.size(),a)###\n",
    "#         c = torch.sum(torch.isnan(a))\n",
    "#         b= torch.sum(a,dim=2)\n",
    "#         print('c num の数',c,'b',b)####\n",
    "#         return b\n",
    "\n",
    "#print('attn_energies2',attn_energies)###1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [random.choice(pairs) for _ in range(5)]\n",
    "# input_variable, lengths, target_variable, mask, max_target_len = batch2TrainData(voc,a)\n",
    "# batch2TrainData(voc,a)\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# encoder = EncoderRNN(3, embedding, 2, 0)\n",
    "# decoder = AttnDecoderRNN(\n",
    "#     \"general\", embedding, 3, voc.num_words, 2, 0\n",
    "# )\n",
    "# encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "#     # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "# decoder_input = torch.LongTensor([[SOS_token for i in range(5)]])\n",
    "# decoder_input = decoder_input\n",
    "\n",
    "# # Set initial decoder hidden state to the encoder's final hidden state\n",
    "# decoder_hidden = encoder_hidden[: 2]\n",
    "# decoder_outputs, decoder_hidden = decoder(\n",
    "#     decoder_input, decoder_hidden, encoder_outputs\n",
    "# )\n",
    "# print(decoder_outputs.size())\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# embedded = embedding(input_variable)\n",
    "# packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "# gru = nn.GRU(3,3,4,dropout=0,bidirectional=True)\n",
    "# outputs, hidden = gru(packed, None)\n",
    "# print(outputs[0].size(),\"hidden\",hidden.size())\n",
    "# outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "# print(outputs.size(),\"hidden\",hidden.size())\n",
    "# outputs = outputs[:, :, :3] + outputs[:, :, 3:]\n",
    "# print(outputs.size())\n",
    "# decoder_input = torch.LongTensor([[SOS_token for i in range(5)]])\n",
    "# decoder_hidden = hidden[:4]\n",
    "# print('decoder_hidden',decoder_hidden.size())\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# embedded = embedding(decoder_input)\n",
    "# grud = nn.GRU(3,3,4,dropout=0)\n",
    "# print(embedded.size(),decoder_hidden.size())\n",
    "# rnn_output, hidden = grud(embedded,decoder_hidden)\n",
    "# print('rnn_output=',rnn_output.size(),'hidden=',hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, attn_model, embeding, hidden_size, output_size, n_layers=1, dropout=0.1\n",
    "    ):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embeding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "#         self.gru = nn.GRU(\n",
    "#             hidden_size,\n",
    "#             hidden_size,\n",
    "#             n_layers,\n",
    "#             dropout=(0 if n_layers == 1 else dropout),\n",
    "#         )\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "        )\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
    "        # rnn_output.Size() = [1(sequenceLength) x B x *]\n",
    "        # hidden.Size() = [layer x B x *]\n",
    "        \n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding of current input word.\n",
    "Forward through unidirectional GRU.\n",
    "Calculate attention weights from the current GRU output from (2).\n",
    "Multiply attention weights to encoder outputs to get new “weighted sum” context vector.\n",
    "Concatenate weighted context vector and GRU output using Luong eq. 5.\n",
    "Predict next word using Luong eq. 6 (without softmax).\n",
    "Return output and final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)(1,3)\n",
    "last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)(1,3,2)\n",
    "encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1834337 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 34296\n",
      "keep_words 31995 / 34293 = 0.9330\n",
      "Trimmed from 1834337 pairs to 1822478, 0.9935 of total\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい']\n",
      "['▁ おち つけ よ', '▁みんな は げろ']\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい']\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真']\n",
      "['▁ は ?', '▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な']\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 28#22#3#\n",
    "print(corpus_name)\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focalLoss(inp, target, mask, gamma=2, eps=1e-7):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)\n",
    "    crossEntropy = -torch.log(logit)# cross entropy\n",
    "    loss = crossEntropy * (1 - logit) ** gamma # focal loss\n",
    "    loss = loss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1))\n",
    "    crossEntropy = -torch.log(logit.squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    clip,\n",
    "    #cal_method,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)#encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    #decoder_hidden = encoder_hidden[: decoder.n_layers]        #lstmにするにはここを変えなきゃいけない気がする。これはGRU用\n",
    "    decoder_hidden = (encoder_hidden[0][: decoder.n_layers], encoder_hidden[1][: decoder.n_layers])\n",
    "    \n",
    "    \n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = focalLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_outputs.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = focalLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "#     propose_loss = propose_loss_cal(\n",
    "#         decoder_hidden, \n",
    "#         target_variable, \n",
    "#         max_target_len, \n",
    "#         decoder,\n",
    "#         encoder_outputs[: decoder.n_layers],\n",
    "#         encoder_hidden,\n",
    "#         decoder.training,\n",
    "#         cal_method=cal_method,\n",
    "#     )\n",
    "    \n",
    "#     loss += propose_loss*max_target_len #重み付けしてない\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    # print(_)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len\n",
    "#lossはたぶん1単語あたりのcross entropy lossの値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    batch_size,\n",
    "    #cal_method,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    #decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "    decoder_hidden = (encoder_hidden[0][: decoder.n_layers], encoder_hidden[1][: decoder.n_layers])\n",
    "\n",
    "    \n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    \n",
    "\n",
    "    for t in range(max_target_len):\n",
    "        decoder_outputs, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        # No teacher forcing: next input is decoder's own current output\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        # Calculate and accumulate loss\n",
    "        mask_loss, nTotal = focalLoss(\n",
    "            decoder_outputs, target_variable[t], mask[t]\n",
    "        )\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item() * nTotal)\n",
    "        n_totals += nTotal\n",
    "    \n",
    "#     propose_loss = propose_loss_cal(\n",
    "#         decoder_hidden, \n",
    "#         target_variable, \n",
    "#         max_target_len, \n",
    "#         decoder,\n",
    "#         encoder_outputs[: decoder.n_layers],\n",
    "#         encoder_hidden,\n",
    "#         decoder.training,\n",
    "#         cal_method=cal_method,\n",
    "#     )\n",
    "#     loss += propose_loss*max_target_len #重み付けしてない\n",
    "\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "def trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "    #cal_method='mse',\n",
    "    start_iteration=1,\n",
    "):\n",
    "\n",
    "    \n",
    "#     train_batches = [\n",
    "#         batch2TrainData(voc, [random.choice(train_pairs) for _ in range(batch_size)])\n",
    "#         for _ in range(n_iteration)\n",
    "#     ]\n",
    "    \n",
    "#     test_batches = [\n",
    "#         batch2TrainData(voc, [random.choice(test_pairs) for _ in range(batch_size)])\n",
    "#         for _ in range(n_iteration)\n",
    "#     ]\n",
    "\n",
    "    # Initializations\n",
    "    print(\"Initializing ...\")\n",
    "    # start_iteration = 1\n",
    "    print_loss = 0\n",
    "    print_val_loss = 0\n",
    "    liveloss = PlotLosses()\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "        print('start from',start_iteration,\"iteration\")\n",
    "        liveloss = checkpoint[\"log\"]\n",
    "    \n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = batch2TrainData(voc, [random.choice(train_pairs) for _ in range(batch_size)])\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        \n",
    "        # Ensure dropout layers are in train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(   #,propose_loss\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            teacher_forcing_ratio,\n",
    "            clip,\n",
    "            #cal_method=cal_method\n",
    "        )\n",
    "        \n",
    "\n",
    "        print_loss += loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_batch = batch2TrainData(voc, [random.choice(test_pairs) for _ in range(batch_size)])\n",
    "            # Extract fields from batch\n",
    "            input_variable, lengths, target_variable, mask, max_target_len = test_batch\n",
    "            # Ensure dropout layers are in eval mode\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            \n",
    "            # Run a training iteration with batch\n",
    "            val_loss = test(     #,val_propose_loss\n",
    "                input_variable,\n",
    "                lengths,\n",
    "                target_variable,\n",
    "                mask,\n",
    "                max_target_len,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                embedding,\n",
    "                batch_size,\n",
    "                #cal_method=cal_method\n",
    "            )\n",
    "\n",
    "            print_val_loss += val_loss\n",
    "        \n",
    "        log={}\n",
    "        log['loss'] = loss\n",
    "        log['val_loss'] = val_loss\n",
    "#         log['vector_distance'] = propose_loss\n",
    "#         log['val_vector_distance'] = val_propose_loss\n",
    "        liveloss.update(log)\n",
    "        if iteration % print_every == 0:\n",
    "            liveloss.draw()\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print_val_loss_avg = print_val_loss / print_every\n",
    "#             print(\n",
    "#                 \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
    "#                     iteration, iteration / n_iteration * 100, print_loss_avg\n",
    "#                 )\n",
    "#             )\n",
    "            with open(voc.name[:-4]+'_logs','a') as f:\n",
    "                f.write(\"\\nIteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}; Average validation loss: {:.4f}\".format(\n",
    "                    iteration, iteration / n_iteration * 100, print_loss_avg, print_val_loss_avg\n",
    "                )\n",
    "                       )\n",
    "            \n",
    "            print_loss = 0\n",
    "            print_val_loss = 0\n",
    "            \n",
    "            graph_x = range(len(liveloss.logs))\n",
    "            pdf = PdfPages(voc.name[:-4]+'_logs.pdf')\n",
    "            plt.figure()\n",
    "            plt.plot(graph_x, [i['loss'] for i in liveloss.logs])\n",
    "            plt.plot(graph_x, [i['val_loss'] for i in liveloss.logs])\n",
    "            plt.legend(('training','validation'))\n",
    "            pdf.savefig()\n",
    "            pdf.close()\n",
    "            \n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                \"{}-{}_{}_{}\".format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"log\": liveloss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, \"{}_{}.tar\".format(iteration, \"checkpoint\")),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "ちゃんと目的のコーパスファイルを読み込んでいるか確認\n",
      "train size 1457982\n",
      "example [['▁ 問い合わせ w 間違いない ですね w', '▁ して はいない んですけどね w それも 運 なの だろう'], ['▁ 珈琲 とか ウイスキー 用', '▁ 娘 用']]\n",
      "test size 364496\n",
      "example [['▁ ピカチュウ は かわいい でも カビゴン はもっと かわいい', '▁いやあ ずみ のほうが'], ['▁ スカ トロ とか やめてくれ だめ や', '▁そこまで 言って ねーよ w それは あかん']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# #Load batches for each iteration\n",
    "# train_pairs,test_pairs = train_test_split(pairs ,test_size=0.2)\n",
    "# with open('data/train_test_pairs_32k.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_pairs,test_pairs), f)\n",
    "# with open(\"data/train_test_pairs_mecab3min.pickle\", \"rb\") as f:\n",
    "#     train_pairs,test_pairs = pickle.load(f)\n",
    "with open(\"data/train_test_pairs_32k.pickle\", \"rb\") as f:\n",
    "    train_pairs,test_pairs = pickle.load(f)\n",
    " \n",
    "print(voc.name)\n",
    "print(\"ちゃんと目的のコーパスファイルを読み込んでいるか確認\")\n",
    "print(\"train size\",len(train_pairs))\n",
    "print(\"example\",train_pairs[:2])\n",
    "print(\"test size\",len(test_pairs))\n",
    "print(\"example\",test_pairs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "print(voc.name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model_gamma2\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 1024\n",
    "teacher_forcing_ratio = 0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 500000#100000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パラメータ数をカウント\n",
      "encoderのパラメータ数\n",
      "13975040\n",
      "decoderパラメータ数\n",
      "18651646\n"
     ]
    }
   ],
   "source": [
    "# パラメータカウント関数\n",
    "def parameters_count(net):\n",
    "    params = 0\n",
    "    for p in net.parameters():\n",
    "        #print(p)\n",
    "        #print(p.numel())\n",
    "        if p.requires_grad:\n",
    "            params += p.numel()\n",
    "        \n",
    "    print(params)  # 121898\n",
    "\n",
    "print(\"パラメータ数をカウント\")\n",
    "print(\"encoderのパラメータ数\")\n",
    "parameters_count(encoder)\n",
    "print(\"decoderパラメータ数\")\n",
    "parameters_count(decoder)\n",
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pip/mamoru_mikami/.conda/envs/py37-mikami_saturn/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      "training   (min:    4.997, max:   10.374, cur:    5.714)\n",
      "validation (min:    6.492, max:   10.367, cur:    7.424)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pip/mamoru_mikami/.conda/envs/py37-mikami_saturn/lib/python3.7/site-packages/ipykernel_launcher.py:142: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.2####\n",
    "learning_rate = 0.0001#学習率を0.001とか大きめにとると（大きく無いと思うけど）途中でnanを吐き始める\n",
    "decoder_learning_ratio = 2.0\n",
    "n_iteration = 800000\n",
    "print_every = 500\n",
    "save_every = 1000\n",
    "print('the number of data is',len(pairs))\n",
    "\n",
    "\n",
    "# Initialize optimizers\n",
    "print(\"Building optimizers ...\")\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"count parameters\")\n",
    "\n",
    "print(\"Starting Training!\")\n",
    "trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "    #cal_method=cal_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab_tagged.txt\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "print(corpus_name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"#\"normal_model\"#\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "teacher_forcing_ratio=0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 35000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-869b6a839456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37-mikami_saturn/lib/python3.7/site-packages/MeCab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_newclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_Tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return [all_tokens], [-torch.sum(torch.log(all_scores).detach().cpu())/float(max_length - 1 + 1e-6)]\n",
    "    \n",
    "    \n",
    "# import sentencepiece as spm\n",
    "# segmentation_model_position = './data'\n",
    "# segmentation_model_name = 'train_model32000.model'\n",
    "\n",
    "# spp = spm.SentencePieceProcessor()\n",
    "# spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "# def normalizeString(input_sentence):\n",
    "#     splitSentence = ' '.join(spp.EncodeAsPieces(input_sentence))\n",
    "#     return splitSentence\n",
    "\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/\")\n",
    "\n",
    "def normalizeString(input_sentence):\n",
    "    splitSentence = ' '.join([ele.split(\"\\t\")[0] for ele in tagger.parse(input_sentence).split(\"\\n\")[:-2]])\n",
    "    return splitSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, beam_width, n_best):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beam_width = beam_width\n",
    "        self.n_best = n_best\n",
    "        \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        #decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        \n",
    "        class BeamSearchNode(object):\n",
    "            def __init__(self, hid, prevNode, wordId, logP, length):\n",
    "                self.hid = hid\n",
    "                self.prevNode = prevNode\n",
    "                self.wordId = wordId\n",
    "                self.logP = logP\n",
    "                self.length = length\n",
    "                \n",
    "            def eval(self, alpha=1.0):\n",
    "                reward = 0\n",
    "                # Add here a function for shaping a reward\n",
    "\n",
    "                return self.logP / float(self.length - 1 + 1e-6) + alpha * reward\n",
    "          \n",
    "        \n",
    "        n_best_batch_list = []\n",
    "        n_best_batch_score_list = []\n",
    "        for batchNum in range(encoder_hidden.size(1)):\n",
    "            \n",
    "            decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
    "            # 一つ前の状態の隠れベクトル、単語をNodeを保持するNodeを生成\n",
    "            node = BeamSearchNode(hid=decoder_hidden[:,batchNum,:].unsqueeze(1), prevNode=None, wordId=decoder_input, logP=0, length=1)\n",
    "            nextNodes=[]\n",
    "            \n",
    "            nextNodes.append((-node.eval(), id(node), node))\n",
    "            n_dec_steps = 0\n",
    "            while True:\n",
    "                \n",
    "                nodes = [sorted(nextNodes)[inode] for inode in range(len(nextNodes)) if inode<self.beam_width]\n",
    "                nextNodes = []\n",
    "                end_node = []\n",
    "                for beamNum in range(self.beam_width):\n",
    "                    if len(nodes)<=0:\n",
    "                        break\n",
    "                    #今から探索するNodeを取得\n",
    "                    score, _, n = nodes.pop(0)\n",
    "                    decoder_input = n.wordId\n",
    "                    decoder_hidden = n.hid\n",
    "                    \n",
    "                    if n.wordId[0][0].item()!=EOS_token and n.wordId[0][0].item()!=PAD_token and n.length<=max_length:\n",
    "                        decoder_output, decoder_hidden = self.decoder(\n",
    "                            decoder_input, decoder_hidden, encoder_outputs\n",
    "                        )\n",
    "\n",
    "                        topk_prob, topk_indexes = torch.topk(decoder_output, self.beam_width) \n",
    "\n",
    "                        for new_k in range(self.beam_width):\n",
    "                            decoded_t = topk_indexes[0][new_k].view(1,1) # (1)\n",
    "                            logp = torch.log(topk_prob[0][new_k]).item() # float log probability val\n",
    "\n",
    "                            node = BeamSearchNode(hid=decoder_hidden,\n",
    "                                                  prevNode=n,\n",
    "                                                  wordId=decoded_t,\n",
    "                                                  logP=n.logP+logp,\n",
    "                                                  length=n.length+1)\n",
    "                            nextNodes.append((-node.eval(), id(node), node))\n",
    "                            \n",
    "                            \n",
    "                    else:\n",
    "                        node = BeamSearchNode(hid=decoder_hidden,\n",
    "                                                  prevNode=n,\n",
    "                                                  wordId=torch.LongTensor([[PAD_token]]).to(device),\n",
    "                                                  logP=n.logP,\n",
    "                                                  length=n.length)\n",
    "                        nextNodes.append((-node.eval(), id(node), node))\n",
    "                        end_node.append(1)\n",
    "                if len(end_node)>=self.beam_width:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "        \n",
    "            if len(nextNodes)!=self.beam_width:\n",
    "                print('assert not match beam width and nextNode length')\n",
    "                \n",
    "            n_best_seq_list = []\n",
    "            n_best_score_list = []\n",
    "            for score, _id, n in sorted(nextNodes):\n",
    "                sequence = [n.wordId.item()]\n",
    "                n_best_score_list.append(score)\n",
    "                # back trace from end node\n",
    "                while n.prevNode is not None:\n",
    "                    n = n.prevNode\n",
    "                    sequence.append(n.wordId.item())\n",
    "                    \n",
    "                sequence = sequence[::-1] # reverse\n",
    "                n_best_seq_list.append(sequence)\n",
    "        \n",
    "            n_best_seq_list = n_best_seq_list[::-1]\n",
    "            n_best_score_list = n_best_score_list[::-1]\n",
    "            \n",
    "            n_best_batch_list.append(n_best_seq_list)\n",
    "            n_best_batch_score_list.append(n_best_score_list)\n",
    "            \n",
    "        n_best_batch_list = torch.tensor(n_best_batch_list)\n",
    "        n_best_batch_score_list = torch.tensor(n_best_batch_score_list)\n",
    "            \n",
    "            #batchを無視して今回は出力することにした\n",
    "        return n_best_batch_list[0], n_best_batch_score_list[0]\n",
    "    \n",
    "# import sentencepiece as spm\n",
    "# segmentation_model_position = './data'\n",
    "# segmentation_model_name = 'train_model20000.model'\n",
    "\n",
    "# spp = spm.SentencePieceProcessor()\n",
    "# spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "# def normalizeString(input_sentence):\n",
    "#     splitSentence = ' '.join(spp.EncodeAsPieces(input_sentence))\n",
    "#     return splitSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    seqs, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    for iseqs in range(len(seqs)):\n",
    "        tokens = seqs[iseqs]\n",
    "    \n",
    "        # indexes -> words\n",
    "        decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "        print(scores[iseqs],decoded_words)\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, searcher1, voc):\n",
    "    input_sentence = \"\"\n",
    "    while 1:\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input(\"> \")\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == \"q\" or input_sentence == \"quit\":\n",
    "                break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            print('\\n入力を分割すると',input_sentence+'\\n')\n",
    "            \n",
    "            # Evaluate sentence with seacher\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"EOS\" or x == \"PAD\")\n",
    "            ]\n",
    "            print(\"\\nBot(BeamSearch):\", \" \".join(output_words))\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Evaluate sentence with seacher1\n",
    "            output_words = evaluate(encoder, decoder, searcher1, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"EOS\" or x == \"PAD\")\n",
    "            ]\n",
    "            print(\"\\nBot(GreedySearch):\", \" \".join(output_words)+\"\\n\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['なんと!\\n',\n",
       " 'なんと!\\n',\n",
       " '意味:都を移すこと、都移り\\n',\n",
       " 'わたし安定のたっくんですよー気づいたらたっくんいないし、いつきあるし誰と交換したのか\\n',\n",
       " 'まさかの増えてるw羨ましい\\n',\n",
       " '連れが誰か引いてたんですよ、きっとそれと交換したんだと思うんですけど誰だったのかwあ、たっくんはしれっと買取に出しましたー\\n',\n",
       " 'おちつけよ\\n',\n",
       " 'みんなはげろ\\n',\n",
       " '昨日髪の毛切ったばっかりだけどこのくらいつるっパゲになってきます!\\n',\n",
       " '今は古すぎて廃盤のものが多いので、いいモンスターが出るのだと高い値で売れますw\\n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/processed_ja_file.txt','r') as file:\n",
    "    bun = file.readlines()\n",
    "bun[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  おはよう\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁おはよう\n",
      "\n",
      "tensor(1.8040) ['SOS', '▁おはようございます', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.7218) ['SOS', '▁', 'きみ', 'EOS', 'PAD']\n",
      "tensor(1.7100) ['SOS', '▁おはよ', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.4883) ['SOS', '▁', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.4581) ['SOS', '▁おはよう', 'EOS', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁おはよう\n",
      "\n",
      "tensor(0.7848) ['▁', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'をお', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ をお\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  こんにちは\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁こんにちは\n",
      "\n",
      "tensor(1.4943) ['SOS', '▁', '。', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.4761) ['SOS', '▁', '56', 'さん', '、', '、', 'EOS', 'PAD']\n",
      "tensor(1.4211) ['SOS', '▁', '56', 'さん', '、', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.0695) ['SOS', '▁', '56', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(0.9639) ['SOS', '▁', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁\n",
      "\n",
      "tensor(0.6234) ['▁', '56', 'EOS', '、', '、', 'EOS', 'EOS', '13', '14', '15', '16', 'EOS', '。', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ 56 、 、 13 14 15 16 。\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  こんばんは\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁こんばんは\n",
      "\n",
      "tensor(1.5753) ['SOS', '▁', 'こんばん', 'に', '!', 'EOS', 'PAD']\n",
      "tensor(1.5342) ['SOS', '▁', 'こんばん', 'ダン', '!', 'EOS', 'PAD']\n",
      "tensor(1.4605) ['SOS', '▁', 'こんばん', 'に', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.3844) ['SOS', '▁', 'こんばん', 'ダン', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.0029) ['SOS', '▁こんばんは', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁こんばんは\n",
      "\n",
      "tensor(0.8932) ['▁こんばんは', 'EOS', 'ダン', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'チーズ', 'EOS', '!', '!', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁こんばんは ダン チーズ ! !\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  いや、なんでやねん\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁いや 、 なんでやねん\n",
      "\n",
      "Error: Encountered unknown word.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  どこいこうか\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁どこ いこう か\n",
      "\n",
      "tensor(1.3370) ['SOS', '▁', 'ば', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.1616) ['SOS', '▁', 'でかく', 'たら', 'EOS', 'PAD']\n",
      "tensor(1.0613) ['SOS', '▁', 'でかく', 'w', 'EOS', 'PAD']\n",
      "tensor(1.0578) ['SOS', '▁', 'でかく', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.0228) ['SOS', '▁', '探せば', '住んでる', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ 探せば 住んでる\n",
      "\n",
      "tensor(0.4608) ['▁', 'でかく', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ でかく\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  あーなるほどね\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁あーなるほど ね\n",
      "\n",
      "tensor(0.9647) ['SOS', '▁', 'ゲスト', '過ぎて', '携帯', 'いたら', 'いたら', 'いたら', '!', '来い', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(0.9626) ['SOS', '▁', 'ゲスト', '過ぎて', '携帯', 'いたら', 'いたら', 'いたら', '!', '来い', 'w', 'EOS', 'PAD', 'PAD']\n",
      "tensor(0.9508) ['SOS', '▁', 'ゲスト', '過ぎて', '入れ', 'いたら', 'いたら', 'いたら', 'やる', '来い', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(0.9258) ['SOS', '▁', 'ゲスト', '過ぎて', '入れ', 'いたら', 'いたら', 'いたら', 'やる', '来い', 'w', 'EOS', 'PAD', 'PAD']\n",
      "tensor(0.8832) ['SOS', '▁', 'ゲスト', '過ぎて', '携帯', 'いたら', 'いたら', 'いたら', '!', '来い', 'の問題', 'w', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ ゲスト 過ぎて 携帯 いたら いたら いたら ! 来い の問題 w\n",
      "\n",
      "tensor(0.5614) ['▁', 'ゲスト', '過ぎて', 'バ', 'いたら', 'いたら', 'いたら', '!', '来い', 'の問題', 'w', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ ゲスト 過ぎて バ いたら いたら いたら ! 来い の問題 w\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  q\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "#searcher = GreedySearchDecoder(encoder, decoder)\n",
    "searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "searcher１ = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, searcher1, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "def cal_bleu_score(pairs, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    searcher = GreedySearchDecoder(encoder, decoder)\n",
    "    score = 0\n",
    "    for pair in tqdm(pairs):\n",
    "        input_sentence = normalizeString(pair[0])\n",
    "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "        candidate = [x for x in output_words if not (x == \"EOS\" or x == \"PAD\")]\n",
    "        reference = [pair[1].split(' ')]\n",
    "        score += sentence_bleu(reference, candidate)\n",
    "        #print(candidate,'\\n',reference,'\\n',sentence_bleu(reference, candidate))\n",
    "    return score/len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load save file\n",
      "Building encoder and decoder ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/53165 [00:00<08:26, 104.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vのパラメタNaNの数 tensor(0)\n",
      "Models built and ready to go!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53165/53165 [07:53<00:00, 112.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4206171036107889"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_model(model_name='cb_model', check_point=20000)\n",
    "cal_bleu_score(pairs, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  5,  3,  4, -4])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "b = [1,2,3,4,5,1,2,3,4,-1,-4]\n",
    "b = np.array(b)\n",
    "b[abs(b)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1))\n",
    "    crossEntropy = -torch.log(logit.squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "def focalLoss(inp, target, mask, gamma=1, eps=1e-7):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)\n",
    "    crossEntropy = -torch.log(logit)# cross entropy\n",
    "    loss = crossEntropy * (1 - logit) ** gamma # focal loss\n",
    "    loss = loss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "\n",
    "xitem = [i/100 for i in range(1,101)]\n",
    "for ite in [ 0.5, 1, 2, 5]:\n",
    "    focalloss = []\n",
    "    normalloss = []\n",
    "    for x in range(1,101):\n",
    "        sample = [[(1-x/100)/3, x/100, (1-x/100)/3, (1-x/100)/3]]\n",
    "        decoder_output = torch.tensor(sample)\n",
    "        mask = torch.BoolTensor([[1]])\n",
    "        target_variable = torch.LongTensor([[1]])\n",
    "        normalloss.append(maskNLLLoss(decoder_output, target_variable[0], mask[0])[0].item())\n",
    "        focalloss.append(focalLoss(decoder_output, target_variable[0], mask[0],gamma=ite)[0].item())\n",
    "        \n",
    "    plt.plot(xitem,focalloss,label=('focal gamma='+str(ite)))\n",
    "    \n",
    "plt.plot(xitem,normalloss,label=('normal'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-mikami_saturn",
   "language": "python",
   "name": "py37-mikami_saturn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
