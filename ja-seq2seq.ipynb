{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)\n",
    "#device=torch.device('cpu')########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train_model20000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"train_model20000.txt\"\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLines(file, n=10):\n",
    "    with open(file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for i in range(len(lines[:n])):\n",
    "        print(lines[i].split('\\t'),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁なんと !', '▁なんと !\\n'] 0\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り\\n'] 1\n",
      "['▁わたし 安定の た っく んですよー 気づいたら た っ くん いない し 、 い つき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい\\n'] 2\n",
      "['▁まさかの 増えてる w 羨ましい', '▁ 連れ が 誰か 引いて た んですよ 、 きっと それ と交換 したんだ と思うんですけど 誰 だったのか w あ 、 た っ くんは しれっと 買取 に出 しましたー\\n'] 3\n",
      "['▁ おち つけ よ', '▁みんな はげ ろ\\n'] 4\n",
      "['▁みんな はげ ろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !\\n'] 5\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多い ので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w\\n'] 6\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w\\n'] 7\n",
      "['▁やだ お', '▁ 目 元 は 知ってる 、 かあ いい\\n'] 8\n",
      "['▁ 目 元 は 知ってる 、 かあ いい', '▁超 過去 写真\\n'] 9\n"
     ]
    }
   ],
   "source": [
    "printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = len(self.index2word)  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print(\n",
    "            \"keep_words {} / {} = {:.4f}\".format(\n",
    "                len(keep_words),\n",
    "                len(self.word2index),\n",
    "                len(keep_words) / len(self.word2index),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1503290 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 21973\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁ おち つけ よ', '▁みんな はげ ろ']\n",
      "['▁みんな はげ ろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目 元 は 知ってる 、 かあ いい']\n",
      "['▁ 目 元 は 知ってる 、 かあ いい', '▁超 過去 写真']\n",
      "['▁いつ できますか 。', '▁今 大丈夫です !']\n",
      "['▁今 大丈夫です !', '▁ カフェ の 内 部 に 来てください']\n",
      "['▁絶対 上手い って ! 描いて みて', '▁それはない ね ! ミニ ー ちゃん ?']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "MAX_LENGTH = 15  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split(\"\\t\")] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and len(p[1].split(\" \")) < MAX_LENGTH\n",
    "\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name):#, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(corpus, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 20355 / 21970 = 0.9265\n",
      "Trimmed from 1503290 pairs to 1499411, 0.9974 of total\n",
      "Counted words in voc 20358\n",
      "Counted words in voc2: 20358\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 10  # Minimum word count threshold for trimming\n",
    "\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\n",
    "        \"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
    "            len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
    "        )\n",
    "    )\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "voc2 = Voc(corpus_name)\n",
    "for pair in pairs:\n",
    "    voc2.addSentence(pair[0])\n",
    "    voc2.addSentence(pair[1])\n",
    "print(\"Counted words in voc\", voc.num_words)\n",
    "#vocはコーパスに対して十分な語彙の辞書、コーパスに無い単語も含まれてる。\n",
    "#(vocを作成後、出現頻度の低い単語を削除し、削除された単語を含む文章をコーパスから削除したため。)\n",
    "print(\"Counted words in voc2:\", voc2.num_words)   \n",
    "#voc2はコーパスに対して必要十分な語彙の辞書、コーパス無いの単語全てを含んでいて、辞書内の単語は必ずコーパスに登場する。\n",
    "#(vocのように削除された単語を含む文章をコーパスから削除した後、その新しいコーパスから作成したのがvoc2だから。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(\" \")] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "\n",
    "####zip_longest(*[[あ,い,う,え,お],[か,き,く,け,こ,さ]])→ (あ,か)(い,き)(う,く)(え,け)(お,こ)(pad,さ)　にしてくれる\n",
    "\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)  # Byte?\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "# print(\"input_variable:\", input_variable)\n",
    "# print(\"lengths:\", lengths)\n",
    "# print(\"target_variable:\", target_variable)\n",
    "# print(\"mask:\", mask)\n",
    "# print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, : self.hidden_size] + outputs[:, :, self.hidden_size :]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method not in [\"dot\", \"general\", \"concat\"]:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "\n",
    "        elif self.method == \"general\":\n",
    "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"concat\":\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            print('self.vのパラメタNaNの数',torch.sum(torch.isnan(self.v)))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_outputs):\n",
    "        return torch.sum(encoder_outputs * hidden, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.attn(\n",
    "            torch.cat(\n",
    "                (hidden.expand(encoder_outputs.size(0), -1, -1), encoder_outputs), dim=2\n",
    "            )\n",
    "        ).tanh()\n",
    "        \n",
    "        return torch.sum(attn_energies * self.v, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_outputs):\n",
    "        energy = self.attn(encoder_outputs)\n",
    "        return torch.sum(energy * hidden, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == \"general\":\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "\n",
    "        elif self.method == \"dot\":\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('attn_energies',torch.sum(torch.isnan(attn_energies)),attn_energies)####2\n",
    "#         a = attn_energies * self.v\n",
    "#         print('a',a.size(),a)###\n",
    "#         c = torch.sum(torch.isnan(a))\n",
    "#         b= torch.sum(a,dim=2)\n",
    "#         print('c num の数',c,'b',b)####\n",
    "#         return b\n",
    "\n",
    "#print('attn_energies2',attn_energies)###1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, attn_model, embeding, hidden_size, output_size, n_layers=1, dropout=0.1\n",
    "    ):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embeding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "        )\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding of current input word.\n",
    "Forward through unidirectional GRU.\n",
    "Calculate attention weights from the current GRU output from (2).\n",
    "Multiply attention weights to encoder outputs to get new “weighted sum” context vector.\n",
    "Concatenate weighted context vector and GRU output using Luong eq. 5.\n",
    "Predict next word using Luong eq. 6 (without softmax).\n",
    "Return output and final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)(1,3)\n",
    "last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)(1,3,2)\n",
    "encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1503290 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 21973\n",
      "keep_words 20355 / 21970 = 0.9265\n",
      "Trimmed from 1503290 pairs to 1499411, 0.9974 of total\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁ おち つけ よ', '▁みんな はげ ろ']\n",
      "['▁みんな はげ ろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目 元 は 知ってる 、 かあ いい']\n",
      "['▁ 目 元 は 知ってる 、 かあ いい', '▁超 過去 写真']\n",
      "['▁いつ できますか 。', '▁今 大丈夫です !']\n",
      "['▁今 大丈夫です !', '▁ カフェ の 内 部 に 来てください']\n",
      "['▁絶対 上手い って ! 描いて みて', '▁それはない ね ! ミニ ー ちゃん ?']\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 10\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def propose_loss_cal(\n",
    "#     generate_hidden, \n",
    "#     target_variable, \n",
    "#     max_target_len, \n",
    "#     decoder, \n",
    "#     encoder_outputs, \n",
    "#     encoder_hidden,\n",
    "#     mode,\n",
    "#     cal_method='mse',\n",
    "# ):\n",
    "#     with torch.no_grad():\n",
    "#         decoder.eval()\n",
    "#         # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "#         decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "#         decoder_input = decoder_input.to(device)\n",
    "\n",
    "#         # Set initial decoder hidden state to the encoder's final hidden state\n",
    "#         decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "\n",
    "#         for t in range(max_target_len):\n",
    "#             decoder_outputs, decoder_hidden = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs\n",
    "#             )\n",
    "#     if mode:\n",
    "#         decoder.train()\n",
    "#     target_hidden = decoder_hidden\n",
    "    \n",
    "#     if cal_method == '':\n",
    "#         None\n",
    "#     elif cal_method == 'mse':\n",
    "#         propose_loss = F.mse_loss(generate_hidden, target_hidden, reduction='mean')\n",
    "#     elif cal_method == 'cos':\n",
    "#         propose_loss = F.cosine_embedding_loss(generate_hidden, target_hidden, torch.tensor(1).float().to(device), reduction='mean')\n",
    "#     return propose_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    clip,\n",
    "    #cal_method,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "    \n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_outputs.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "#     propose_loss = propose_loss_cal(\n",
    "#         decoder_hidden, \n",
    "#         target_variable, \n",
    "#         max_target_len, \n",
    "#         decoder,\n",
    "#         encoder_outputs[: decoder.n_layers],\n",
    "#         encoder_hidden,\n",
    "#         decoder.training,\n",
    "#         cal_method=cal_method,\n",
    "#     )\n",
    "    \n",
    "#     loss += propose_loss*max_target_len #重み付けしてない\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    # print(_)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len\n",
    "#lossはたぶん1単語あたりのcross entropy lossの値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    batch_size,\n",
    "    #cal_method,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    \n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    \n",
    "\n",
    "    for t in range(max_target_len):\n",
    "        decoder_outputs, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        # No teacher forcing: next input is decoder's own current output\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        # Calculate and accumulate loss\n",
    "        mask_loss, nTotal = maskNLLLoss(\n",
    "            decoder_outputs, target_variable[t], mask[t]\n",
    "        )\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item() * nTotal)\n",
    "        n_totals += nTotal\n",
    "    \n",
    "#     propose_loss = propose_loss_cal(\n",
    "#         decoder_hidden, \n",
    "#         target_variable, \n",
    "#         max_target_len, \n",
    "#         decoder,\n",
    "#         encoder_outputs[: decoder.n_layers],\n",
    "#         encoder_hidden,\n",
    "#         decoder.training,\n",
    "#         cal_method=cal_method,\n",
    "#     )\n",
    "#     loss += propose_loss*max_target_len #重み付けしてない\n",
    "\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "def trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "    #cal_method='mse',\n",
    "    start_iteration=1,\n",
    "):\n",
    "\n",
    "    \n",
    "#     train_batches = [\n",
    "#         batch2TrainData(voc, [random.choice(train_pairs) for _ in range(batch_size)])\n",
    "#         for _ in range(n_iteration)\n",
    "#     ]\n",
    "    \n",
    "#     test_batches = [\n",
    "#         batch2TrainData(voc, [random.choice(test_pairs) for _ in range(batch_size)])\n",
    "#         for _ in range(n_iteration)\n",
    "#     ]\n",
    "\n",
    "    # Initializations\n",
    "    print(\"Initializing ...\")\n",
    "    # start_iteration = 1\n",
    "    print_loss = 0\n",
    "    print_val_loss = 0\n",
    "    liveloss = PlotLosses()\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "        print('start from',start_iteration,\"iteration\")\n",
    "        liveloss = checkpoint[\"log\"]\n",
    "    \n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = batch2TrainData(voc, [random.choice(train_pairs) for _ in range(batch_size)])\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        \n",
    "        # Ensure dropout layers are in train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(   #,propose_loss\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            teacher_forcing_ratio,\n",
    "            clip,\n",
    "            #cal_method=cal_method\n",
    "        )\n",
    "        \n",
    "\n",
    "        print_loss += loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_batch = batch2TrainData(voc, [random.choice(test_pairs) for _ in range(batch_size)])\n",
    "            # Extract fields from batch\n",
    "            input_variable, lengths, target_variable, mask, max_target_len = test_batch\n",
    "            # Ensure dropout layers are in eval mode\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            \n",
    "            # Run a training iteration with batch\n",
    "            val_loss = test(     #,val_propose_loss\n",
    "                input_variable,\n",
    "                lengths,\n",
    "                target_variable,\n",
    "                mask,\n",
    "                max_target_len,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                embedding,\n",
    "                batch_size,\n",
    "                #cal_method=cal_method\n",
    "            )\n",
    "\n",
    "            print_val_loss += val_loss\n",
    "        \n",
    "        log={}\n",
    "        log['loss'] = loss\n",
    "        log['val_loss'] = val_loss\n",
    "#         log['vector_distance'] = propose_loss\n",
    "#         log['val_vector_distance'] = val_propose_loss\n",
    "        liveloss.update(log)\n",
    "        if iteration % print_every == 0:\n",
    "            liveloss.draw()\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print_val_loss_avg = print_val_loss / print_every\n",
    "#             print(\n",
    "#                 \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
    "#                     iteration, iteration / n_iteration * 100, print_loss_avg\n",
    "#                 )\n",
    "#             )\n",
    "            with open('loss_logs','a') as f:\n",
    "                f.write(\"\\nIteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}; Average validation loss: {:.4f}\".format(\n",
    "                    iteration, iteration / n_iteration * 100, print_loss_avg, print_val_loss_avg\n",
    "                )\n",
    "                       )\n",
    "            \n",
    "            print_loss = 0\n",
    "            print_val_loss = 0\n",
    "            \n",
    "            graph_x = range(len(liveloss.logs))\n",
    "            pdf = PdfPages('loss_logs.pdf')\n",
    "            plt.figure()\n",
    "            plt.plot(graph_x, [i['loss'] for i in liveloss.logs])\n",
    "            plt.plot(graph_x, [i['val_loss'] for i in liveloss.logs])\n",
    "            plt.legend(('training','validation'))\n",
    "            pdf.savefig()\n",
    "            pdf.close()\n",
    "            \n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                \"{}-{}_{}_{}\".format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"log\": liveloss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, \"{}_{}.tar\".format(iteration, \"checkpoint\")),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ちゃんと目的のコーパスファイルを読み込んでいるか確認\n",
      "train size 1199528\n",
      "example [['▁おまえ さ 、 加工 するな や', '▁ したら こうなる'], ['▁ 声かけ られた 17 ゆー た w', '▁草 w']]\n",
      "test size 299883\n",
      "example [['▁それは ひどい w', '▁ すね 毛 は 嫌や もん ! w'], ['▁今日 や すみ', '▁なるほど']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load batches for each iteration\n",
    "# train_pairs,test_pairs = train_test_split(pairs ,test_size=0.2)\n",
    "# with open('data/train_test_pairs.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_pairs,test_pairs), f)\n",
    "with open(\"data/train_test_pairs.pickle\", \"rb\") as f:\n",
    "    train_pairs,test_pairs = pickle.load(f)\n",
    "    \n",
    "print(\"ちゃんと目的のコーパスファイルを読み込んでいるか確認\")\n",
    "print(\"train size\",len(train_pairs))\n",
    "print(\"example\",train_pairs[:2])\n",
    "print(\"test size\",len(test_pairs))\n",
    "print(\"example\",test_pairs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "self.vのパラメタNaNの数 tensor(0)\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "\n",
    "# Configure models\n",
    "model_name = \"normal_model\"\n",
    "# attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "attn_model = \"concat\"\n",
    "hidden_size = 1024\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 1024\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 12000#100000\n",
    "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      "training   (min:    6.270, max:    9.923, cur:    6.316)\n",
      "validation (min:    6.336, max:    9.786, cur:    6.430)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.2####\n",
    "learning_rate = 0.0001#学習率を0.001とか大きめにとると（大きく無いと思うけど）途中でnanを吐き始める\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 500000\n",
    "print_every = 500\n",
    "save_every = 1000\n",
    "#cal_method='cos'\n",
    "print('the number of data is',len(pairs))\n",
    "\n",
    "\n",
    "# Initialize optimizers\n",
    "print(\"Building optimizers ...\")\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "    #cal_method=cal_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価するときだけ\n",
    "def prepare_model(tf_ratio=0.2,\n",
    "                  model_name='normal_model',\n",
    "                  h_size=1024,\n",
    "                  e_layer=4,\n",
    "                  d_layer=4,\n",
    "                  check_point=20000):\n",
    "    teacher_forcing_ratio = tf_ratio####\n",
    "    # Configure models\n",
    "    #model_name = \"cos_propose_model\"\n",
    "    model_name = model_name\n",
    "    # attn_model = 'dot'\n",
    "    # attn_model = 'general'\n",
    "    attn_model = \"concat\"\n",
    "    #hidden_size = 1024\n",
    "    hidden_size = h_size\n",
    "    encoder_n_layers = e_layer\n",
    "    decoder_n_layers = d_layer\n",
    "    dropout = 0.1\n",
    "    batch_size = 512\n",
    "\n",
    "    # Set checkpoint to load from; set to None if starting from scratch\n",
    "    loadFilename = None\n",
    "    checkpoint_iter = check_point\n",
    "    loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                               '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                               '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "    # Load model if a loadFilename is provided\n",
    "    if loadFilename:\n",
    "        print(\"load save file\")\n",
    "        # If loading on same machine the model was trained on\n",
    "        checkpoint = torch.load(loadFilename)\n",
    "        # If loading a model trained on GPU to CPU\n",
    "        # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "        encoder_sd = checkpoint[\"en\"]\n",
    "        decoder_sd = checkpoint[\"de\"]\n",
    "        encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "        decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "        embedding_sd = checkpoint[\"embedding\"]\n",
    "        voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "    print(\"Building encoder and decoder ...\")\n",
    "    # Initialize word embeddings\n",
    "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "    if loadFilename:\n",
    "        embedding.load_state_dict(embedding_sd)\n",
    "    # Initialize encoder & decoder models\n",
    "    encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "    decoder = AttnDecoderRNN(\n",
    "        attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    "    )\n",
    "    if loadFilename:\n",
    "        encoder.load_state_dict(encoder_sd)\n",
    "        decoder.load_state_dict(decoder_sd)\n",
    "    # Use appropriate device\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    print(\"Models built and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "self.vのパラメタNaNの数 tensor(0)\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "\n",
    "# Configure models\n",
    "model_name = \"normal_model\"\n",
    "# attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "attn_model = \"concat\"\n",
    "hidden_size = 1024\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 3000\n",
    "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-073c4bd2d434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch2TrainData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# batch2TrainData(voc,a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-073c4bd2d434>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch2TrainData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# batch2TrainData(voc,a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "a = [random.choice(pairs) for _ in range(5)]\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batch2TrainData(voc,a)\n",
    "# batch2TrainData(voc,a)\n",
    "embedding = nn.Embedding(voc.num_words, 3)\n",
    "encoder = EncoderRNN(3, embedding, 2, 0)\n",
    "# decoder = AttnDecoderRNN(\n",
    "#     \"general\", embedding, 3, voc.num_words, 2, 0\n",
    "# )\n",
    "print(input_variable.size())\n",
    "encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "print(encoder_outputs.size(),encoder_hidden.size())\n",
    "#     # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "# decoder_input = torch.LongTensor([[SOS_token for i in range(5)]])\n",
    "# decoder_input = decoder_input\n",
    "\n",
    "# # Set initial decoder hidden state to the encoder's final hidden state\n",
    "# decoder_hidden = encoder_hidden[: 2]\n",
    "# decoder_outputs, decoder_hidden = decoder(\n",
    "#     decoder_input, decoder_hidden, encoder_outputs\n",
    "# )\n",
    "# print(decoder_outputs.size())\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# embedded = embedding(input_variable)\n",
    "# packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "# gru = nn.GRU(3,3,4,dropout=0,bidirectional=True)\n",
    "# outputs, hidden = gru(packed, None)\n",
    "# print(outputs[0].size(),\"hidden\",hidden.size())\n",
    "# outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "# print(outputs.size(),\"hidden\",hidden.size())\n",
    "# outputs = outputs[:, :, :3] + outputs[:, :, 3:]\n",
    "# print(outputs.size())\n",
    "# decoder_input = torch.LongTensor([[SOS_token for i in range(5)]])\n",
    "# decoder_hidden = hidden[:4]\n",
    "# print('decoder_hidden',decoder_hidden.size())\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# embedded = embedding(decoder_input)\n",
    "# grud = nn.GRU(3,3,4,dropout=0)\n",
    "# print(embedded.size(),decoder_hidden.size())\n",
    "# grud(embedded,decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, beam_width, n_best):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beam_width = beam_width\n",
    "        self.n_best = n_best\n",
    "        \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores\n",
    "    \n",
    "    \n",
    "import sentencepiece as spm\n",
    "segmentation_model_position = './data'\n",
    "segmentation_model_name = 'train_model8000.model'\n",
    "\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "def normalizeString(input_sentence):\n",
    "    splitSentence = ' '.join(spp.EncodeAsPieces(input_sentence))\n",
    "    return splitSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = \"\"\n",
    "    while 1:\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input(\"> \")\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == \"q\" or input_sentence == \"quit\":\n",
    "                break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            print('入力を分割すると',input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"EOS\" or x == \"PAD\")\n",
    "            ]\n",
    "            print(\"Bot:\", \" \".join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  帰りたい\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力を分割すると ▁ 帰り たい\n",
      "Bot: ぼ ぼ ぼ ぼ ぼ しく しく あげよう あげよう の為に の為に 酩酊 酩酊 したことない 曙\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  酒のみすぎたんだけど\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力を分割すると ▁ 酒 のみ すぎた んだけど\n",
      "Bot: 袁 嫌な 嫌な 嫌な 嫌な 嫌な でございます でございます 嫌な 嫌な 変えて 変えて でございます でございます 変えて\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  q\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "def cal_bleu_score(pairs, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    searcher = GreedySearchDecoder(encoder, decoder)\n",
    "    score = 0\n",
    "    for pair in tqdm(pairs):\n",
    "        input_sentence = normalizeString(pair[0])\n",
    "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "        candidate = [x for x in output_words if not (x == \"EOS\" or x == \"PAD\")]\n",
    "        reference = [pair[1].split(' ')]\n",
    "        score += sentence_bleu(reference, candidate)\n",
    "        #print(candidate,'\\n',reference,'\\n',sentence_bleu(reference, candidate))\n",
    "    return score/len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load save file\n",
      "Building encoder and decoder ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/53165 [00:00<08:26, 104.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vのパラメタNaNの数 tensor(0)\n",
      "Models built and ready to go!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53165/53165 [07:53<00:00, 112.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4206171036107889"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_model(model_name='cb_model', check_point=20000)\n",
    "cal_bleu_score(pairs, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-mikami_saturn",
   "language": "python",
   "name": "py37-mikami_saturn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
