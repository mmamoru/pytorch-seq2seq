{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)\n",
    "#device=torch.device('cpu')########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train_model32000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"train_model32000.txt\"#\"mecab_tagged.txt\"#\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLines(file, n=10):\n",
    "    with open(file, \"r\") as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for i in range(len(lines[:n])):\n",
    "        print(lines[i].split('\\t'),len(lines[i].split('\\t')[0].split()),len(lines[i].split('\\t')[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁なんと !', '▁なんと !\\n'] 2 2\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り\\n'] 2 12\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい\\n'] 13 4\n",
      "['▁まさかの 増えてる w 羨ましい', '▁ 連れ が 誰か 引いて た んですよ 、 きっと それ と交換 したんだ と思うんですけど 誰 だったのか w あ 、 たっくん は しれっと 買取 に 出しました ー\\n'] 4 25\n",
      "['▁ おち つけ よ', '▁みんな は げろ\\n'] 4 3\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !\\n'] 3 13\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w\\n'] 19 7\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w\\n'] 7 4\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい\\n'] 2 6\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真\\n'] 6 3\n"
     ]
    }
   ],
   "source": [
    "printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_name = \"train_model20000.txt\"\n",
    "# position = \"./data\"\n",
    "# corpus = os.path.join(position, corpus_name)\n",
    "# printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = len(self.index2word)  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print(\n",
    "            \"keep_words {} / {} = {:.4f}\".format(\n",
    "                len(keep_words),\n",
    "                len(self.word2index),\n",
    "                len(keep_words) / len(self.word2index),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 31,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1834337 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 34296\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい']\n",
      "['▁ おち つけ よ', '▁みんな は げろ']\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい']\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真']\n",
      "['▁ は ?', '▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な']\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "MAX_LENGTH = 20  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split(\"\\t\")] for l in lines]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(\" \")) < MAX_LENGTH and len(p[1].split(\" \")) < MAX_LENGTH\n",
    "\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus, corpus_name):#, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(corpus, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 32,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 31995 / 34293 = 0.9330\n",
      "Trimmed from 1834337 pairs to 1822478, 0.9935 of total\n",
      "Counted words in voc 31998\n",
      "Counted words in voc2: 31998\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 28#22#  # Minimum word count threshold for trimming\n",
    "\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(\" \"):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\n",
    "        \"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
    "            len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
    "        )\n",
    "    )\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "voc2 = Voc(corpus_name)\n",
    "for pair in pairs:\n",
    "    voc2.addSentence(pair[0])\n",
    "    voc2.addSentence(pair[1])\n",
    "print(\"Counted words in voc\", voc.num_words)\n",
    "#vocはコーパスに対して十分な語彙の辞書、コーパスに無い単語も含まれてる。\n",
    "#(vocを作成後、出現頻度の低い単語を削除し、削除された単語を含む文章をコーパスから削除したため。)\n",
    "print(\"Counted words in voc2:\", voc2.num_words)   \n",
    "#voc2はコーパスに対して必要十分な語彙の辞書、コーパス無いの単語全てを含んでいて、辞書内の単語は必ずコーパスに登場する。\n",
    "#(vocのように削除された単語を含む文章をコーパスから削除した後、その新しいコーパスから作成したのがvoc2だから。)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 33,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(\" \")] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "\n",
    "####zip_longest(*[[あ,い,う,え,お],[か,き,く,け,こ,さ]])→ (あ,か)(い,き)(う,く)(え,け)(お,こ)(pad,さ)　にしてくれる\n",
    "\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)  # Byte?\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "# print(\"input_variable:\", input_variable)\n",
    "# print(\"lengths:\", lengths)\n",
    "# print(\"target_variable:\", target_variable)\n",
    "# print(\"mask:\", mask)\n",
    "# print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 34,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "\n",
    "        # Unpack padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, : self.hidden_size] + outputs[:, :, self.hidden_size :]\n",
    "        # [T x B x *]\n",
    "        # T is the length of the longest sequence\n",
    "        # B is the batch size\n",
    "        # * is any number of dimensions\n",
    "        \n",
    "        # hidden.size()=[(layer x numDirection) x B x *]\n",
    "        \n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 35,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method not in [\"dot\", \"general\", \"concat\"]:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "\n",
    "        elif self.method == \"general\":\n",
    "            self.attn = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "\n",
    "        elif self.method == \"concat\":\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "            print('self.vのパラメタNaNの数',torch.sum(torch.isnan(self.v)))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_outputs):\n",
    "        return torch.sum(encoder_outputs * hidden, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.attn(\n",
    "            torch.cat(\n",
    "                (hidden.expand(encoder_outputs.size(0), -1, -1), encoder_outputs), dim=2\n",
    "            )\n",
    "        ).tanh()\n",
    "        \n",
    "        return torch.sum(attn_energies * self.v, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_outputs):\n",
    "        energy = self.attn(encoder_outputs)\n",
    "        return torch.sum(energy * hidden, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == \"general\":\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "\n",
    "        elif self.method == \"dot\":\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "            \n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 36,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('attn_energies',torch.sum(torch.isnan(attn_energies)),attn_energies)####2\n",
    "#         a = attn_energies * self.v\n",
    "#         print('a',a.size(),a)###\n",
    "#         c = torch.sum(torch.isnan(a))\n",
    "#         b= torch.sum(a,dim=2)\n",
    "#         print('c num の数',c,'b',b)####\n",
    "#         return b\n",
    "\n",
    "#print('attn_energies2',attn_energies)###1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 37,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [random.choice(pairs) for _ in range(5)]\n",
    "# input_variable, lengths, target_variable, mask, max_target_len = batch2TrainData(voc,a)\n",
    "# batch2TrainData(voc,a)\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# encoder = EncoderRNN(3, embedding, 2, 0)\n",
    "# decoder = AttnDecoderRNN(\n",
    "#     \"general\", embedding, 3, voc.num_words, 2, 0\n",
    "# )\n",
    "# encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "#     # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "# decoder_input = torch.LongTensor([[SOS_token for i in range(5)]])\n",
    "# decoder_input = decoder_input\n",
    "\n",
    "# # Set initial decoder hidden state to the encoder's final hidden state\n",
    "# decoder_hidden = encoder_hidden[: 2]\n",
    "# decoder_outputs, decoder_hidden = decoder(\n",
    "#     decoder_input, decoder_hidden, encoder_outputs\n",
    "# )\n",
    "# print(decoder_outputs.size())\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# embedded = embedding(input_variable)\n",
    "# packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "# gru = nn.GRU(3,3,4,dropout=0,bidirectional=True)\n",
    "# outputs, hidden = gru(packed, None)\n",
    "# print(outputs[0].size(),\"hidden\",hidden.size())\n",
    "# outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "# print(outputs.size(),\"hidden\",hidden.size())\n",
    "# outputs = outputs[:, :, :3] + outputs[:, :, 3:]\n",
    "# print(outputs.size())\n",
    "# decoder_input = torch.LongTensor([[SOS_token for i in range(5)]])\n",
    "# decoder_hidden = hidden[:4]\n",
    "# print('decoder_hidden',decoder_hidden.size())\n",
    "# embedding = nn.Embedding(voc.num_words, 3)\n",
    "# embedded = embedding(decoder_input)\n",
    "# grud = nn.GRU(3,3,4,dropout=0)\n",
    "# print(embedded.size(),decoder_hidden.size())\n",
    "# rnn_output, hidden = grud(embedded,decoder_hidden)\n",
    "# print('rnn_output=',rnn_output.size(),'hidden=',hidden.size())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 38,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, attn_model, embeding, hidden_size, output_size, n_layers=1, dropout=0.1\n",
    "    ):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embeding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=(0 if n_layers == 1 else dropout),\n",
    "        )\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # rnn_output.Size() = [1(sequenceLength) x B x *]\n",
    "        # hidden.Size() = [layer x B x *]\n",
    "        \n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding of current input word.\n",
    "Forward through unidirectional GRU.\n",
    "Calculate attention weights from the current GRU output from (2).\n",
    "Multiply attention weights to encoder outputs to get new “weighted sum” context vector.\n",
    "Concatenate weighted context vector and GRU output using Luong eq. 5.\n",
    "Predict next word using Luong eq. 6 (without softmax).\n",
    "Return output and final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)(1,3)\n",
    "last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)(1,3,2)\n",
    "encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 39,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1834337 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 34296\n",
      "keep_words 31995 / 34293 = 0.9330\n",
      "Trimmed from 1834337 pairs to 1822478, 0.9935 of total\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい']\n",
      "['▁ おち つけ よ', '▁みんな は げろ']\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい']\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真']\n",
      "['▁ は ?', '▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な']\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 28#22#\n",
    "print(corpus_name)\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 40,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "def focalLoss(inp, target, mask, gamma=2, eps=1e-7):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)\n",
    "    crossEntropy = -torch.log(logit)# cross entropy\n",
    "    loss = crossEntropy * (1 - logit) ** gamma # focal loss\n",
    "    loss = loss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def propose_loss_cal(\n",
    "#     generate_hidden, \n",
    "#     target_variable, \n",
    "#     max_target_len, \n",
    "#     decoder, \n",
    "#     encoder_outputs, \n",
    "#     encoder_hidden,\n",
    "#     mode,\n",
    "#     cal_method='mse',\n",
    "# ):\n",
    "#     with torch.no_grad():\n",
    "#         decoder.eval()\n",
    "#         # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "#         decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "#         decoder_input = decoder_input.to(device)\n",
    "\n",
    "#         # Set initial decoder hidden state to the encoder's final hidden state\n",
    "#         decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "\n",
    "#         for t in range(max_target_len):\n",
    "#             decoder_outputs, decoder_hidden = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs\n",
    "#             )\n",
    "#     if mode:\n",
    "#         decoder.train()\n",
    "#     target_hidden = decoder_hidden\n",
    "    \n",
    "#     if cal_method == '':\n",
    "#         None\n",
    "#     elif cal_method == 'mse':\n",
    "#         propose_loss = F.mse_loss(generate_hidden, target_hidden, reduction='mean')\n",
    "#     elif cal_method == 'cos':\n",
    "#         propose_loss = F.cosine_embedding_loss(generate_hidden, target_hidden, torch.tensor(1).float().to(device), reduction='mean')\n",
    "#     return propose_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    clip,\n",
    "    #cal_method,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "    \n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = focalLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_outputs.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = focalLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "#     propose_loss = propose_loss_cal(\n",
    "#         decoder_hidden, \n",
    "#         target_variable, \n",
    "#         max_target_len, \n",
    "#         decoder,\n",
    "#         encoder_outputs[: decoder.n_layers],\n",
    "#         encoder_hidden,\n",
    "#         decoder.training,\n",
    "#         cal_method=cal_method,\n",
    "#     )\n",
    "    \n",
    "#     loss += propose_loss*max_target_len #重み付けしてない\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    # print(_)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len\n",
    "#lossはたぶん1単語あたりのcross entropy lossの値"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 43,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    batch_size,\n",
    "    #cal_method,\n",
    "    max_length=MAX_LENGTH,\n",
    "):\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    \n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    \n",
    "\n",
    "    for t in range(max_target_len):\n",
    "        decoder_outputs, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        # No teacher forcing: next input is decoder's own current output\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        # Calculate and accumulate loss\n",
    "        mask_loss, nTotal = focalLoss(\n",
    "            decoder_outputs, target_variable[t], mask[t]\n",
    "        )\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item() * nTotal)\n",
    "        n_totals += nTotal\n",
    "    \n",
    "#     propose_loss = propose_loss_cal(\n",
    "#         decoder_hidden, \n",
    "#         target_variable, \n",
    "#         max_target_len, \n",
    "#         decoder,\n",
    "#         encoder_outputs[: decoder.n_layers],\n",
    "#         encoder_hidden,\n",
    "#         decoder.training,\n",
    "#         cal_method=cal_method,\n",
    "#     )\n",
    "#     loss += propose_loss*max_target_len #重み付けしてない\n",
    "\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "def trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "    #cal_method='mse',\n",
    "    start_iteration=1,\n",
    "):\n",
    "\n",
    "    \n",
    "#     train_batches = [\n",
    "#         batch2TrainData(voc, [random.choice(train_pairs) for _ in range(batch_size)])\n",
    "#         for _ in range(n_iteration)\n",
    "#     ]\n",
    "    \n",
    "#     test_batches = [\n",
    "#         batch2TrainData(voc, [random.choice(test_pairs) for _ in range(batch_size)])\n",
    "#         for _ in range(n_iteration)\n",
    "#     ]\n",
    "\n",
    "    # Initializations\n",
    "    print(\"Initializing ...\")\n",
    "    # start_iteration = 1\n",
    "    print_loss = 0\n",
    "    print_val_loss = 0\n",
    "    liveloss = PlotLosses()\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "        print('start from',start_iteration,\"iteration\")\n",
    "        liveloss = checkpoint[\"log\"]\n",
    "    \n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = batch2TrainData(voc, [random.choice(train_pairs) for _ in range(batch_size)])\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        \n",
    "        # Ensure dropout layers are in train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(   #,propose_loss\n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            teacher_forcing_ratio,\n",
    "            clip,\n",
    "            #cal_method=cal_method\n",
    "        )\n",
    "        \n",
    "\n",
    "        print_loss += loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_batch = batch2TrainData(voc, [random.choice(test_pairs) for _ in range(batch_size)])\n",
    "            # Extract fields from batch\n",
    "            input_variable, lengths, target_variable, mask, max_target_len = test_batch\n",
    "            # Ensure dropout layers are in eval mode\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            \n",
    "            # Run a training iteration with batch\n",
    "            val_loss = test(     #,val_propose_loss\n",
    "                input_variable,\n",
    "                lengths,\n",
    "                target_variable,\n",
    "                mask,\n",
    "                max_target_len,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                embedding,\n",
    "                batch_size,\n",
    "                #cal_method=cal_method\n",
    "            )\n",
    "\n",
    "            print_val_loss += val_loss\n",
    "        \n",
    "        log={}\n",
    "        log['loss'] = loss\n",
    "        log['val_loss'] = val_loss\n",
    "#         log['vector_distance'] = propose_loss\n",
    "#         log['val_vector_distance'] = val_propose_loss\n",
    "        liveloss.update(log)\n",
    "        if iteration % print_every == 0:\n",
    "            liveloss.draw()\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print_val_loss_avg = print_val_loss / print_every\n",
    "#             print(\n",
    "#                 \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
    "#                     iteration, iteration / n_iteration * 100, print_loss_avg\n",
    "#                 )\n",
    "#             )\n",
    "            with open(voc.name[:-4]+'_logs','a') as f:\n",
    "                f.write(\"\\nIteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}; Average validation loss: {:.4f}\".format(\n",
    "                    iteration, iteration / n_iteration * 100, print_loss_avg, print_val_loss_avg\n",
    "                )\n",
    "                       )\n",
    "            \n",
    "            print_loss = 0\n",
    "            print_val_loss = 0\n",
    "            \n",
    "            graph_x = range(len(liveloss.logs))\n",
    "            pdf = PdfPages(voc.name[:-4]+'_logs.pdf')\n",
    "            plt.figure()\n",
    "            plt.plot(graph_x, [i['loss'] for i in liveloss.logs])\n",
    "            plt.plot(graph_x, [i['val_loss'] for i in liveloss.logs])\n",
    "            plt.legend(('training','validation'))\n",
    "            pdf.savefig()\n",
    "            pdf.close()\n",
    "            \n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                \"{}-{}_{}_{}\".format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"log\": liveloss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, \"{}_{}.tar\".format(iteration, \"checkpoint\")),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "ちゃんと目的のコーパスファイルを読み込んでいるか確認\n",
      "train size 1457982\n",
      "example [['▁ 問い合わせ w 間違いない ですね w', '▁ して はいない んですけどね w それも 運 なの だろう'], ['▁ 珈琲 とか ウイスキー 用', '▁ 娘 用']]\n",
      "test size 364496\n",
      "example [['▁ ピカチュウ は かわいい でも カビゴン はもっと かわいい', '▁いやあ ずみ のほうが'], ['▁ スカ トロ とか やめてくれ だめ や', '▁そこまで 言って ねーよ w それは あかん']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# #Load batches for each iteration\n",
    "# train_pairs,test_pairs = train_test_split(pairs ,test_size=0.2)\n",
    "# with open('data/train_test_pairs_mecab.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_pairs,test_pairs), f)\n",
    "with open(\"data/train_test_pairs_32k.pickle\", \"rb\") as f:\n",
    "    train_pairs,test_pairs = pickle.load(f)\n",
    " \n",
    "print(voc.name)\n",
    "print(\"ちゃんと目的のコーパスファイルを読み込んでいるか確認\")\n",
    "print(\"train size\",len(train_pairs))\n",
    "print(\"example\",train_pairs[:2])\n",
    "print(\"test size\",len(test_pairs))\n",
    "print(\"example\",test_pairs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "print(voc.name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 1024\n",
    "teacher_forcing_ratio = 0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 27000#100000\n",
    "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パラメータ数をカウント\n",
      "encoderのパラメータ数\n",
      "12529152\n",
      "decoderパラメータ数\n",
      "18125310\n"
     ]
    }
   ],
   "source": [
    "# パラメータカウント関数\n",
    "def parameters_count(net):\n",
    "    params = 0\n",
    "    for p in net.parameters():\n",
    "        #print(p)\n",
    "        #print(p.numel())\n",
    "        if p.requires_grad:\n",
    "            params += p.numel()\n",
    "        \n",
    "    print(params)  # 121898\n",
    "\n",
    "print(\"パラメータ数をカウント\")\n",
    "print(\"encoderのパラメータ数\")\n",
    "parameters_count(encoder)\n",
    "print(\"decoderパラメータ数\")\n",
    "parameters_count(decoder)\n",
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\n",
      "training   (min:    6.758, max:   10.364, cur:    6.838)\n",
      "validation (min:    6.709, max:   10.351, cur:    6.851)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.2####\n",
    "learning_rate = 0.0001#学習率を0.001とか大きめにとると（大きく無いと思うけど）途中でnanを吐き始める\n",
    "decoder_learning_ratio = 2.0\n",
    "n_iteration = 500000\n",
    "print_every = 500\n",
    "save_every = 1000\n",
    "#cal_method='cos'\n",
    "print('the number of data is',len(pairs))\n",
    "\n",
    "\n",
    "# Initialize optimizers\n",
    "print(\"Building optimizers ...\")\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"count parameters\")\n",
    "\n",
    "print(\"Starting Training!\")\n",
    "trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "    #cal_method=cal_method\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 74,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "self.vのパラメタNaNの数 tensor(0)\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "print(corpus_name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
<<<<<<< HEAD
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
=======
    "attn_model = \"concat\"\n",
    "hidden_size = 1024\n",
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "teacher_forcing_ratio=0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
<<<<<<< HEAD
    "checkpoint_iter = 36000\n",
=======
    "checkpoint_iter = 233000\n",
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
=======
   "execution_count": 75,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return [all_tokens], [-torch.sum(torch.log(all_scores).detach().cpu())/float(max_length - 1 + 1e-6)]\n",
    "    \n",
    "    \n",
    "import sentencepiece as spm\n",
    "segmentation_model_position = './data'\n",
    "segmentation_model_name = 'train_model32000.model'\n",
    "\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "def normalizeString(input_sentence):\n",
    "    splitSentence = ' '.join(spp.EncodeAsPieces(input_sentence))\n",
    "    return splitSentence\n",
    "\n",
    "# import MeCab\n",
    "# tagger = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/\")\n",
    "\n",
    "# def normalizeString(input_sentence):\n",
    "#     splitSentence = ' '.join([ele.split(\"\\t\")[0] for ele in tagger.parse(input_sentence).split(\"\\n\")[:-2]])\n",
    "#     return splitSentence"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 76,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, beam_width, n_best):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beam_width = beam_width\n",
    "        self.n_best = n_best\n",
    "        \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        #decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        \n",
    "        class BeamSearchNode(object):\n",
    "            def __init__(self, hid, prevNode, wordId, logP, length):\n",
    "                self.hid = hid\n",
    "                self.prevNode = prevNode\n",
    "                self.wordId = wordId\n",
    "                self.logP = logP\n",
    "                self.length = length\n",
    "                \n",
    "            def eval(self, alpha=1.0):\n",
    "                reward = 0\n",
    "                # Add here a function for shaping a reward\n",
    "\n",
    "                return self.logP / float(self.length - 1 + 1e-6) + alpha * reward\n",
    "          \n",
    "        \n",
    "        n_best_batch_list = []\n",
    "        n_best_batch_score_list = []\n",
    "        for batchNum in range(encoder_hidden.size(1)):\n",
    "            \n",
    "            decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
    "            # 一つ前の状態の隠れベクトル、単語をNodeを保持するNodeを生成\n",
    "            node = BeamSearchNode(hid=decoder_hidden[:,batchNum,:].unsqueeze(1), prevNode=None, wordId=decoder_input, logP=0, length=1)\n",
    "            nextNodes=[]\n",
    "            \n",
    "            nextNodes.append((-node.eval(), id(node), node))\n",
    "            n_dec_steps = 0\n",
    "            while True:\n",
    "                \n",
    "                nodes = [sorted(nextNodes)[inode] for inode in range(len(nextNodes)) if inode<self.beam_width]\n",
    "                nextNodes = []\n",
    "                end_node = []\n",
    "                for beamNum in range(self.beam_width):\n",
    "                    if len(nodes)<=0:\n",
    "                        break\n",
    "                    #今から探索するNodeを取得\n",
    "                    score, _, n = nodes.pop(0)\n",
    "                    decoder_input = n.wordId\n",
    "                    decoder_hidden = n.hid\n",
    "                    \n",
    "                    if n.wordId[0][0].item()!=EOS_token and n.wordId[0][0].item()!=PAD_token and n.length<=max_length:\n",
    "                        decoder_output, decoder_hidden = self.decoder(\n",
    "                            decoder_input, decoder_hidden, encoder_outputs\n",
    "                        )\n",
    "\n",
    "                        topk_prob, topk_indexes = torch.topk(decoder_output, self.beam_width) \n",
    "\n",
    "                        for new_k in range(self.beam_width):\n",
    "                            decoded_t = topk_indexes[0][new_k].view(1,1) # (1)\n",
    "                            logp = torch.log(topk_prob[0][new_k]).item() # float log probability val\n",
    "\n",
    "                            node = BeamSearchNode(hid=decoder_hidden,\n",
    "                                                  prevNode=n,\n",
    "                                                  wordId=decoded_t,\n",
    "                                                  logP=n.logP+logp,\n",
    "                                                  length=n.length+1)\n",
    "                            nextNodes.append((-node.eval(), id(node), node))\n",
    "                            \n",
    "                            \n",
    "                    else:\n",
    "                        node = BeamSearchNode(hid=decoder_hidden,\n",
    "                                                  prevNode=n,\n",
    "                                                  wordId=torch.LongTensor([[PAD_token]]).to(device),\n",
    "                                                  logP=n.logP,\n",
    "                                                  length=n.length)\n",
    "                        nextNodes.append((-node.eval(), id(node), node))\n",
    "                        end_node.append(1)\n",
    "                if len(end_node)>=self.beam_width:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "        \n",
    "            if len(nextNodes)!=self.beam_width:\n",
    "                print('assert not match beam width and nextNode length')\n",
    "                \n",
    "            n_best_seq_list = []\n",
    "            n_best_score_list = []\n",
    "            for score, _id, n in sorted(nextNodes):\n",
    "                sequence = [n.wordId.item()]\n",
    "                n_best_score_list.append(score)\n",
    "                # back trace from end node\n",
    "                while n.prevNode is not None:\n",
    "                    n = n.prevNode\n",
    "                    sequence.append(n.wordId.item())\n",
    "                    \n",
    "                sequence = sequence[::-1] # reverse\n",
    "                n_best_seq_list.append(sequence)\n",
    "        \n",
    "            n_best_seq_list = n_best_seq_list[::-1]\n",
    "            n_best_score_list = n_best_score_list[::-1]\n",
    "            \n",
    "            n_best_batch_list.append(n_best_seq_list)\n",
    "            n_best_batch_score_list.append(n_best_score_list)\n",
    "            \n",
    "        n_best_batch_list = torch.tensor(n_best_batch_list)\n",
    "        n_best_batch_score_list = torch.tensor(n_best_batch_score_list)\n",
    "            \n",
    "            #batchを無視して今回は出力することにした\n",
    "        return n_best_batch_list[0], n_best_batch_score_list[0]\n",
    "    \n",
    "# import sentencepiece as spm\n",
    "# segmentation_model_position = './data'\n",
    "# segmentation_model_name = 'train_model20000.model'\n",
    "\n",
    "# spp = spm.SentencePieceProcessor()\n",
    "# spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "# def normalizeString(input_sentence):\n",
    "#     splitSentence = ' '.join(spp.EncodeAsPieces(input_sentence))\n",
    "#     return splitSentence"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 77,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    seqs, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    for iseqs in range(len(seqs)):\n",
    "        tokens = seqs[iseqs]\n",
    "    \n",
    "        # indexes -> words\n",
    "        decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "        print(scores[iseqs],decoded_words)\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, searcher1, voc):\n",
    "    input_sentence = \"\"\n",
    "    while 1:\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input(\"> \")\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == \"q\" or input_sentence == \"quit\":\n",
    "                break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            print('\\n入力を分割すると',input_sentence+'\\n')\n",
    "            \n",
    "            # Evaluate sentence with seacher\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"EOS\" or x == \"PAD\")\n",
    "            ]\n",
    "            print(\"\\nBot(BeamSearch):\", \" \".join(output_words))\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Evaluate sentence with seacher1\n",
    "            output_words = evaluate(encoder, decoder, searcher1, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"EOS\" or x == \"PAD\")\n",
    "            ]\n",
    "            print(\"\\nBot(GreedySearch):\", \" \".join(output_words)+\"\\n\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['なんと!\\n',\n",
       " 'なんと!\\n',\n",
       " '意味:都を移すこと、都移り\\n',\n",
       " 'わたし安定のたっくんですよー気づいたらたっくんいないし、いつきあるし誰と交換したのか\\n',\n",
       " 'まさかの増えてるw羨ましい\\n',\n",
       " '連れが誰か引いてたんですよ、きっとそれと交換したんだと思うんですけど誰だったのかwあ、たっくんはしれっと買取に出しましたー\\n',\n",
       " 'おちつけよ\\n',\n",
       " 'みんなはげろ\\n',\n",
       " '昨日髪の毛切ったばっかりだけどこのくらいつるっパゲになってきます!\\n',\n",
       " '今は古すぎて廃盤のものが多いので、いいモンスターが出るのだと高い値で売れますw\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/processed_ja_file.txt','r') as file:\n",
    "    bun = file.readlines()\n",
    "bun[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
=======
   "execution_count": 78,
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  おはよう\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁おはよう\n",
<<<<<<< HEAD
      "\n",
      "tensor(1.6807) ['SOS', '▁おはよー', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.6797) ['SOS', '▁おはようございます', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.6356) ['SOS', '▁おはよう', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.5915) ['SOS', '▁おはよう', '!', 'EOS', 'PAD']\n",
      "tensor(1.4366) ['SOS', '▁', 'EOS', 'PAD', 'PAD']\n",
=======
      "tensor(1.8866) ['SOS', '▁', 'きみ', 'EOS', 'PAD']\n",
      "tensor(1.6138) ['SOS', '▁', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.5386) ['SOS', '▁おはよう', '!', 'EOS', 'PAD']\n",
      "tensor(1.4312) ['SOS', '▁おはよう', 'w', 'EOS', 'PAD']\n",
      "tensor(1.4100) ['SOS', '▁おはよう', 'EOS', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁おはよう\n",
      "\n",
      "tensor(1.1211, device='cuda:0') ['▁おはよう', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'よくない', 'ぞ', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁おはよう よくない ぞ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  テニスやりたくね\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力を分割すると ▁ テニス やりたく ね\n",
      "tensor(0.8257) ['SOS', '▁', 'すてき', '!', '楽しかった', 'ね', 'EOS', 'PAD', 'PAD']\n",
      "tensor(0.7715) ['SOS', '▁', 'すてき', '!', '楽しかった', 'よー', '!', 'EOS', 'PAD']\n",
      "tensor(0.7574) ['SOS', '▁', 'すてき', '!', '楽しかった', 'ね', 'w', 'EOS', 'PAD']\n",
      "tensor(0.7082) ['SOS', '▁', '楽しかった', '!', '楽しかった', 'ね', '!', 'EOS', 'PAD']\n",
      "tensor(0.6431) ['SOS', '▁', 'すてき', '!', '楽しかった', 'ね', '!', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ すてき ! 楽しかった ね !\n",
      "\n",
      "tensor(0.5110, device='cuda:0') ['▁', 'すてき', '!', '楽しかった', 'ね', '!', 'EOS', 'EOS', 'まくって', 'まくって', 'けど', 'した', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ すてき ! 楽しかった ね ! まくって まくって けど した\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  なにが楽しかった？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力を分割すると ▁なにが 楽しかった ?\n",
      "tensor(0.9067) ['SOS', '▁', '作品', 'に', '袋', 'しよう', 'みんなの', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(0.8629) ['SOS', '▁', '作品', 'に', '袋', 'しよう', 'みんなの', 'みんなの', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(0.7686) ['SOS', '▁', '作品', 'に', '袋', 'しよう', 'みんなの', 'みんなの', 'お', 'ー', 'EOS', 'PAD']\n",
      "tensor(0.7481) ['SOS', '▁', '作品', '楽しかった', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(0.5188) ['SOS', '▁', '作品', 'に', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ 作品 に\n",
      "\n",
      "tensor(0.4429, device='cuda:0') ['▁', '作品', 'に', 'EOS', 'EOS', 'みんなの', 'みんなの', 'EOS', 'ー', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ 作品 に みんなの みんなの ー\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  昨日なにしてた？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力を分割すると ▁昨日 なに してた ?\n",
      "tensor(0.7645) ['SOS', '▁', 'シージ', 'ます', 'るー', 'EOS', 'PAD']\n",
      "tensor(0.7110) ['SOS', '▁', 'シージ', 'ます', 'EOS', 'PAD', 'PAD']\n",
      "tensor(0.7043) ['SOS', '▁', 'シージ', 'ます', 'ます', 'EOS', 'PAD']\n",
      "tensor(0.6622) ['SOS', '▁', 'シージ', 'ます', '!', 'EOS', 'PAD']\n",
      "tensor(0.5107) ['SOS', '▁', 'シージ', 'ます', 'w', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ シージ ます w\n",
      "\n",
      "tensor(0.3289, device='cuda:0') ['▁', 'シージ', 'ます', 'w', 'EOS', 'EOS', 'EOS', 'EOS', 'してた', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ シージ ます w してた\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  暇すぎてなんもやることない\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力を分割すると ▁ 暇すぎて なんも やること ない\n",
      "tensor(1.4127) ['SOS', '▁', 'w', '寝ちゃう', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.3755) ['SOS', '▁はい', 'w', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.3738) ['SOS', '▁大丈夫', 'w', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.0508) ['SOS', '▁', 'w', '寝ちゃう', 'w', 'EOS', 'PAD']\n",
      "tensor(0.9921) ['SOS', '▁', 'w', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ w\n",
      "\n",
      "tensor(0.4861, device='cuda:0') ['▁', 'w', 'EOS', 'EOS', 'EOS', 'EOS', '寝る', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ w 寝る\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  海水パンツ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力を分割すると ▁海 水 パンツ\n",
      "tensor(0.8552) ['SOS', '▁そして', 'オイ', 'してる', '追', 'ト', 'してる', 'EOS', 'PAD']\n",
      "tensor(0.7886) ['SOS', '▁そして', '鬼', 'してる', '追', '追', 'してる', 'EOS', 'PAD']\n",
      "tensor(0.7772) ['SOS', '▁水', '川', 'してる', '追', '追', 'してる', 'EOS', 'PAD']\n",
      "tensor(0.7299) ['SOS', '▁そして', 'オイ', 'してる', '追', '追', 'してる', 'EOS', 'PAD']\n",
      "tensor(0.7117) ['SOS', '▁そして', 'w', 'してる', '追', '追', 'してる', 'EOS', 'PAD']\n",
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
      "\n",
      "Bot(BeamSearch): SOS ▁そして w してる 追 追 してる\n",
      "\n",
<<<<<<< HEAD
      "tensor(1.1157) ['▁', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁\n",
      "\n"
=======
      "tensor(0.3818, device='cuda:0') ['▁そして', 'オイ', 'してる', '追', '追', 'してる', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁そして オイ してる 追 追 してる\n"
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      ">  研究室行きたい\n"
=======
      ">  海行きたくないですか？\n"
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "\n",
      "入力を分割すると ▁ 研究 室 行きたい\n",
      "\n",
      "tensor(2.3737) ['SOS', '▁', '、', 'EOS', 'PAD']\n",
      "tensor(2.3525) ['SOS', '▁', '!', 'EOS', 'PAD']\n",
      "tensor(2.3408) ['SOS', '▁', 'EOS', 'PAD', 'PAD']\n",
      "tensor(2.3231) ['SOS', '▁', 'w', 'EOS', 'PAD']\n",
      "tensor(2.2988) ['SOS', '▁', '?', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ ?\n",
      "\n",
      "tensor(1.6352) ['▁', 'w', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ w\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  なんでやんねん\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁なんでや んねん\n",
      "\n",
      "tensor(2.2624) ['SOS', '▁', '、', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(2.2099) ['SOS', '▁え', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(2.1931) ['SOS', '▁', 'w', 'w', 'w', 'EOS', 'PAD']\n",
      "tensor(2.1411) ['SOS', '▁', 'w', 'w', 'EOS', 'PAD', 'PAD']\n",
      "tensor(2.0777) ['SOS', '▁', 'w', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁ w\n",
      "\n",
      "tensor(1.5885) ['▁', 'w', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ w\n",
      "\n"
=======
      "入力を分割すると ▁海 行きたくない ですか ?\n",
      "tensor(1.1180) ['SOS', '▁', '公園', '公園', '田', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.0826) ['SOS', '▁', '公園', 'ー', '田', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.0423) ['SOS', '▁海', '公園', 'ー', '田', '千葉', 'の', '公園', 'です', 'れん', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(0.9760) ['SOS', '▁海', '公園', 'ー', '田', '千葉', 'の', '公園', 'です', '公園', 'です', 'れん', 'EOS', 'PAD', 'PAD']\n",
      "tensor(0.8933) ['SOS', '▁海', '公園', 'ー', '田', '千葉', 'の', '公園', 'です', '公園', 'です', 'れん', 'w', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): SOS ▁海 公園 ー 田 千葉 の 公園 です 公園 です れん w\n",
      "\n",
      "tensor(0.8853, device='cuda:0') ['▁', '公園', '公園', '田', 'EOS', '公園', 'の', 'あたり', '公園', 'です', 'れん', 'w', 'EOS', 'EOS', 'EOS']\n",
      "\n",
      "Bot(GreedySearch): ▁ 公園 公園 田 公園 の あたり 公園 です れん w\n"
>>>>>>> a71cd07b0252c419942c36b0c7b8065772a693fe
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  q\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "#searcher = GreedySearchDecoder(encoder, decoder)\n",
    "searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "searcher１ = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, searcher1, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nlg-eval: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!nlg-eval --hypothesis=examples/hyp.txt --references=examples/ref1.txt --references=examples/ref2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu_1: 0.550000\n",
      "Bleu_2: 0.428174\n",
      "Bleu_3: 0.284043\n",
      "Bleu_4: 0.201143\n",
      "METEOR: 0.295797\n",
      "ROUGE_L: 0.522104\n",
      "CIDEr: 1.242192\n",
      "SkipThoughtsCosineSimilarity: 0.626149\n",
      "EmbeddingAverageCosineSimilarity: 0.884690\n",
      "EmbeddingAverageCosineSimilairty: 0.884690\n",
      "VectorExtremaCosineSimilarity: 0.568696\n",
      "GreedyMatchingScore: 0.784205\n"
     ]
    }
   ],
   "source": [
    "from nlgeval import compute_metrics\n",
    "metrics_dict = compute_metrics(hypothesis='examples/hyp.txt',\n",
    "                               references=['examples/ref1.txt', 'examples/ref2.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "def cal_bleu_score(pairs, encoder, decoder):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    searcher = GreedySearchDecoder(encoder, decoder)\n",
    "    score = 0\n",
    "    for pair in tqdm(pairs):\n",
    "        input_sentence = normalizeString(pair[0])\n",
    "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "        candidate = [x for x in output_words if not (x == \"EOS\" or x == \"PAD\")]\n",
    "        reference = [pair[1].split(' ')]\n",
    "        score += sentence_bleu(reference, candidate)\n",
    "        #print(candidate,'\\n',reference,'\\n',sentence_bleu(reference, candidate))\n",
    "    return score/len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load save file\n",
      "Building encoder and decoder ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/53165 [00:00<08:26, 104.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.vのパラメタNaNの数 tensor(0)\n",
      "Models built and ready to go!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53165/53165 [07:53<00:00, 112.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4206171036107889"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_model(model_name='cb_model', check_point=20000)\n",
    "cal_bleu_score(pairs, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  5,  3,  4, -4])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "b = [1,2,3,4,5,1,2,3,4,-1,-4]\n",
    "b = np.array(b)\n",
    "b[abs(b)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1))\n",
    "    crossEntropy = -torch.log(logit.squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "def focalLoss(inp, target, mask, gamma=1, eps=1e-7):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)\n",
    "    crossEntropy = -torch.log(logit)# cross entropy\n",
    "    loss = crossEntropy * (1 - logit) ** gamma # focal loss\n",
    "    loss = loss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "\n",
    "xitem = [i/100 for i in range(1,101)]\n",
    "for ite in [ 0.5, 1, 2, 5]:\n",
    "    focalloss = []\n",
    "    normalloss = []\n",
    "    for x in range(1,101):\n",
    "        sample = [[(1-x/100)/3, x/100, (1-x/100)/3, (1-x/100)/3]]\n",
    "        decoder_output = torch.tensor(sample)\n",
    "        mask = torch.BoolTensor([[1]])\n",
    "        target_variable = torch.LongTensor([[1]])\n",
    "        normalloss.append(maskNLLLoss(decoder_output, target_variable[0], mask[0])[0].item())\n",
    "        focalloss.append(focalLoss(decoder_output, target_variable[0], mask[0],gamma=ite)[0].item())\n",
    "        \n",
    "    plt.plot(xitem,focalloss,label=('focal gamma='+str(ite)))\n",
    "    \n",
    "plt.plot(xitem,normalloss,label=('normal'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-mikami-mercury",
   "language": "python",
   "name": "py37-mikami-mercury"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
