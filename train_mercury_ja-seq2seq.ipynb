{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)\n",
    "#device=torch.device('cpu')########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/mecab_tagged.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"mecab_tagged.txt\"#\"train_model32000.txt\"#\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Voc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['なんと !', 'なんと !\\n'] 2 2\n",
      "['なんと !', '意味 : 都 を 移す こと 、 都 移り\\n'] 2 9\n",
      "['わたし 安定 の たっくん です よー 気づい たら たっくん い ない し 、 いつき ある し 誰 と 交換 し た の か', 'まさかの 増え てる w 羨ましい\\n'] 23 5\n",
      "['まさかの 増え てる w 羨ましい', '連れ が 誰か 引い て た ん です よ 、 きっと それ と 交換 し た ん だ と 思う ん です けど 誰 だっ た の か w あ 、 たっくん は しれっと 買取 に 出し まし た ー\\n'] 5 40\n",
      "['おちつけ よ', 'みんな は げろ\\n'] 2 3\n",
      "['みんな は げろ', '昨日 髪の毛 切っ た ばっかり だ けど この くらい つるっ パゲ に なっ て き ます !\\n'] 3 17\n",
      "['今 は 古 すぎ て 廃盤 の もの が 多い ので 、 いい モンスター が 出る の だ と 高い 値 で 売れ ます w', 'あ 、 やっぱり そう 言う の ある です ね w\\n'] 25 10\n",
      "['あ 、 やっぱり そう 言う の ある です ね w', 'あり ます あり ます w\\n'] 10 5\n",
      "['や だお', '目元 は 知っ てる 、 か あ いい\\n'] 2 8\n",
      "['目元 は 知っ てる 、 か あ いい', '超 過去 写真\\n'] 8 3\n"
     ]
    }
   ],
   "source": [
    "printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab_tagged.txt\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1642304 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 255836\n",
      "keep_words 108420 / 255833 = 0.4238\n",
      "Trimmed from 1642304 pairs to 1465962, 0.8926 of total\n",
      "\n",
      "pairs:\n",
      "['なんと !', 'なんと !']\n",
      "['なんと !', '意味 : 都 を 移す こと 、 都 移り']\n",
      "['おちつけ よ', 'みんな は げろ']\n",
      "['みんな は げろ', '昨日 髪の毛 切っ た ばっかり だ けど この くらい つるっ パゲ に なっ て き ます !']\n",
      "['あ 、 やっぱり そう 言う の ある です ね w', 'あり ます あり ます w']\n",
      "['や だお', '目元 は 知っ てる 、 か あ いい']\n",
      "['目元 は 知っ てる 、 か あ いい', '超 過去 写真']\n",
      "['いつ でき ます か 。', '今 大丈夫 です !']\n",
      "['今 大丈夫 です !', 'カフェ の 内部 に 来 て ください']\n",
      "['絶対 上手いっ て ! 描い て み て', 'それ は ない ね ! ミニー ちゃん ?']\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3#28#22#\n",
    "print(corpus_name)\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab_tagged.txt\n",
      "ちゃんと目的のコーパスファイルを読み込んでいるか確認\n",
      "train size 1172769\n",
      "example [['メン 入り の ワカメスープ でし た w', 'もう それ わかめスープ で いい ん じゃ w'], ['私 も 、 あやね ちゃん の スクショ 心待ち に し てる ー', 'も少し 極める から 待っ て すでに 30枚 くらい とり まし た']]\n",
      "test size 293193\n",
      "example [['別に 痛く も 痒く も ない けど ね', 'w なんか 、 急 に かっ ちょい い w'], ['おやすみ !', 'ぐんない']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# #Load batches for each iteration\n",
    "# train_pairs,test_pairs = train_test_split(pairs ,test_size=0.2)\n",
    "# with open('data/train_test_pairs_mecab.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_pairs,test_pairs), f)\n",
    "with open(\"data/train_test_pairs_mecab3min.pickle\", \"rb\") as f:\n",
    "    #train_test_pairs_32k.pickle\n",
    "    train_pairs,test_pairs = pickle.load(f)\n",
    " \n",
    "print(voc.name)\n",
    "print(\"ちゃんと目的のコーパスファイルを読み込んでいるか確認\")\n",
    "print(\"train size\",len(train_pairs))\n",
    "print(\"example\",train_pairs[:2])\n",
    "print(\"test size\",len(test_pairs))\n",
    "print(\"example\",test_pairs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.pyのモデルの説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding of current input word.\n",
    "Forward through unidirectional GRU.\n",
    "Calculate attention weights from the current GRU output from (2).\n",
    "Multiply attention weights to encoder outputs to get new “weighted sum” context vector.\n",
    "Concatenate weighted context vector and GRU output using Luong eq. 5.\n",
    "Predict next word using Luong eq. 6 (without softmax).\n",
    "Return output and final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)(1,3)\n",
    "last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)(1,3,2)\n",
    "encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab_tagged.txt\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "print(voc.name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 1024\n",
    "teacher_forcing_ratio = 0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 72000#100000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focalLoss(inp, target, mask, gamma=2, eps=1e-7):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)\n",
    "    crossEntropy = -torch.log(logit)# cross entropy\n",
    "    loss = crossEntropy * (1 - logit) ** gamma # focal loss\n",
    "    loss = loss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    clip,\n",
    "):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "    \n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = focalLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_outputs, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_outputs.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = focalLoss(\n",
    "                decoder_outputs, target_variable[t], mask[t]\n",
    "            )\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    # print(_)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len\n",
    "#lossはたぶん1単語あたりのcross entropy lossの値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    input_variable,\n",
    "    lengths,\n",
    "    target_variable,\n",
    "    mask,\n",
    "    max_target_len,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    embedding,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for i in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "\n",
    "    \n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    \n",
    "\n",
    "    for t in range(max_target_len):\n",
    "        decoder_outputs, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        # No teacher forcing: next input is decoder's own current output\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        # Calculate and accumulate loss\n",
    "        mask_loss, nTotal = focalLoss(\n",
    "            decoder_outputs, target_variable[t], mask[t]\n",
    "        )\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item() * nTotal)\n",
    "        n_totals += nTotal\n",
    "    \n",
    "\n",
    "\n",
    "    return sum(print_losses) / n_totals#, propose_loss.item()*max_target_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "def trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    "    start_iteration=1,\n",
    "):\n",
    "\n",
    "    \n",
    "\n",
    "    # Initializations\n",
    "    print(\"Initializing ...\")\n",
    "    # start_iteration = 1\n",
    "    print_loss = 0\n",
    "    print_val_loss = 0\n",
    "    liveloss = PlotLosses()\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "        print('start from',start_iteration,\"iteration\")\n",
    "        liveloss = checkpoint[\"log\"]\n",
    "    \n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = batch2TrainData(voc, [random.choice(train_pairs) for _ in range(batch_size)])\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        \n",
    "        # Ensure dropout layers are in train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(   \n",
    "            input_variable,\n",
    "            lengths,\n",
    "            target_variable,\n",
    "            mask,\n",
    "            max_target_len,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            embedding,\n",
    "            encoder_optimizer,\n",
    "            decoder_optimizer,\n",
    "            batch_size,\n",
    "            teacher_forcing_ratio,\n",
    "            clip,\n",
    "        )\n",
    "        \n",
    "\n",
    "        print_loss += loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_batch = batch2TrainData(voc, [random.choice(test_pairs) for _ in range(batch_size)])\n",
    "            # Extract fields from batch\n",
    "            input_variable, lengths, target_variable, mask, max_target_len = test_batch\n",
    "            # Ensure dropout layers are in eval mode\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "            \n",
    "            # Run a training iteration with batch\n",
    "            val_loss = test(     #,val_propose_loss\n",
    "                input_variable,\n",
    "                lengths,\n",
    "                target_variable,\n",
    "                mask,\n",
    "                max_target_len,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                embedding,\n",
    "                batch_size,\n",
    "            )\n",
    "\n",
    "            print_val_loss += val_loss\n",
    "        \n",
    "        log={}\n",
    "        log['loss'] = loss\n",
    "        log['val_loss'] = val_loss\n",
    "#         log['vector_distance'] = propose_loss\n",
    "#         log['val_vector_distance'] = val_propose_loss\n",
    "        liveloss.update(log)\n",
    "        if iteration % print_every == 0:\n",
    "            liveloss.draw()\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print_val_loss_avg = print_val_loss / print_every\n",
    "#             print(\n",
    "#                 \"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
    "#                     iteration, iteration / n_iteration * 100, print_loss_avg\n",
    "#                 )\n",
    "#             )\n",
    "            with open(voc.name[:-4]+'_logs','a') as f:\n",
    "                f.write(\"\\nIteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}; Average validation loss: {:.4f}\".format(\n",
    "                    iteration, iteration / n_iteration * 100, print_loss_avg, print_val_loss_avg\n",
    "                )\n",
    "                       )\n",
    "            \n",
    "            print_loss = 0\n",
    "            print_val_loss = 0\n",
    "            \n",
    "            graph_x = range(len(liveloss.logs))\n",
    "            pdf = PdfPages(voc.name[:-4]+'_logs.pdf')\n",
    "            plt.figure()\n",
    "            plt.plot(graph_x, [i['loss'] for i in liveloss.logs])\n",
    "            plt.plot(graph_x, [i['val_loss'] for i in liveloss.logs])\n",
    "            plt.legend(('training','validation'))\n",
    "            pdf.savefig()\n",
    "            pdf.close()\n",
    "            \n",
    "\n",
    "        # Save checkpoint\n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(\n",
    "                save_dir,\n",
    "                model_name,\n",
    "                corpus_name,\n",
    "                \"{}-{}_{}_{}\".format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"iteration\": iteration,\n",
    "                    \"en\": encoder.state_dict(),\n",
    "                    \"de\": decoder.state_dict(),\n",
    "                    \"en_opt\": encoder_optimizer.state_dict(),\n",
    "                    \"de_opt\": decoder_optimizer.state_dict(),\n",
    "                    \"log\": liveloss,\n",
    "                    \"voc_dict\": voc.__dict__,\n",
    "                    \"embedding\": embedding.state_dict(),\n",
    "                },\n",
    "                os.path.join(directory, \"{}_{}.tar\".format(iteration, \"checkpoint\")),\n",
    "            )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パラメータ数をカウント\n",
      "encoderのパラメータ数\n",
      "32093952\n",
      "decoderパラメータ数\n",
      "57331335\n"
     ]
    }
   ],
   "source": [
    "# パラメータカウント関数\n",
    "print(\"パラメータ数をカウント\")\n",
    "print(\"encoderのパラメータ数\")\n",
    "parameters_count(encoder)\n",
    "print(\"decoderパラメータ数\")\n",
    "parameters_count(decoder)\n",
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of data is 1465962\n",
      "Building optimizers ...\n",
      "count parameters\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "start from 72001 iteration\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 424.00 MiB (GPU 0; 10.73 GiB total capacity; 9.69 GiB already allocated; 97.19 MiB free; 70.41 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bde7e4efdafa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mcorpus_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mloadFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-12-c0f0d16dd7ae>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, train_pairs, test_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, teacher_forcing_ratio, print_every, save_every, clip, corpus_name, loadFilename, start_iteration)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         )\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bb57fc0852a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, teacher_forcing_ratio, clip)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             decoder_outputs, decoder_hidden = decoder(\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             )\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# No teacher forcing: next input is decoder's own current output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37-mikami-mercury/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nfs/workspace/2017/mamoru_mikami/uranus/M1/gpu_github/pytorch-seq2seq/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_step, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Predict next word using Luong eq. 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Return output and final hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37-mikami-mercury/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 424.00 MiB (GPU 0; 10.73 GiB total capacity; 9.69 GiB already allocated; 97.19 MiB free; 70.41 MiB cached)"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.2####\n",
    "learning_rate = 0.0001#学習率を0.001とか大きめにとると（大きく無いと思うけど）途中でnanを吐き始める\n",
    "decoder_learning_ratio = 2.0\n",
    "n_iteration = 500000\n",
    "print_every = 500\n",
    "save_every = 1000\n",
    "#cal_method='cos'\n",
    "print('the number of data is',len(pairs))\n",
    "\n",
    "\n",
    "# Initialize optimizers\n",
    "print(\"Building optimizers ...\")\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(\n",
    "    decoder.parameters(), lr=learning_rate * decoder_learning_ratio\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"count parameters\")\n",
    "\n",
    "print(\"Starting Training!\")\n",
    "trainIters(\n",
    "    model_name,\n",
    "    voc,\n",
    "    train_pairs,\n",
    "    test_pairs,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    encoder_optimizer,\n",
    "    decoder_optimizer,\n",
    "    embedding,\n",
    "    encoder_n_layers,\n",
    "    decoder_n_layers,\n",
    "    save_dir,\n",
    "    n_iteration,\n",
    "    batch_size,\n",
    "    teacher_forcing_ratio,\n",
    "    print_every,\n",
    "    save_every,\n",
    "    clip,\n",
    "    corpus_name,\n",
    "    loadFilename,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1))\n",
    "    crossEntropy = -torch.log(logit.squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "def focalLoss(inp, target, mask, gamma=1, eps=1e-7):\n",
    "    nTotal = mask.sum()\n",
    "    logit = torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)\n",
    "    crossEntropy = -torch.log(logit)# cross entropy\n",
    "    loss = crossEntropy * (1 - logit) ** gamma # focal loss\n",
    "    loss = loss.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "\n",
    "xitem = [i/100 for i in range(1,101)]\n",
    "for ite in [ 0.5, 1, 2, 5]:\n",
    "    focalloss = []\n",
    "    normalloss = []\n",
    "    for x in range(1,101):\n",
    "        sample = [[(1-x/100)/3, x/100, (1-x/100)/3, (1-x/100)/3]]\n",
    "        decoder_output = torch.tensor(sample)\n",
    "        mask = torch.BoolTensor([[1]])\n",
    "        target_variable = torch.LongTensor([[1]])\n",
    "        normalloss.append(maskNLLLoss(decoder_output, target_variable[0], mask[0])[0].item())\n",
    "        focalloss.append(focalLoss(decoder_output, target_variable[0], mask[0],gamma=ite)[0].item())\n",
    "        \n",
    "    plt.plot(xitem,focalloss,label=('focal gamma='+str(ite)))\n",
    "    \n",
    "plt.plot(xitem,normalloss,label=('normal'))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-mikami-mercury",
   "language": "python",
   "name": "py37-mikami-mercury"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
