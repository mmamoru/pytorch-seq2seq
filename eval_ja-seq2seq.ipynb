{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)\n",
    "#device=torch.device('cpu')########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train_model32000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"train_model32000.txt\"#\"mecab_tagged.txt\"#\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Voc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁なんと !', '▁なんと !\\n'] 2 2\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り\\n'] 2 12\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい\\n'] 13 4\n",
      "['▁まさかの 増えてる w 羨ましい', '▁ 連れ が 誰か 引いて た んですよ 、 きっと それ と交換 したんだ と思うんですけど 誰 だったのか w あ 、 たっくん は しれっと 買取 に 出しました ー\\n'] 4 25\n",
      "['▁ おち つけ よ', '▁みんな は げろ\\n'] 4 3\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !\\n'] 3 13\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w\\n'] 19 7\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w\\n'] 7 4\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい\\n'] 2 6\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真\\n'] 6 3\n"
     ]
    }
   ],
   "source": [
    "printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1834337 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 34296\n",
      "keep_words 31995 / 34293 = 0.9330\n",
      "Trimmed from 1834337 pairs to 1822478, 0.9935 of total\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい']\n",
      "['▁ おち つけ よ', '▁みんな は げろ']\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい']\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真']\n",
      "['▁ は ?', '▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な']\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 28#22#3#\n",
    "print(corpus_name)\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "ちゃんと目的のコーパスファイルを読み込んでいるか確認\n",
      "train size 1457982\n",
      "example [['▁ 問い合わせ w 間違いない ですね w', '▁ して はいない んですけどね w それも 運 なの だろう'], ['▁ 珈琲 とか ウイスキー 用', '▁ 娘 用']]\n",
      "test size 364496\n",
      "example [['▁ ピカチュウ は かわいい でも カビゴン はもっと かわいい', '▁いやあ ずみ のほうが'], ['▁ スカ トロ とか やめてくれ だめ や', '▁そこまで 言って ねーよ w それは あかん']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# #Load batches for each iteration\n",
    "# train_pairs,test_pairs = train_test_split(pairs ,test_size=0.2)\n",
    "# with open('data/train_test_pairs_mecab.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_pairs,test_pairs), f)\n",
    "# with open(\"data/train_test_pairs_mecab3min.pickle\", \"rb\") as f:\n",
    "#     train_pairs,test_pairs = pickle.load(f)\n",
    "with open(\"data/train_test_pairs_32k.pickle\", \"rb\") as f:\n",
    "    train_pairs,test_pairs = pickle.load(f)\n",
    " \n",
    "print(voc.name)\n",
    "print(\"ちゃんと目的のコーパスファイルを読み込んでいるか確認\")\n",
    "print(\"train size\",len(train_pairs))\n",
    "print(\"example\",train_pairs[:2])\n",
    "print(\"test size\",len(test_pairs))\n",
    "print(\"example\",test_pairs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import *\n",
    "from model_lstm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.pyのモデルの説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding of current input word.\n",
    "Forward through unidirectional GRU.\n",
    "Calculate attention weights from the current GRU output from (2).\n",
    "Multiply attention weights to encoder outputs to get new “weighted sum” context vector.\n",
    "Concatenate weighted context vector and GRU output using Luong eq. 5.\n",
    "Predict next word using Luong eq. 6 (without softmax).\n",
    "Return output and final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)(1,3)\n",
    "last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)(1,3,2)\n",
    "encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "print(corpus_name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "teacher_forcing_ratio=0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 300000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パラメータ数をカウント\n",
      "encoderのパラメータ数\n",
      "13975040\n",
      "decoderパラメータ数\n",
      "18651646\n"
     ]
    }
   ],
   "source": [
    "# パラメータカウント関数\n",
    "print(\"パラメータ数をカウント\")\n",
    "print(\"encoderのパラメータ数\")\n",
    "parameters_count(encoder)\n",
    "print(\"decoderパラメータ数\")\n",
    "parameters_count(decoder)\n",
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 評価用pyファイルをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eval_ja_seq2seq import *\n",
    "# from eval_ja_seq2seq import mecabNormalizeString as normalizeString\n",
    "# mecabOrSp = 'mecab'\n",
    "from eval_lstm_ja_seq2seq import *\n",
    "from eval_lstm_ja_seq2seq import sentencePieceNormalizeString as normalizeString\n",
    "mecabOrSp = 'sentencePiece'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自分の入力で評価(会話)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  q\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "#searcher = GreedySearchDecoder(encoder, decoder)\n",
    "searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "searcher１ = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, searcher1, voc, mecabOrSp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数文をランダムで入力して出力して評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力: ▁ごめんねスムージー可愛すぎるわw初見にやけすぎてミスしすぎw\n",
      "入力を分割すると: ▁ごめんね ス ムー ジー 可愛すぎる わ w 初見 にやけ すぎて ミス しすぎ w\n",
      "\n",
      "Bot(BeamSearch): ▁wwwww\n",
      "正解: ▁目の色違うから人違いかな!\n",
      "\n",
      "\n",
      "入力: ▁あやて私だったり、?\n",
      "入力を分割すると: ▁あや て 私 だったり 、 ?\n",
      "\n",
      "Bot(BeamSearch): ▁え?w\n",
      "正解: ▁暇すぎて暗闇の中ガンプラ作る始末\n",
      "\n",
      "\n",
      "入力: ▁いいねで\n",
      "入力を分割すると: ▁いいね で\n",
      "\n",
      "Bot(BeamSearch): ▁\n",
      "正解: ▁リドルジョーカーねぇなぁって\n",
      "\n",
      "\n",
      "入力: ▁15時間寝てたwありがとう\n",
      "入力を分割すると: ▁15 時間 寝てた w ありがとう\n",
      "\n",
      "Bot(BeamSearch): ▁wwwwww\n",
      "正解: ▁みんなに知られたまーくんであったー\n",
      "\n",
      "\n",
      "入力: ▁楽しかったねーまた遊ぼあれよあれ。もうこたんめんのしんはつびー\n",
      "入力を分割すると: ▁楽しかった ねー また 遊ぼ あれ よ あれ 。 もう こたん めん の しん は つ びー\n",
      "\n",
      "Bot(BeamSearch): ▁wwwww\n",
      "正解: ▁それは宜野座伸元でしょうかw\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num=5\n",
    "test_choice = [(random.choice(test_pairs)[0].replace(' ',''),random.choice(test_pairs)[1].replace(' ','')) for i in range(num)]\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "# Initialize search module\n",
    "#searcher = GreedySearchDecoder(encoder, decoder)\n",
    "searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "\n",
    "for input_pair in test_choice:\n",
    "    \n",
    "    input_sentence, output_sentence = input_pair\n",
    "    print(\"入力:\",input_sentence)\n",
    "    input_sentence = normalizeString(input_sentence)\n",
    "    print('入力を分割すると:',input_sentence)\n",
    "    \n",
    "\n",
    "    # Evaluate sentence with seacher\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    # Format and print response sentence\n",
    "    output_words[:] = [\n",
    "        x for x in output_words if not (x == \"EOS\" or x == \"PAD\" or x==\"SOS\")\n",
    "    ]\n",
    "    print(\"\\nBot(BeamSearch):\", \"\".join(output_words))\n",
    "    print(\"正解:\",output_sentence+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数文を自動評価指標で評価、ここから下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu_1: 0.550000\n",
      "Bleu_2: 0.428174\n",
      "Bleu_3: 0.284043\n",
      "Bleu_4: 0.201143\n",
      "METEOR: 0.295797\n",
      "ROUGE_L: 0.522104\n",
      "CIDEr: 2.001651\n",
      "SkipThoughtsCosineSimilarity: 0.626149\n",
      "EmbeddingAverageCosineSimilarity: 0.866522\n",
      "EmbeddingAverageCosineSimilairty: 0.866522\n",
      "VectorExtremaCosineSimilarity: 0.558104\n",
      "GreedyMatchingScore: 0.779369\n"
     ]
    }
   ],
   "source": [
    "eval_folder = 'examples'\n",
    "from nlgeval import compute_metrics\n",
    "metrics_dict = compute_metrics(hypothesis=eval_folder+'/hyp.txt',\n",
    "                               references=[eval_folder+'/ref1.txt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ 違った 意味で 有名 になりそう やわ w', '▁なんか 、 すべて において 、 聞いて らん なかったです w', '▁ 残り 2 日 応援 お願いします', '▁大丈夫 ! ロック は 宗教 だから そんな 気持ち パッパ ラ パー だよ !', '▁10 代 ボイス なのか ! なん という なん ということ !', '▁3 枚目 保存して 使って いい ?', '▁エロ オタク の友達 w', '▁元気 さんが 俺の 隣で にゃ にゃ に ー するんですか ? 笑 w', '▁大丈夫です !', '▁やっぱ そーなの !? w']\n",
      "['▁ 江 田 島 みたいに やってほしい w 絶対 ピコ タン のほうが 面白い けど', '▁ やっちゃった なー が正解 かと w', '▁はい 頑張って', '▁ ロック は 宗教 そっか それなら 仕方ない 機会あれば 角 砂糖 焼肉 の タレ 添え を 試 そう', '▁声 、 格好いい よね', '▁ちょっとまって ! 比較 してくれ !', '▁あや のこと かな ?', '▁ お互いに w', '▁こちら 集計 となります ! 本日は対戦ありがとうございました !', '▁そう でしょ w あと 漢 方 なら 水 よりも 白 湯 が おすすめ b']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# eval_folder = 'mecab3min'\n",
    "eval_folder = 'sentencePiece28min'\n",
    "# test_extract = [random.choice(test_pairs) for i in range(5000)]\n",
    "\n",
    "\n",
    "# input_choices = [ele[0] for ele in test_extract]\n",
    "# output_choices = [ele[1] for ele in test_extract]\n",
    "\n",
    "\n",
    "# with open(eval_folder+'/eval_input_output.pickle', 'wb') as f:\n",
    "#     pickle.dump((input_choices ,output_choices), f)\n",
    "\n",
    "with open(eval_folder+\"/eval_input_output.pickle\", \"rb\") as f:\n",
    "    \n",
    "    input_choices ,output_choices= pickle.load(f)\n",
    "    \n",
    "print(input_choices[:10])\n",
    "\n",
    "print(output_choices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentencePiece28min'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(eval_folder+'/ref.txt','w') as f:\n",
    "#     f.write('\\n'.join(output_choices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイル名注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Bleu_1, Bleu_2, Bleu_3, Bleu_4, METEOR, ROUGE_L, CIDEr, SkipThoughtCS, EmbeddingAverageCosineSimilarity, EmbeddingAverageCosineSimilairty, VectorExtremaCosineSimilarity, GreedyMatchingScore]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = 'to_csv_out4.csv'\n",
    "df = pd.DataFrame(columns=list(metrics_dict.keys()))\n",
    "df.to_csv(eval_folder+'/'+csv_file)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "38\n",
      "['9000_checkpoint.tar', '310000_checkpoint.tar', '410000_checkpoint.tar', '100000_checkpoint.tar', '20000_checkpoint.tar', '90000_checkpoint.tar', '250000_checkpoint.tar', '50000_checkpoint.tar', '285000_checkpoint.tar', '460000_checkpoint.tar', '360000_checkpoint.tar', '200000_checkpoint.tar', '8000_checkpoint.tar', '500000_checkpoint.tar', '330000_checkpoint.tar', '430000_checkpoint.tar', '380000_checkpoint.tar', '480000_checkpoint.tar', '70000_checkpoint.tar', '150000_checkpoint.tar', '440000_checkpoint.tar', '340000_checkpoint.tar', '490000_checkpoint.tar', '390000_checkpoint.tar', '350000_checkpoint.tar', '60000_checkpoint.tar', '450000_checkpoint.tar', '420000_checkpoint.tar', '10000_checkpoint.tar', '320000_checkpoint.tar', '80000_checkpoint.tar', '23000_checkpoint.tar', '370000_checkpoint.tar', '470000_checkpoint.tar', '7000_checkpoint.tar', '400000_checkpoint.tar', '30000_checkpoint.tar', '300000_checkpoint.tar']\n",
      "\n",
      "38\n",
      "[7000, 8000, 9000, 10000, 20000, 23000, 30000, 50000, 60000, 70000, 80000, 90000, 100000, 150000, 200000, 250000, 285000, 300000, 310000, 320000, 330000, 340000, 350000, 360000, 370000, 380000, 390000, 400000, 410000, 420000, 430000, 440000, 450000, 460000, 470000, 480000, 490000, 500000]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "# pathPos = './data/save/focal_loss_model/mecab_tagged.txt/4-4_256_0.2/'\n",
    "pathPos = './data/save/focal_loss_model/train_model32000.txt/4-4_256_0.2/'\n",
    "fileList = os.listdir(pathPos)\n",
    "print(len(fileList))\n",
    "files_file = [f for f in fileList if os.path.isfile(os.path.join(pathPos,f))]\n",
    "print(len(files_file))\n",
    "print(files_file)\n",
    "\n",
    "checkpoint_list = []\n",
    "\n",
    "for f in files_file:\n",
    "    numStr = re.search(r'\\d+',f)\n",
    "    numStr = int(numStr.group())\n",
    "    checkpoint_list.append(numStr)\n",
    "checkpoint_list.sort()\n",
    "print()\n",
    "print(len(checkpoint_list))\n",
    "print(checkpoint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "test 7000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000016\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.032861\n",
      "ROUGE_L: 0.002542\n",
      "CIDEr: 0.001128\n",
      "SkipThoughtsCosineSimilarity: 0.388682\n",
      "EmbeddingAverageCosineSimilarity: 0.686857\n",
      "EmbeddingAverageCosineSimilairty: 0.686857\n",
      "VectorExtremaCosineSimilarity: 0.230561\n",
      "GreedyMatchingScore: 0.884528\n",
      "test 8000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000105\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.034706\n",
      "ROUGE_L: 0.013108\n",
      "CIDEr: 0.005267\n",
      "SkipThoughtsCosineSimilarity: 0.381632\n",
      "EmbeddingAverageCosineSimilarity: 0.635135\n",
      "EmbeddingAverageCosineSimilairty: 0.635135\n",
      "VectorExtremaCosineSimilarity: 0.240693\n",
      "GreedyMatchingScore: 0.872019\n",
      "test 9000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000018\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.034682\n",
      "ROUGE_L: 0.003034\n",
      "CIDEr: 0.001352\n",
      "SkipThoughtsCosineSimilarity: 0.383354\n",
      "EmbeddingAverageCosineSimilarity: 0.643269\n",
      "EmbeddingAverageCosineSimilairty: 0.643269\n",
      "VectorExtremaCosineSimilarity: 0.235292\n",
      "GreedyMatchingScore: 0.874769\n",
      "test 10000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000193\n",
      "Bleu_2: 0.000001\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.034282\n",
      "ROUGE_L: 0.024369\n",
      "CIDEr: 0.009703\n",
      "SkipThoughtsCosineSimilarity: 0.378258\n",
      "EmbeddingAverageCosineSimilarity: 0.613555\n",
      "EmbeddingAverageCosineSimilairty: 0.613555\n",
      "VectorExtremaCosineSimilarity: 0.235934\n",
      "GreedyMatchingScore: 0.867786\n",
      "test 20000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000126\n",
      "Bleu_2: 0.000001\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.034562\n",
      "ROUGE_L: 0.016715\n",
      "CIDEr: 0.006707\n",
      "SkipThoughtsCosineSimilarity: 0.372848\n",
      "EmbeddingAverageCosineSimilarity: 0.543189\n",
      "EmbeddingAverageCosineSimilairty: 0.543189\n",
      "VectorExtremaCosineSimilarity: 0.233687\n",
      "GreedyMatchingScore: 0.851651\n",
      "test 23000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000092\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.036979\n",
      "ROUGE_L: 0.012041\n",
      "CIDEr: 0.004791\n",
      "SkipThoughtsCosineSimilarity: 0.374299\n",
      "EmbeddingAverageCosineSimilarity: 0.566593\n",
      "EmbeddingAverageCosineSimilairty: 0.566593\n",
      "VectorExtremaCosineSimilarity: 0.228934\n",
      "GreedyMatchingScore: 0.856952\n",
      "test 30000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000098\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.035534\n",
      "ROUGE_L: 0.013036\n",
      "CIDEr: 0.005475\n",
      "SkipThoughtsCosineSimilarity: 0.358874\n",
      "EmbeddingAverageCosineSimilarity: 0.479966\n",
      "EmbeddingAverageCosineSimilairty: 0.479966\n",
      "VectorExtremaCosineSimilarity: 0.212233\n",
      "GreedyMatchingScore: 0.836072\n",
      "test 50000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000099\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.037704\n",
      "ROUGE_L: 0.014070\n",
      "CIDEr: 0.006633\n",
      "SkipThoughtsCosineSimilarity: 0.370656\n",
      "EmbeddingAverageCosineSimilarity: 0.563883\n",
      "EmbeddingAverageCosineSimilairty: 0.563883\n",
      "VectorExtremaCosineSimilarity: 0.238137\n",
      "GreedyMatchingScore: 0.855098\n",
      "test 60000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000066\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.039394\n",
      "ROUGE_L: 0.008848\n",
      "CIDEr: 0.003892\n",
      "SkipThoughtsCosineSimilarity: 0.371216\n",
      "EmbeddingAverageCosineSimilarity: 0.570733\n",
      "EmbeddingAverageCosineSimilairty: 0.570733\n",
      "VectorExtremaCosineSimilarity: 0.241669\n",
      "GreedyMatchingScore: 0.856960\n",
      "test 70000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000106\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.040037\n",
      "ROUGE_L: 0.014216\n",
      "CIDEr: 0.006303\n",
      "SkipThoughtsCosineSimilarity: 0.371869\n",
      "EmbeddingAverageCosineSimilarity: 0.552117\n",
      "EmbeddingAverageCosineSimilairty: 0.552117\n",
      "VectorExtremaCosineSimilarity: 0.243133\n",
      "GreedyMatchingScore: 0.852672\n",
      "test 80000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000061\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.037961\n",
      "ROUGE_L: 0.008294\n",
      "CIDEr: 0.003299\n",
      "SkipThoughtsCosineSimilarity: 0.374527\n",
      "EmbeddingAverageCosineSimilarity: 0.589285\n",
      "EmbeddingAverageCosineSimilairty: 0.589285\n",
      "VectorExtremaCosineSimilarity: 0.238796\n",
      "GreedyMatchingScore: 0.859825\n",
      "test 90000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000122\n",
      "Bleu_2: 0.000001\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.038532\n",
      "ROUGE_L: 0.016200\n",
      "CIDEr: 0.006957\n",
      "SkipThoughtsCosineSimilarity: 0.383559\n",
      "EmbeddingAverageCosineSimilarity: 0.599531\n",
      "EmbeddingAverageCosineSimilairty: 0.599531\n",
      "VectorExtremaCosineSimilarity: 0.252032\n",
      "GreedyMatchingScore: 0.864058\n",
      "test 100000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000092\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.037603\n",
      "ROUGE_L: 0.012366\n",
      "CIDEr: 0.005696\n",
      "SkipThoughtsCosineSimilarity: 0.375162\n",
      "EmbeddingAverageCosineSimilarity: 0.593939\n",
      "EmbeddingAverageCosineSimilairty: 0.593939\n",
      "VectorExtremaCosineSimilarity: 0.234935\n",
      "GreedyMatchingScore: 0.861631\n",
      "test 150000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000081\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.038222\n",
      "ROUGE_L: 0.010818\n",
      "CIDEr: 0.004574\n",
      "SkipThoughtsCosineSimilarity: 0.377091\n",
      "EmbeddingAverageCosineSimilarity: 0.607040\n",
      "EmbeddingAverageCosineSimilairty: 0.607040\n",
      "VectorExtremaCosineSimilarity: 0.238378\n",
      "GreedyMatchingScore: 0.865773\n",
      "test 200000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000104\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.039781\n",
      "ROUGE_L: 0.013549\n",
      "CIDEr: 0.005625\n",
      "SkipThoughtsCosineSimilarity: 0.378287\n",
      "EmbeddingAverageCosineSimilarity: 0.590861\n",
      "EmbeddingAverageCosineSimilairty: 0.590861\n",
      "VectorExtremaCosineSimilarity: 0.233511\n",
      "GreedyMatchingScore: 0.860332\n",
      "test 250000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000093\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.039254\n",
      "ROUGE_L: 0.012988\n",
      "CIDEr: 0.005598\n",
      "SkipThoughtsCosineSimilarity: 0.372562\n",
      "EmbeddingAverageCosineSimilarity: 0.563561\n",
      "EmbeddingAverageCosineSimilairty: 0.563561\n",
      "VectorExtremaCosineSimilarity: 0.225162\n",
      "GreedyMatchingScore: 0.853009\n",
      "test 285000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000080\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.042282\n",
      "ROUGE_L: 0.011000\n",
      "CIDEr: 0.004692\n",
      "SkipThoughtsCosineSimilarity: 0.375263\n",
      "EmbeddingAverageCosineSimilarity: 0.570927\n",
      "EmbeddingAverageCosineSimilairty: 0.570927\n",
      "VectorExtremaCosineSimilarity: 0.231172\n",
      "GreedyMatchingScore: 0.856831\n",
      "test 300000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000099\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.040995\n",
      "ROUGE_L: 0.013695\n",
      "CIDEr: 0.005899\n",
      "SkipThoughtsCosineSimilarity: 0.374379\n",
      "EmbeddingAverageCosineSimilarity: 0.570854\n",
      "EmbeddingAverageCosineSimilairty: 0.570854\n",
      "VectorExtremaCosineSimilarity: 0.232113\n",
      "GreedyMatchingScore: 0.855911\n",
      "test 310000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000061\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.040152\n",
      "ROUGE_L: 0.008417\n",
      "CIDEr: 0.003646\n",
      "SkipThoughtsCosineSimilarity: 0.372999\n",
      "EmbeddingAverageCosineSimilarity: 0.566982\n",
      "EmbeddingAverageCosineSimilairty: 0.566982\n",
      "VectorExtremaCosineSimilarity: 0.224921\n",
      "GreedyMatchingScore: 0.854333\n",
      "test 320000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000109\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.040374\n",
      "ROUGE_L: 0.014577\n",
      "CIDEr: 0.006045\n",
      "SkipThoughtsCosineSimilarity: 0.372076\n",
      "EmbeddingAverageCosineSimilarity: 0.548526\n",
      "EmbeddingAverageCosineSimilairty: 0.548526\n",
      "VectorExtremaCosineSimilarity: 0.226546\n",
      "GreedyMatchingScore: 0.849279\n",
      "test 330000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000071\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.040724\n",
      "ROUGE_L: 0.009437\n",
      "CIDEr: 0.004208\n",
      "SkipThoughtsCosineSimilarity: 0.374051\n",
      "EmbeddingAverageCosineSimilarity: 0.565825\n",
      "EmbeddingAverageCosineSimilairty: 0.565825\n",
      "VectorExtremaCosineSimilarity: 0.229365\n",
      "GreedyMatchingScore: 0.854023\n",
      "test 340000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000069\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.040987\n",
      "ROUGE_L: 0.009106\n",
      "CIDEr: 0.004016\n",
      "SkipThoughtsCosineSimilarity: 0.373340\n",
      "EmbeddingAverageCosineSimilarity: 0.561437\n",
      "EmbeddingAverageCosineSimilairty: 0.561437\n",
      "VectorExtremaCosineSimilarity: 0.225004\n",
      "GreedyMatchingScore: 0.852701\n",
      "test 350000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000075\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041437\n",
      "ROUGE_L: 0.009964\n",
      "CIDEr: 0.004368\n",
      "SkipThoughtsCosineSimilarity: 0.372851\n",
      "EmbeddingAverageCosineSimilarity: 0.550084\n",
      "EmbeddingAverageCosineSimilairty: 0.550084\n",
      "VectorExtremaCosineSimilarity: 0.225280\n",
      "GreedyMatchingScore: 0.850194\n",
      "test 360000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000074\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.040754\n",
      "ROUGE_L: 0.009987\n",
      "CIDEr: 0.004167\n",
      "SkipThoughtsCosineSimilarity: 0.374713\n",
      "EmbeddingAverageCosineSimilarity: 0.564888\n",
      "EmbeddingAverageCosineSimilairty: 0.564888\n",
      "VectorExtremaCosineSimilarity: 0.224350\n",
      "GreedyMatchingScore: 0.853559\n",
      "test 370000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000086\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041043\n",
      "ROUGE_L: 0.011426\n",
      "CIDEr: 0.004749\n",
      "SkipThoughtsCosineSimilarity: 0.376806\n",
      "EmbeddingAverageCosineSimilarity: 0.578259\n",
      "EmbeddingAverageCosineSimilairty: 0.578259\n",
      "VectorExtremaCosineSimilarity: 0.230694\n",
      "GreedyMatchingScore: 0.858432\n",
      "test 380000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000113\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.042003\n",
      "ROUGE_L: 0.015263\n",
      "CIDEr: 0.006282\n",
      "SkipThoughtsCosineSimilarity: 0.379547\n",
      "EmbeddingAverageCosineSimilarity: 0.568653\n",
      "EmbeddingAverageCosineSimilairty: 0.568653\n",
      "VectorExtremaCosineSimilarity: 0.239880\n",
      "GreedyMatchingScore: 0.856287\n",
      "test 390000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000080\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041383\n",
      "ROUGE_L: 0.010752\n",
      "CIDEr: 0.004415\n",
      "SkipThoughtsCosineSimilarity: 0.376390\n",
      "EmbeddingAverageCosineSimilarity: 0.562089\n",
      "EmbeddingAverageCosineSimilairty: 0.562089\n",
      "VectorExtremaCosineSimilarity: 0.230464\n",
      "GreedyMatchingScore: 0.853850\n",
      "test 400000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000094\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041041\n",
      "ROUGE_L: 0.012597\n",
      "CIDEr: 0.005440\n",
      "SkipThoughtsCosineSimilarity: 0.376107\n",
      "EmbeddingAverageCosineSimilarity: 0.571041\n",
      "EmbeddingAverageCosineSimilairty: 0.571041\n",
      "VectorExtremaCosineSimilarity: 0.226376\n",
      "GreedyMatchingScore: 0.855455\n",
      "test 410000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000086\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041795\n",
      "ROUGE_L: 0.011417\n",
      "CIDEr: 0.004671\n",
      "SkipThoughtsCosineSimilarity: 0.377620\n",
      "EmbeddingAverageCosineSimilarity: 0.568324\n",
      "EmbeddingAverageCosineSimilairty: 0.568324\n",
      "VectorExtremaCosineSimilarity: 0.238258\n",
      "GreedyMatchingScore: 0.854944\n",
      "test 420000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000087\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041413\n",
      "ROUGE_L: 0.011715\n",
      "CIDEr: 0.005082\n",
      "SkipThoughtsCosineSimilarity: 0.377739\n",
      "EmbeddingAverageCosineSimilarity: 0.560123\n",
      "EmbeddingAverageCosineSimilairty: 0.560123\n",
      "VectorExtremaCosineSimilarity: 0.236360\n",
      "GreedyMatchingScore: 0.853963\n",
      "test 430000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000089\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041617\n",
      "ROUGE_L: 0.012048\n",
      "CIDEr: 0.004971\n",
      "SkipThoughtsCosineSimilarity: 0.379045\n",
      "EmbeddingAverageCosineSimilarity: 0.568531\n",
      "EmbeddingAverageCosineSimilairty: 0.568531\n",
      "VectorExtremaCosineSimilarity: 0.229881\n",
      "GreedyMatchingScore: 0.855456\n",
      "test 440000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000088\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.042561\n",
      "ROUGE_L: 0.011422\n",
      "CIDEr: 0.004790\n",
      "SkipThoughtsCosineSimilarity: 0.379080\n",
      "EmbeddingAverageCosineSimilarity: 0.578390\n",
      "EmbeddingAverageCosineSimilairty: 0.578390\n",
      "VectorExtremaCosineSimilarity: 0.236392\n",
      "GreedyMatchingScore: 0.857844\n",
      "test 450000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000084\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041265\n",
      "ROUGE_L: 0.011620\n",
      "CIDEr: 0.004860\n",
      "SkipThoughtsCosineSimilarity: 0.379807\n",
      "EmbeddingAverageCosineSimilarity: 0.584922\n",
      "EmbeddingAverageCosineSimilairty: 0.584922\n",
      "VectorExtremaCosineSimilarity: 0.236813\n",
      "GreedyMatchingScore: 0.858921\n",
      "test 460000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000086\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.042064\n",
      "ROUGE_L: 0.011383\n",
      "CIDEr: 0.004704\n",
      "SkipThoughtsCosineSimilarity: 0.378603\n",
      "EmbeddingAverageCosineSimilarity: 0.586617\n",
      "EmbeddingAverageCosineSimilairty: 0.586617\n",
      "VectorExtremaCosineSimilarity: 0.238292\n",
      "GreedyMatchingScore: 0.859930\n",
      "test 470000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000116\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.041600\n",
      "ROUGE_L: 0.015576\n",
      "CIDEr: 0.006383\n",
      "SkipThoughtsCosineSimilarity: 0.376341\n",
      "EmbeddingAverageCosineSimilarity: 0.571948\n",
      "EmbeddingAverageCosineSimilairty: 0.571948\n",
      "VectorExtremaCosineSimilarity: 0.236743\n",
      "GreedyMatchingScore: 0.855509\n",
      "test 480000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000100\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.043551\n",
      "ROUGE_L: 0.013196\n",
      "CIDEr: 0.005417\n",
      "SkipThoughtsCosineSimilarity: 0.380697\n",
      "EmbeddingAverageCosineSimilarity: 0.573874\n",
      "EmbeddingAverageCosineSimilairty: 0.573874\n",
      "VectorExtremaCosineSimilarity: 0.244351\n",
      "GreedyMatchingScore: 0.857451\n",
      "test 490000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000105\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.042475\n",
      "ROUGE_L: 0.014059\n",
      "CIDEr: 0.005770\n",
      "SkipThoughtsCosineSimilarity: 0.375836\n",
      "EmbeddingAverageCosineSimilarity: 0.571124\n",
      "EmbeddingAverageCosineSimilairty: 0.571124\n",
      "VectorExtremaCosineSimilarity: 0.235775\n",
      "GreedyMatchingScore: 0.855869\n",
      "test 500000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000120\n",
      "Bleu_2: 0.000001\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.042728\n",
      "ROUGE_L: 0.016157\n",
      "CIDEr: 0.007021\n",
      "SkipThoughtsCosineSimilarity: 0.380380\n",
      "EmbeddingAverageCosineSimilarity: 0.592272\n",
      "EmbeddingAverageCosineSimilairty: 0.592272\n",
      "VectorExtremaCosineSimilarity: 0.236224\n",
      "GreedyMatchingScore: 0.861920\n"
     ]
    }
   ],
   "source": [
    "from nlgeval import compute_metrics\n",
    "import pandas as pd\n",
    "\n",
    "print(corpus_name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "teacher_forcing_ratio=0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "\n",
    "#1000,2000,3000,4000,5000,6000,7000,8000,9000,15000,30000,52000,56000,60000,64000,68000,72000,96000,100000,102000,104000,120000,124000,128000,129000,148000,150000,151000,152000,153000,154000,155000,156000,157000,158000\n",
    "#159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000\n",
    "#174000,175000,176000,177000,178000,179000,180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000\n",
    "# checkpoint_list = [4000,15000,30000,52000,56000,68000,72000,100000,104000,120000,124000,128000,148000,150000,151000,152000,153000,155000,156000,157000,158000,\n",
    "#                    159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000,174000,175000,176000,178000,179000,\n",
    "#                    180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000]\n",
    "\n",
    "for checkpoint_iter in checkpoint_list:\n",
    "    \n",
    "    print('test',checkpoint_iter,'iter')\n",
    "\n",
    "    loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                               '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                               '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "    # Load model if a loadFilename is provided\n",
    "    if loadFilename:\n",
    "        print(\"load save file\")\n",
    "        # If loading on same machine the model was trained on\n",
    "        checkpoint = torch.load(loadFilename)\n",
    "        # If loading a model trained on GPU to CPU\n",
    "        # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "        encoder_sd = checkpoint[\"en\"]\n",
    "        decoder_sd = checkpoint[\"de\"]\n",
    "        encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "        decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "        embedding_sd = checkpoint[\"embedding\"]\n",
    "        voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "    print(\"Building encoder and decoder ...\")\n",
    "    # Initialize word embeddings\n",
    "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "    if loadFilename:\n",
    "        embedding.load_state_dict(embedding_sd)\n",
    "    # Initialize encoder & decoder models\n",
    "    encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "    decoder = AttnDecoderRNN(\n",
    "        attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    "    )\n",
    "    if loadFilename:\n",
    "        encoder.load_state_dict(encoder_sd)\n",
    "        decoder.load_state_dict(decoder_sd)\n",
    "    # Use appropriate device\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    print(\"Models built and ready to go!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "    hyp_sentences = []\n",
    "    \n",
    "    \n",
    "    for input_sentence in input_choices:\n",
    "        input_sentence = normalizeString(input_sentence)\n",
    "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "        output_words[:] = [\n",
    "            x for x in output_words if not (x == \"EOS\" or x == \"PAD\" or x==\"SOS\")\n",
    "        ]\n",
    "        hyp_sentences.append(\"\".join(output_words))\n",
    "\n",
    "        \n",
    "    with open(eval_folder+'/hyp.txt','w') as f:\n",
    "        f.write('\\n'.join(hyp_sentences))\n",
    "        \n",
    "        \n",
    "    \n",
    "    metrics_dict = compute_metrics(hypothesis=eval_folder+'/hyp.txt',\n",
    "                                   references=[eval_folder+'/ref.txt'])\n",
    "    \n",
    "    df = pd.read_csv(eval_folder+'/'+csv_file, index_col=0)\n",
    "    nums = metrics_dict.values()\n",
    "    df.loc[checkpoint_iter] = list(map(round,nums,[6]*len(nums)))\n",
    "    df.to_csv(eval_folder+'/'+csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bleu_1: 0.003000\n",
    "Bleu_2: 0.000055\n",
    "Bleu_3: 0.000014\n",
    "Bleu_4: 0.000007\n",
    "METEOR: 0.036579\n",
    "ROUGE_L: 0.003000\n",
    "CIDEr: 0.007500\n",
    "SkipThoughtsCosineSimilarity: 0.736541\n",
    "EmbeddingAverageCosineSimilarity: 0.633790\n",
    "EmbeddingAverageCosineSimilairty: 0.633790\n",
    "VectorExtremaCosineSimilarity: 0.562120\n",
    "GreedyMatchingScore: 0.868408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-mikami_saturn",
   "language": "python",
   "name": "py37-mikami_saturn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
