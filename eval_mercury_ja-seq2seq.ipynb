{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)\n",
    "#device=torch.device('cpu')########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/train_model32000.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpus_name = \"train_model32000.txt\"#\"mecab_tagged.txt\"#\n",
    "position = \"./data\"\n",
    "corpus = os.path.join(position, corpus_name)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Voc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁なんと !', '▁なんと !\\n'] 2 2\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り\\n'] 2 12\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい\\n'] 13 4\n",
      "['▁まさかの 増えてる w 羨ましい', '▁ 連れ が 誰か 引いて た んですよ 、 きっと それ と交換 したんだ と思うんですけど 誰 だったのか w あ 、 たっくん は しれっと 買取 に 出しました ー\\n'] 4 25\n",
      "['▁ おち つけ よ', '▁みんな は げろ\\n'] 4 3\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !\\n'] 3 13\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w\\n'] 19 7\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w\\n'] 7 4\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい\\n'] 2 6\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真\\n'] 6 3\n"
     ]
    }
   ],
   "source": [
    "printLines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2187212 sentence pairs\n",
      "Trimmed to 1834337 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 34296\n",
      "keep_words 31995 / 34293 = 0.9330\n",
      "Trimmed from 1834337 pairs to 1822478, 0.9935 of total\n",
      "\n",
      "pairs:\n",
      "['▁なんと !', '▁なんと !']\n",
      "['▁なんと !', '▁ 意味 : 都 を 移 すこ と 、 都 移 り']\n",
      "['▁わたし 安定の たっくん ですよー 気づいたら たっくん いないし 、 いつき あるし 誰 と交換 したのか', '▁まさかの 増えてる w 羨ましい']\n",
      "['▁ おち つけ よ', '▁みんな は げろ']\n",
      "['▁みんな は げろ', '▁昨日 髪の毛 切った ばっかり だけど このくらい つる っ パ ゲ になって きます !']\n",
      "['▁今は 古 すぎて 廃 盤 のもの が多いので 、 いい モンスター が出る の だと 高い 値 で 売れ ます w', '▁あ 、 やっぱり そう言う のある ですね w']\n",
      "['▁あ 、 やっぱり そう言う のある ですね w', '▁ あります あります w']\n",
      "['▁やだ お', '▁ 目元 は 知ってる 、 かあいい']\n",
      "['▁ 目元 は 知ってる 、 かあいい', '▁超 過去 写真']\n",
      "['▁ は ?', '▁クソ じゃん 気にし ねー で いいんじゃね w まあ 俺は ファイヤー 党 の人 なので もちろん あの方 に入れた が な']\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 28#22#3#\n",
    "print(corpus_name)\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus, corpus_name)#, datafile, save_dir)\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "ちゃんと目的のコーパスファイルを読み込んでいるか確認\n",
      "train size 1457982\n",
      "example [['▁ 問い合わせ w 間違いない ですね w', '▁ して はいない んですけどね w それも 運 なの だろう'], ['▁ 珈琲 とか ウイスキー 用', '▁ 娘 用']]\n",
      "test size 364496\n",
      "example [['▁ ピカチュウ は かわいい でも カビゴン はもっと かわいい', '▁いやあ ずみ のほうが'], ['▁ スカ トロ とか やめてくれ だめ や', '▁そこまで 言って ねーよ w それは あかん']]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# #Load batches for each iteration\n",
    "# train_pairs,test_pairs = train_test_split(pairs ,test_size=0.2)\n",
    "# with open('data/train_test_pairs_mecab.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_pairs,test_pairs), f)\n",
    "# with open(\"data/train_test_pairs_mecab3min.pickle\", \"rb\") as f:\n",
    "#     train_pairs,test_pairs = pickle.load(f)\n",
    "with open(\"data/train_test_pairs_32k.pickle\", \"rb\") as f:\n",
    "    train_pairs,test_pairs = pickle.load(f)\n",
    " \n",
    "print(voc.name)\n",
    "print(\"ちゃんと目的のコーパスファイルを読み込んでいるか確認\")\n",
    "print(\"train size\",len(train_pairs))\n",
    "print(\"example\",train_pairs[:2])\n",
    "print(\"test size\",len(test_pairs))\n",
    "print(\"example\",test_pairs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import *\n",
    "from model_lstm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.pyのモデルの説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get embedding of current input word.\n",
    "Forward through unidirectional GRU.\n",
    "Calculate attention weights from the current GRU output from (2).\n",
    "Multiply attention weights to encoder outputs to get new “weighted sum” context vector.\n",
    "Concatenate weighted context vector and GRU output using Luong eq. 5.\n",
    "Predict next word using Luong eq. 6 (without softmax).\n",
    "Return output and final hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_step: one time step (one word) of input sequence batch; shape=(1, batch_size)(1,3)\n",
    "last_hidden: final hidden layer of GRU; shape=(n_layers x num_directions, batch_size, hidden_size)(1,3,2)\n",
    "encoder_outputs: encoder model’s output; shape=(max_length, batch_size, hidden_size)(4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_model32000.txt\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "#訓練するときだけ\n",
    "print(corpus_name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "teacher_forcing_ratio=0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 35000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    print(\"load save file\")\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "print(\"Building encoder and decoder ...\")\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(\n",
    "    attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    ")\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print(\"Models built and ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "パラメータ数をカウント\n",
      "encoderのパラメータ数\n",
      "13975040\n",
      "decoderパラメータ数\n",
      "18651646\n"
     ]
    }
   ],
   "source": [
    "# パラメータカウント関数\n",
    "print(\"パラメータ数をカウント\")\n",
    "print(\"encoderのパラメータ数\")\n",
    "parameters_count(encoder)\n",
    "print(\"decoderパラメータ数\")\n",
    "parameters_count(decoder)\n",
    "#print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return [all_tokens], [-torch.sum(torch.log(all_scores).detach().cpu())/float(max_length - 1 + 1e-6)]\n",
    "    \n",
    "    \n",
    "import sentencepiece as spm\n",
    "segmentation_model_position = './data'\n",
    "segmentation_model_name = 'train_model32000.model'\n",
    "\n",
    "spp = spm.SentencePieceProcessor()\n",
    "spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "def normalizeString(input_sentence):\n",
    "    splitSentence = ' '.join(spp.EncodeAsPieces(input_sentence))\n",
    "    return splitSentence\n",
    "\n",
    "# import MeCab\n",
    "# tagger = MeCab.Tagger(\"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/\")\n",
    "\n",
    "# def normalizeString(input_sentence):\n",
    "#     splitSentence = ' '.join([ele.split(\"\\t\")[0] for ele in tagger.parse(input_sentence).split(\"\\n\")[:-2]])\n",
    "#     return splitSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, beam_width, n_best):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beam_width = beam_width\n",
    "        self.n_best = n_best\n",
    "        \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[: decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        #decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        \n",
    "        class BeamSearchNode(object):\n",
    "            def __init__(self, hid, prevNode, wordId, logP, length):\n",
    "                self.hid = hid\n",
    "                self.prevNode = prevNode\n",
    "                self.wordId = wordId\n",
    "                self.logP = logP\n",
    "                self.length = length\n",
    "                \n",
    "            def eval(self, alpha=1.0):\n",
    "                reward = 0\n",
    "                # Add here a function for shaping a reward\n",
    "\n",
    "                return self.logP / float(self.length - 1 + 1e-6) + alpha * reward\n",
    "          \n",
    "        \n",
    "        n_best_batch_list = []\n",
    "        n_best_batch_score_list = []\n",
    "        for batchNum in range(encoder_hidden.size(1)):\n",
    "            \n",
    "            decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
    "            # 一つ前の状態の隠れベクトル、単語をNodeを保持するNodeを生成\n",
    "            node = BeamSearchNode(hid=decoder_hidden[:,batchNum,:].unsqueeze(1), prevNode=None, wordId=decoder_input, logP=0, length=1)\n",
    "            nextNodes=[]\n",
    "            \n",
    "            nextNodes.append((-node.eval(), id(node), node))\n",
    "            n_dec_steps = 0\n",
    "            while True:\n",
    "                \n",
    "                nodes = [sorted(nextNodes)[inode] for inode in range(len(nextNodes)) if inode<self.beam_width]\n",
    "                nextNodes = []\n",
    "                end_node = []\n",
    "                for beamNum in range(self.beam_width):\n",
    "                    if len(nodes)<=0:\n",
    "                        break\n",
    "                    #今から探索するNodeを取得\n",
    "                    score, _, n = nodes.pop(0)\n",
    "                    decoder_input = n.wordId\n",
    "                    decoder_hidden = n.hid\n",
    "                    \n",
    "                    if n.wordId[0][0].item()!=EOS_token and n.wordId[0][0].item()!=PAD_token and n.length<=max_length:\n",
    "                        decoder_output, decoder_hidden = self.decoder(\n",
    "                            decoder_input, decoder_hidden, encoder_outputs\n",
    "                        )\n",
    "\n",
    "                        topk_prob, topk_indexes = torch.topk(decoder_output, self.beam_width) \n",
    "\n",
    "                        for new_k in range(self.beam_width):\n",
    "                            decoded_t = topk_indexes[0][new_k].view(1,1) # (1)\n",
    "                            logp = torch.log(topk_prob[0][new_k]).item() # float log probability val\n",
    "\n",
    "                            node = BeamSearchNode(hid=decoder_hidden,\n",
    "                                                  prevNode=n,\n",
    "                                                  wordId=decoded_t,\n",
    "                                                  logP=n.logP+logp,\n",
    "                                                  length=n.length+1)\n",
    "                            nextNodes.append((-node.eval(), id(node), node))\n",
    "                            \n",
    "                            \n",
    "                    else:\n",
    "                        node = BeamSearchNode(hid=decoder_hidden,\n",
    "                                                  prevNode=n,\n",
    "                                                  wordId=torch.LongTensor([[PAD_token]]).to(device),\n",
    "                                                  logP=n.logP,\n",
    "                                                  length=n.length)\n",
    "                        nextNodes.append((-node.eval(), id(node), node))\n",
    "                        end_node.append(1)\n",
    "                if len(end_node)>=self.beam_width:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "        \n",
    "            if len(nextNodes)!=self.beam_width:\n",
    "                print('assert not match beam width and nextNode length')\n",
    "                \n",
    "            n_best_seq_list = []\n",
    "            n_best_score_list = []\n",
    "            for score, _id, n in sorted(nextNodes):\n",
    "                sequence = [n.wordId.item()]\n",
    "                n_best_score_list.append(score)\n",
    "                # back trace from end node\n",
    "                while n.prevNode is not None:\n",
    "                    n = n.prevNode\n",
    "                    sequence.append(n.wordId.item())\n",
    "                    \n",
    "                sequence = sequence[::-1] # reverse\n",
    "                n_best_seq_list.append(sequence)\n",
    "        \n",
    "            n_best_seq_list = n_best_seq_list[::-1]\n",
    "            n_best_score_list = n_best_score_list[::-1]\n",
    "            \n",
    "            n_best_batch_list.append(n_best_seq_list)\n",
    "            n_best_batch_score_list.append(n_best_score_list)\n",
    "            \n",
    "        n_best_batch_list = torch.tensor(n_best_batch_list)\n",
    "        n_best_batch_score_list = torch.tensor(n_best_batch_score_list)\n",
    "            \n",
    "            #batchを無視して今回は出力することにした\n",
    "        return n_best_batch_list[0], n_best_batch_score_list[0]\n",
    "    \n",
    "# import sentencepiece as spm\n",
    "# segmentation_model_position = './data'\n",
    "# segmentation_model_name = 'train_model20000.model'\n",
    "\n",
    "# spp = spm.SentencePieceProcessor()\n",
    "# spp.Load(os.path.join(segmentation_model_position, segmentation_model_name))\n",
    "# def normalizeString(input_sentence):\n",
    "#     splitSentence = ' '.join(spp.EncodeAsPieces(input_sentence))\n",
    "#     return splitSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    seqs, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    for iseqs in range(len(seqs)):\n",
    "        tokens = seqs[iseqs]\n",
    "    \n",
    "        # indexes -> words\n",
    "        decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "        print(scores[iseqs],decoded_words)\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, searcher1, voc):\n",
    "    input_sentence = \"\"\n",
    "    while 1:\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input(\"> \")\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == \"q\" or input_sentence == \"quit\":\n",
    "                break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            print('\\n入力を分割すると',input_sentence+'\\n')\n",
    "            \n",
    "            # Evaluate sentence with seacher\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"EOS\" or x == \"PAD\" or x == \"SOS\")\n",
    "            ]\n",
    "            print(\"\\nBot(BeamSearch):\", \" \".join(output_words))\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Evaluate sentence with seacher1\n",
    "            output_words = evaluate(encoder, decoder, searcher1, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [\n",
    "                x for x in output_words if not (x == \"EOS\" or x == \"PAD\" or x == \"SOS\")\n",
    "            ]\n",
    "            print(\"\\nBot(GreedySearch):\", \" \".join(output_words)+\"\\n\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自分の入力で評価(会話)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  おはよう\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "入力を分割すると ▁おはよう\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-70f0e49a921f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-12c5989b01df>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, searcher1, voc)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Evaluate sentence with seacher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Format and print response sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             output_words[:] = [\n",
      "\u001b[0;32m<ipython-input-15-12c5989b01df>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, searcher, voc, sentence, max_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Decode sentence with searcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miseqs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37-mikami_saturn/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-86eeaaa26598>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, input_length, max_length)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mn_best_batch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mn_best_batch_score_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatchNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "#searcher = GreedySearchDecoder(encoder, decoder)\n",
    "searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "searcher１ = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, searcher1, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数文をランダムで入力して出力して評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力: 通学路\n",
      "入力を分割すると: 通学 路\n",
      "tensor(3.1944) ['SOS', 'いい', 'EOS', 'PAD', 'PAD']\n",
      "tensor(3.0795) ['SOS', 'そう', 'EOS', 'PAD', 'PAD']\n",
      "tensor(2.6557) ['SOS', 'これ', 'だ', 'EOS', 'PAD']\n",
      "tensor(2.5137) ['SOS', 'これ', 'は', 'EOS', 'PAD']\n",
      "tensor(2.4725) ['SOS', 'これ', 'EOS', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): これ\n",
      "正解: 絶品の大盛りと欲張り盛りです!\n",
      "\n",
      "\n",
      "入力: 間違えたならないことを願うねw\n",
      "入力を分割すると: 間違え た なら ない こと を 願う ね w\n",
      "tensor(2.1689) ['SOS', 'すまそ', 'w', 'w', 'w', 'w', 'EOS', 'PAD']\n",
      "tensor(2.0860) ['SOS', 'すまそ', 'w', 'w', 'w', 'EOS', 'PAD', 'PAD']\n",
      "tensor(2.0609) ['SOS', 'うん', 'w', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.9430) ['SOS', 'すまそ', 'w', 'w', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.5871) ['SOS', 'すまそ', 'w', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): すまそw\n",
      "正解: やれやれやれやれ\n",
      "\n",
      "\n",
      "入力: 模型w確かにいいですねw\n",
      "入力を分割すると: 模型 w 確か に いい です ね w\n",
      "tensor(1.6738) ['SOS', 'こっち', 'の', 'こと', 'に', 'なっ', 'てる', 'の', 'か', 'と', '思っ', 'て', 'ない', 'w', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.6613) ['SOS', 'こっち', 'の', 'こと', 'に', 'なっ', 'てる', 'の', 'か', 'と', '思い', 'まし', 'た', 'w', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.6583) ['SOS', 'こっち', 'の', 'こと', 'に', 'なっ', 'てる', 'の', 'か', 'と', '思っ', 'て', 'ます', 'w', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.6289) ['SOS', 'こっち', 'の', 'こと', 'に', 'なっ', 'てる', 'の', 'か', 'と', '思っ', 'て', 'ない', 'です', 'w', 'EOS', 'PAD']\n",
      "tensor(1.6281) ['SOS', 'こっち', 'の', 'こと', 'に', 'なっ', 'てる', 'の', 'か', 'と', '思っ', 'て', 'ない', '模様', 'w', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): こっちのことになってるのかと思ってない模様w\n",
      "正解: なんか笑い止まらんかったw\n",
      "\n",
      "\n",
      "入力: 寝ろょ\n",
      "入力を分割すると: 寝ろ ょ\n",
      "tensor(2.5257) ['SOS', '起き', 'て', '寝', 'た', 'EOS', 'PAD', 'PAD']\n",
      "tensor(2.5243) ['SOS', '起き', 'て', 'た', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(2.4458) ['SOS', '起き', 'て', '寝', 'て', 'た', 'EOS', 'PAD']\n",
      "tensor(2.3909) ['SOS', '起き', 'て', '寝', 'て', 'EOS', 'PAD', 'PAD']\n",
      "tensor(2.3874) ['SOS', '起き', 'て', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): 起きて\n",
      "正解: 寝みw\n",
      "\n",
      "\n",
      "入力: 私は1枚目がすこ\n",
      "入力を分割すると: 私 は 1枚 目 が すこ\n",
      "tensor(1.9601) ['SOS', '染め', 'てる', 'の', 'か', 'な', '?', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.9586) ['SOS', '染め', 'てる', 'の', 'か', 'と', '思い', 'ます', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.9567) ['SOS', '染め', 'てる', 'の', 'か', 'と', '思っ', 'た', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tensor(1.8413) ['SOS', '染め', 'てる', 'の', 'か', 'と', '思い', 'ます', 'ね', 'EOS', 'PAD', 'PAD']\n",
      "tensor(1.7961) ['SOS', '染め', 'てる', 'の', 'か', 'と', '思い', 'ます', 'ね', 'w', 'EOS', 'PAD']\n",
      "\n",
      "Bot(BeamSearch): 染めてるのかと思いますねw\n",
      "正解: 勝ち確では?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "num=5\n",
    "test_choice = [(random.choice(test_pairs)[0].replace(' ',''),random.choice(test_pairs)[1].replace(' ','')) for i in range(num)]\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "# Initialize search module\n",
    "#searcher = GreedySearchDecoder(encoder, decoder)\n",
    "searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "\n",
    "for input_pair in test_choice:\n",
    "    \n",
    "    input_sentence, output_sentence = input_pair\n",
    "    print(\"入力:\",input_sentence)\n",
    "    input_sentence = normalizeString(input_sentence)\n",
    "    print('入力を分割すると:',input_sentence)\n",
    "    \n",
    "\n",
    "    # Evaluate sentence with seacher\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    # Format and print response sentence\n",
    "    output_words[:] = [\n",
    "        x for x in output_words if not (x == \"EOS\" or x == \"PAD\" or x==\"SOS\")\n",
    "    ]\n",
    "    print(\"\\nBot(BeamSearch):\", \"\".join(output_words))\n",
    "    print(\"正解:\",output_sentence+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複数文を自動評価指標で評価、ここから下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    seqs, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    for iseqs in range(len(seqs)):\n",
    "        tokens = seqs[iseqs]\n",
    "    \n",
    "        # indexes -> words\n",
    "        decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "        #print(scores[iseqs],decoded_words)\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_extract = [random.choice(test_pairs) for i in range(5000)]\n",
    "\n",
    "\n",
    "# input_choices = [ele[0] for ele in test_extract]\n",
    "# output_choices = [ele[1].replace(' ','') for ele in test_extract]\n",
    "\n",
    "\n",
    "# eval_folder = 'mecab3min'\n",
    "# with open(eval_folder+'/ref.txt','w') as f:\n",
    "#     f.write('\\n'.join(output_choices))\n",
    "        \n",
    "\n",
    "# encoder.eval()\n",
    "# decoder.eval()\n",
    "# searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "# hyp_sentences = []\n",
    "# i=0\n",
    "# try:\n",
    "    \n",
    "\n",
    "#     for input_sentence in input_choices:\n",
    "#         input_sentence = normalizeString(input_sentence)\n",
    "#         output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "#         output_words[:] = [\n",
    "#             x for x in output_words if not (x == \"EOS\" or x == \"PAD\" or x==\"SOS\")\n",
    "#         ]\n",
    "#         hyp_sentences.append(\"\".join(output_words))\n",
    "\n",
    "#         i+=1\n",
    "#     with open(eval_folder+'/hyp.txt','w') as f:\n",
    "#         f.write('\\n'.join(hyp_sentences))\n",
    "# except Exception:\n",
    "#     print('error')\n",
    "#     print(i,hyp_sentences[-1])\n",
    "#     print(input_sentence)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu_1: 0.550000\n",
      "Bleu_2: 0.428174\n",
      "Bleu_3: 0.284043\n",
      "Bleu_4: 0.201143\n",
      "METEOR: 0.295797\n",
      "ROUGE_L: 0.522104\n",
      "CIDEr: 2.001651\n",
      "SkipThoughtsCosineSimilarity: 0.626149\n",
      "EmbeddingAverageCosineSimilarity: 0.866522\n",
      "EmbeddingAverageCosineSimilairty: 0.866522\n",
      "VectorExtremaCosineSimilarity: 0.558104\n",
      "GreedyMatchingScore: 0.779369\n"
     ]
    }
   ],
   "source": [
    "eval_folder = 'examples'\n",
    "from nlgeval import compute_metrics\n",
    "metrics_dict = compute_metrics(hypothesis=eval_folder+'/hyp.txt',\n",
    "                               references=[eval_folder+'/ref1.txt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['お仕事 ふぁいと です !', '卵 は こないだ 渡し たろ う !', 'そろそろ ねむり ます おやすみなさい です', 'この カオナシ の クセ 強い w', 'アレェ !? やめ た の ? てか 元 から やっ て ない の ?', 'わかる', '黒 パンツ でし た ね', 'バイツ', 'を 作成 し た よ', 'マヨ が 壊れ た']\n",
      "['あっ仕事は終わりましたw', 'あっそっかこれねすぐ食べちゃった', 'おやすみなさい。', 'これは?', 'やめたというか熱がねw', 'でもラクス派やねんw', '黒と言うか透明と言うかw', '新幹線できて', '貧乳に片思いって感じで草', '家の猫のまね']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "eval_folder = 'mecab3min'\n",
    "# test_extract = [random.choice(test_pairs) for i in range(5000)]\n",
    "\n",
    "\n",
    "# input_choices = [ele[0] for ele in test_extract]\n",
    "# output_choices = [ele[1].replace(' ','') for ele in test_extract]\n",
    "\n",
    "\n",
    "# with open(eval_folder+'/eval_input_output.pickle', 'wb') as f:\n",
    "#     pickle.dump((input_choices ,output_choices), f)\n",
    "\n",
    "with open(eval_folder+\"/eval_input_output.pickle\", \"rb\") as f:\n",
    "    \n",
    "    input_choices ,output_choices= pickle.load(f)\n",
    "    \n",
    "print(input_choices[:10])\n",
    "\n",
    "print(output_choices[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ファイル名注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Bleu_1, Bleu_2, Bleu_3, Bleu_4, METEOR, ROUGE_L, CIDEr, SkipThoughtCS, EmbeddingAverageCosineSimilarity, EmbeddingAverageCosineSimilairty, VectorExtremaCosineSimilarity, GreedyMatchingScore]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=list(metrics_dict.keys()))\n",
    "df.to_csv('mecab3min/to_csv_out5.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "63\n",
      "['210000_checkpoint.tar', '320000_checkpoint.tar', '420000_checkpoint.tar', '435000_checkpoint.tar', '390000_checkpoint.tar', '72000_checkpoint.tar', '385000_checkpoint.tar', '260000_checkpoint.tar', '485000_checkpoint.tar', '180000_checkpoint.tar', '490000_checkpoint.tar', '128000_checkpoint.tar', '450000_checkpoint.tar', '445000_checkpoint.tar', '350000_checkpoint.tar', '30000_checkpoint.tar', '225000_checkpoint.tar', '230000_checkpoint.tar', '124000_checkpoint.tar', '300000_checkpoint.tar', '415000_checkpoint.tar', '400000_checkpoint.tar', '52000_checkpoint.tar', '240000_checkpoint.tar', '280000_checkpoint.tar', '160000_checkpoint.tar', '465000_checkpoint.tar', '470000_checkpoint.tar', '370000_checkpoint.tar', '4000_checkpoint.tar', '250000_checkpoint.tar', '68000_checkpoint.tar', '360000_checkpoint.tar', '375000_checkpoint.tar', '15000_checkpoint.tar', '290000_checkpoint.tar', '170000_checkpoint.tar', '475000_checkpoint.tar', '460000_checkpoint.tar', '220000_checkpoint.tar', '100000_checkpoint.tar', '405000_checkpoint.tar', '410000_checkpoint.tar', '310000_checkpoint.tar', '270000_checkpoint.tar', '495000_checkpoint.tar', '190000_checkpoint.tar', '480000_checkpoint.tar', '56000_checkpoint.tar', '380000_checkpoint.tar', '395000_checkpoint.tar', '355000_checkpoint.tar', '340000_checkpoint.tar', '440000_checkpoint.tar', '150000_checkpoint.tar', '455000_checkpoint.tar', '500000_checkpoint.tar', '200000_checkpoint.tar', '148000_checkpoint.tar', '430000_checkpoint.tar', '120000_checkpoint.tar', '425000_checkpoint.tar', '330000_checkpoint.tar']\n",
      "\n",
      "63\n",
      "[4000, 15000, 30000, 52000, 56000, 68000, 72000, 100000, 120000, 124000, 128000, 148000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 225000, 230000, 240000, 250000, 260000, 270000, 280000, 290000, 300000, 310000, 320000, 330000, 340000, 350000, 355000, 360000, 370000, 375000, 380000, 385000, 390000, 395000, 400000, 405000, 410000, 415000, 420000, 425000, 430000, 435000, 440000, 445000, 450000, 455000, 460000, 465000, 470000, 475000, 480000, 485000, 490000, 495000, 500000]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "pathPos = './data/save/focal_loss_model/mecab_tagged.txt/4-4_256_0.2/'\n",
    "fileList = os.listdir(pathPos)\n",
    "print(len(fileList))\n",
    "files_file = [f for f in fileList if os.path.isfile(os.path.join(pathPos,f))]\n",
    "print(len(files_file))\n",
    "print(files_file)\n",
    "\n",
    "checkpoint_list = []\n",
    "\n",
    "for f in files_file:\n",
    "    numStr = re.search(r'\\d+',f)\n",
    "    numStr = int(numStr.group())\n",
    "    checkpoint_list.append(numStr)\n",
    "checkpoint_list.sort()\n",
    "print()\n",
    "print(len(checkpoint_list))\n",
    "print(checkpoint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mecab_tagged.txt\n",
      "test 4000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.017348\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.697018\n",
      "EmbeddingAverageCosineSimilarity: 0.582509\n",
      "EmbeddingAverageCosineSimilairty: 0.582509\n",
      "VectorExtremaCosineSimilarity: 0.508448\n",
      "GreedyMatchingScore: 0.850139\n",
      "test 15000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.025073\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.673066\n",
      "EmbeddingAverageCosineSimilarity: 0.529561\n",
      "EmbeddingAverageCosineSimilairty: 0.529561\n",
      "VectorExtremaCosineSimilarity: 0.443988\n",
      "GreedyMatchingScore: 0.832434\n",
      "test 30000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.025223\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.706080\n",
      "EmbeddingAverageCosineSimilarity: 0.577197\n",
      "EmbeddingAverageCosineSimilairty: 0.577197\n",
      "VectorExtremaCosineSimilarity: 0.493312\n",
      "GreedyMatchingScore: 0.851834\n",
      "test 52000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.024696\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.711493\n",
      "EmbeddingAverageCosineSimilarity: 0.588873\n",
      "EmbeddingAverageCosineSimilairty: 0.588873\n",
      "VectorExtremaCosineSimilarity: 0.506730\n",
      "GreedyMatchingScore: 0.854974\n",
      "test 56000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027175\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.700731\n",
      "EmbeddingAverageCosineSimilarity: 0.574973\n",
      "EmbeddingAverageCosineSimilairty: 0.574973\n",
      "VectorExtremaCosineSimilarity: 0.491203\n",
      "GreedyMatchingScore: 0.850243\n",
      "test 68000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.026716\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.707667\n",
      "EmbeddingAverageCosineSimilarity: 0.584031\n",
      "EmbeddingAverageCosineSimilairty: 0.584031\n",
      "VectorExtremaCosineSimilarity: 0.500477\n",
      "GreedyMatchingScore: 0.853773\n",
      "test 72000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.026439\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.708322\n",
      "EmbeddingAverageCosineSimilarity: 0.583480\n",
      "EmbeddingAverageCosineSimilairty: 0.583480\n",
      "VectorExtremaCosineSimilarity: 0.498073\n",
      "GreedyMatchingScore: 0.853769\n",
      "test 100000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.027236\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.692109\n",
      "EmbeddingAverageCosineSimilarity: 0.561909\n",
      "EmbeddingAverageCosineSimilairty: 0.561909\n",
      "VectorExtremaCosineSimilarity: 0.477322\n",
      "GreedyMatchingScore: 0.843831\n",
      "test 120000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.028144\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.692520\n",
      "EmbeddingAverageCosineSimilarity: 0.565411\n",
      "EmbeddingAverageCosineSimilairty: 0.565411\n",
      "VectorExtremaCosineSimilarity: 0.480762\n",
      "GreedyMatchingScore: 0.846160\n",
      "test 124000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.025187\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.701394\n",
      "EmbeddingAverageCosineSimilarity: 0.578659\n",
      "EmbeddingAverageCosineSimilairty: 0.578659\n",
      "VectorExtremaCosineSimilarity: 0.496687\n",
      "GreedyMatchingScore: 0.850207\n",
      "test 128000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027485\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.698010\n",
      "EmbeddingAverageCosineSimilarity: 0.570659\n",
      "EmbeddingAverageCosineSimilairty: 0.570659\n",
      "VectorExtremaCosineSimilarity: 0.486994\n",
      "GreedyMatchingScore: 0.846980\n",
      "test 148000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.025026\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.711774\n",
      "EmbeddingAverageCosineSimilarity: 0.589598\n",
      "EmbeddingAverageCosineSimilairty: 0.589598\n",
      "VectorExtremaCosineSimilarity: 0.508250\n",
      "GreedyMatchingScore: 0.854900\n",
      "test 150000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.026154\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.707206\n",
      "EmbeddingAverageCosineSimilarity: 0.583196\n",
      "EmbeddingAverageCosineSimilairty: 0.583196\n",
      "VectorExtremaCosineSimilarity: 0.500732\n",
      "GreedyMatchingScore: 0.852973\n",
      "test 160000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.025578\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.706055\n",
      "EmbeddingAverageCosineSimilarity: 0.580736\n",
      "EmbeddingAverageCosineSimilairty: 0.580736\n",
      "VectorExtremaCosineSimilarity: 0.499244\n",
      "GreedyMatchingScore: 0.850966\n",
      "test 170000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.027564\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.696986\n",
      "EmbeddingAverageCosineSimilarity: 0.569409\n",
      "EmbeddingAverageCosineSimilairty: 0.569409\n",
      "VectorExtremaCosineSimilarity: 0.486517\n",
      "GreedyMatchingScore: 0.847370\n",
      "test 180000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.024706\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.721600\n",
      "EmbeddingAverageCosineSimilarity: 0.601626\n",
      "EmbeddingAverageCosineSimilairty: 0.601626\n",
      "VectorExtremaCosineSimilarity: 0.520599\n",
      "GreedyMatchingScore: 0.858286\n",
      "test 190000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.026987\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.714236\n",
      "EmbeddingAverageCosineSimilarity: 0.592145\n",
      "EmbeddingAverageCosineSimilairty: 0.592145\n",
      "VectorExtremaCosineSimilarity: 0.510765\n",
      "GreedyMatchingScore: 0.855209\n",
      "test 200000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.026348\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.712082\n",
      "EmbeddingAverageCosineSimilarity: 0.588457\n",
      "EmbeddingAverageCosineSimilairty: 0.588457\n",
      "VectorExtremaCosineSimilarity: 0.504154\n",
      "GreedyMatchingScore: 0.853901\n",
      "test 210000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.028421\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.706504\n",
      "EmbeddingAverageCosineSimilarity: 0.583636\n",
      "EmbeddingAverageCosineSimilairty: 0.583636\n",
      "VectorExtremaCosineSimilarity: 0.501229\n",
      "GreedyMatchingScore: 0.852169\n",
      "test 220000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027555\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.705318\n",
      "EmbeddingAverageCosineSimilarity: 0.582882\n",
      "EmbeddingAverageCosineSimilairty: 0.582882\n",
      "VectorExtremaCosineSimilarity: 0.500865\n",
      "GreedyMatchingScore: 0.851789\n",
      "test 225000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029795\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.696562\n",
      "EmbeddingAverageCosineSimilarity: 0.567870\n",
      "EmbeddingAverageCosineSimilairty: 0.567870\n",
      "VectorExtremaCosineSimilarity: 0.481568\n",
      "GreedyMatchingScore: 0.847156\n",
      "test 230000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.028310\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.705393\n",
      "EmbeddingAverageCosineSimilarity: 0.578984\n",
      "EmbeddingAverageCosineSimilairty: 0.578984\n",
      "VectorExtremaCosineSimilarity: 0.494868\n",
      "GreedyMatchingScore: 0.850895\n",
      "test 240000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000400\n",
      "Bleu_2: 0.000020\n",
      "Bleu_3: 0.000007\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.026861\n",
      "ROUGE_L: 0.000400\n",
      "CIDEr: 0.001000\n",
      "SkipThoughtsCosineSimilarity: 0.703061\n",
      "EmbeddingAverageCosineSimilarity: 0.580660\n",
      "EmbeddingAverageCosineSimilairty: 0.580660\n",
      "VectorExtremaCosineSimilarity: 0.497575\n",
      "GreedyMatchingScore: 0.850431\n",
      "test 250000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.028208\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.702470\n",
      "EmbeddingAverageCosineSimilarity: 0.575081\n",
      "EmbeddingAverageCosineSimilairty: 0.575081\n",
      "VectorExtremaCosineSimilarity: 0.491448\n",
      "GreedyMatchingScore: 0.848163\n",
      "test 260000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027909\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.702044\n",
      "EmbeddingAverageCosineSimilarity: 0.574384\n",
      "EmbeddingAverageCosineSimilairty: 0.574384\n",
      "VectorExtremaCosineSimilarity: 0.489525\n",
      "GreedyMatchingScore: 0.848405\n",
      "test 270000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027278\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.712011\n",
      "EmbeddingAverageCosineSimilarity: 0.588351\n",
      "EmbeddingAverageCosineSimilairty: 0.588351\n",
      "VectorExtremaCosineSimilarity: 0.503821\n",
      "GreedyMatchingScore: 0.853527\n",
      "test 280000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.029179\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.711804\n",
      "EmbeddingAverageCosineSimilarity: 0.585803\n",
      "EmbeddingAverageCosineSimilairty: 0.585803\n",
      "VectorExtremaCosineSimilarity: 0.501753\n",
      "GreedyMatchingScore: 0.853239\n",
      "test 290000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027011\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.712488\n",
      "EmbeddingAverageCosineSimilarity: 0.590590\n",
      "EmbeddingAverageCosineSimilairty: 0.590590\n",
      "VectorExtremaCosineSimilarity: 0.509647\n",
      "GreedyMatchingScore: 0.854422\n",
      "test 300000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027962\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.713653\n",
      "EmbeddingAverageCosineSimilarity: 0.591635\n",
      "EmbeddingAverageCosineSimilairty: 0.591635\n",
      "VectorExtremaCosineSimilarity: 0.509213\n",
      "GreedyMatchingScore: 0.855283\n",
      "test 310000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.028316\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.701278\n",
      "EmbeddingAverageCosineSimilarity: 0.575656\n",
      "EmbeddingAverageCosineSimilairty: 0.575656\n",
      "VectorExtremaCosineSimilarity: 0.493590\n",
      "GreedyMatchingScore: 0.847719\n",
      "test 320000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.028705\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.708594\n",
      "EmbeddingAverageCosineSimilarity: 0.584437\n",
      "EmbeddingAverageCosineSimilairty: 0.584437\n",
      "VectorExtremaCosineSimilarity: 0.499466\n",
      "GreedyMatchingScore: 0.853274\n",
      "test 330000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030346\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.696128\n",
      "EmbeddingAverageCosineSimilarity: 0.570103\n",
      "EmbeddingAverageCosineSimilairty: 0.570103\n",
      "VectorExtremaCosineSimilarity: 0.486242\n",
      "GreedyMatchingScore: 0.847896\n",
      "test 340000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029147\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.716228\n",
      "EmbeddingAverageCosineSimilarity: 0.594754\n",
      "EmbeddingAverageCosineSimilairty: 0.594754\n",
      "VectorExtremaCosineSimilarity: 0.512066\n",
      "GreedyMatchingScore: 0.855613\n",
      "test 350000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030459\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.702026\n",
      "EmbeddingAverageCosineSimilarity: 0.574618\n",
      "EmbeddingAverageCosineSimilairty: 0.574618\n",
      "VectorExtremaCosineSimilarity: 0.490226\n",
      "GreedyMatchingScore: 0.848031\n",
      "test 355000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.031372\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.711935\n",
      "EmbeddingAverageCosineSimilarity: 0.589695\n",
      "EmbeddingAverageCosineSimilairty: 0.589695\n",
      "VectorExtremaCosineSimilarity: 0.505907\n",
      "GreedyMatchingScore: 0.855984\n",
      "test 360000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030717\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.699247\n",
      "EmbeddingAverageCosineSimilarity: 0.571981\n",
      "EmbeddingAverageCosineSimilairty: 0.571981\n",
      "VectorExtremaCosineSimilarity: 0.486746\n",
      "GreedyMatchingScore: 0.847279\n",
      "test 370000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.028748\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.712395\n",
      "EmbeddingAverageCosineSimilarity: 0.593310\n",
      "EmbeddingAverageCosineSimilairty: 0.593310\n",
      "VectorExtremaCosineSimilarity: 0.514171\n",
      "GreedyMatchingScore: 0.855286\n",
      "test 375000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.028632\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.702407\n",
      "EmbeddingAverageCosineSimilarity: 0.576892\n",
      "EmbeddingAverageCosineSimilairty: 0.576892\n",
      "VectorExtremaCosineSimilarity: 0.494167\n",
      "GreedyMatchingScore: 0.848858\n",
      "test 380000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029184\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.699866\n",
      "EmbeddingAverageCosineSimilarity: 0.573423\n",
      "EmbeddingAverageCosineSimilairty: 0.573423\n",
      "VectorExtremaCosineSimilarity: 0.490427\n",
      "GreedyMatchingScore: 0.847534\n",
      "test 385000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030114\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.702229\n",
      "EmbeddingAverageCosineSimilarity: 0.576691\n",
      "EmbeddingAverageCosineSimilairty: 0.576691\n",
      "VectorExtremaCosineSimilarity: 0.492881\n",
      "GreedyMatchingScore: 0.849644\n",
      "test 390000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030740\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.692171\n",
      "EmbeddingAverageCosineSimilarity: 0.561874\n",
      "EmbeddingAverageCosineSimilairty: 0.561874\n",
      "VectorExtremaCosineSimilarity: 0.474568\n",
      "GreedyMatchingScore: 0.843442\n",
      "test 395000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.028658\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.711464\n",
      "EmbeddingAverageCosineSimilarity: 0.591202\n",
      "EmbeddingAverageCosineSimilairty: 0.591202\n",
      "VectorExtremaCosineSimilarity: 0.509061\n",
      "GreedyMatchingScore: 0.854916\n",
      "test 400000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.030460\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.700082\n",
      "EmbeddingAverageCosineSimilarity: 0.571730\n",
      "EmbeddingAverageCosineSimilairty: 0.571730\n",
      "VectorExtremaCosineSimilarity: 0.487078\n",
      "GreedyMatchingScore: 0.846949\n",
      "test 405000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029365\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.705475\n",
      "EmbeddingAverageCosineSimilarity: 0.582297\n",
      "EmbeddingAverageCosineSimilairty: 0.582297\n",
      "VectorExtremaCosineSimilarity: 0.497993\n",
      "GreedyMatchingScore: 0.851739\n",
      "test 410000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030644\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.704911\n",
      "EmbeddingAverageCosineSimilarity: 0.580339\n",
      "EmbeddingAverageCosineSimilairty: 0.580339\n",
      "VectorExtremaCosineSimilarity: 0.494709\n",
      "GreedyMatchingScore: 0.850537\n",
      "test 415000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029436\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.712242\n",
      "EmbeddingAverageCosineSimilarity: 0.589070\n",
      "EmbeddingAverageCosineSimilairty: 0.589070\n",
      "VectorExtremaCosineSimilarity: 0.504097\n",
      "GreedyMatchingScore: 0.854681\n",
      "test 420000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029242\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.708135\n",
      "EmbeddingAverageCosineSimilarity: 0.585885\n",
      "EmbeddingAverageCosineSimilairty: 0.585885\n",
      "VectorExtremaCosineSimilarity: 0.502412\n",
      "GreedyMatchingScore: 0.853029\n",
      "test 425000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027671\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.708766\n",
      "EmbeddingAverageCosineSimilarity: 0.586318\n",
      "EmbeddingAverageCosineSimilairty: 0.586318\n",
      "VectorExtremaCosineSimilarity: 0.502446\n",
      "GreedyMatchingScore: 0.853130\n",
      "test 430000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029586\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.707329\n",
      "EmbeddingAverageCosineSimilarity: 0.584107\n",
      "EmbeddingAverageCosineSimilairty: 0.584107\n",
      "VectorExtremaCosineSimilarity: 0.501170\n",
      "GreedyMatchingScore: 0.852473\n",
      "test 435000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027878\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.699693\n",
      "EmbeddingAverageCosineSimilarity: 0.576363\n",
      "EmbeddingAverageCosineSimilairty: 0.576363\n",
      "VectorExtremaCosineSimilarity: 0.492817\n",
      "GreedyMatchingScore: 0.848656\n",
      "test 440000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.031482\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.699115\n",
      "EmbeddingAverageCosineSimilarity: 0.572608\n",
      "EmbeddingAverageCosineSimilairty: 0.572608\n",
      "VectorExtremaCosineSimilarity: 0.487056\n",
      "GreedyMatchingScore: 0.847877\n",
      "test 445000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.031403\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.710766\n",
      "EmbeddingAverageCosineSimilarity: 0.587128\n",
      "EmbeddingAverageCosineSimilairty: 0.587128\n",
      "VectorExtremaCosineSimilarity: 0.502693\n",
      "GreedyMatchingScore: 0.854752\n",
      "test 450000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029957\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.703022\n",
      "EmbeddingAverageCosineSimilarity: 0.578096\n",
      "EmbeddingAverageCosineSimilairty: 0.578096\n",
      "VectorExtremaCosineSimilarity: 0.492894\n",
      "GreedyMatchingScore: 0.850895\n",
      "test 455000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.031619\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.691106\n",
      "EmbeddingAverageCosineSimilarity: 0.563593\n",
      "EmbeddingAverageCosineSimilairty: 0.563593\n",
      "VectorExtremaCosineSimilarity: 0.478577\n",
      "GreedyMatchingScore: 0.844882\n",
      "test 460000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029798\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.698064\n",
      "EmbeddingAverageCosineSimilarity: 0.572230\n",
      "EmbeddingAverageCosineSimilairty: 0.572230\n",
      "VectorExtremaCosineSimilarity: 0.487594\n",
      "GreedyMatchingScore: 0.848686\n",
      "test 465000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.029132\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.710777\n",
      "EmbeddingAverageCosineSimilarity: 0.588753\n",
      "EmbeddingAverageCosineSimilairty: 0.588753\n",
      "VectorExtremaCosineSimilarity: 0.504714\n",
      "GreedyMatchingScore: 0.855210\n",
      "test 470000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.028729\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.707807\n",
      "EmbeddingAverageCosineSimilarity: 0.585668\n",
      "EmbeddingAverageCosineSimilairty: 0.585668\n",
      "VectorExtremaCosineSimilarity: 0.499951\n",
      "GreedyMatchingScore: 0.851476\n",
      "test 475000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030984\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.708267\n",
      "EmbeddingAverageCosineSimilarity: 0.584037\n",
      "EmbeddingAverageCosineSimilairty: 0.584037\n",
      "VectorExtremaCosineSimilarity: 0.499132\n",
      "GreedyMatchingScore: 0.853000\n",
      "test 480000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000200\n",
      "Bleu_2: 0.000014\n",
      "Bleu_3: 0.000006\n",
      "Bleu_4: 0.000004\n",
      "METEOR: 0.030318\n",
      "ROUGE_L: 0.000200\n",
      "CIDEr: 0.000500\n",
      "SkipThoughtsCosineSimilarity: 0.720385\n",
      "EmbeddingAverageCosineSimilarity: 0.597791\n",
      "EmbeddingAverageCosineSimilairty: 0.597791\n",
      "VectorExtremaCosineSimilarity: 0.514385\n",
      "GreedyMatchingScore: 0.857636\n",
      "test 485000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.027729\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.714784\n",
      "EmbeddingAverageCosineSimilarity: 0.592718\n",
      "EmbeddingAverageCosineSimilairty: 0.592718\n",
      "VectorExtremaCosineSimilarity: 0.509503\n",
      "GreedyMatchingScore: 0.855427\n",
      "test 490000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030814\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.706546\n",
      "EmbeddingAverageCosineSimilarity: 0.581677\n",
      "EmbeddingAverageCosineSimilairty: 0.581677\n",
      "VectorExtremaCosineSimilarity: 0.497784\n",
      "GreedyMatchingScore: 0.851758\n",
      "test 495000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n",
      "Bleu_1: 0.000000\n",
      "Bleu_2: 0.000000\n",
      "Bleu_3: 0.000000\n",
      "Bleu_4: 0.000000\n",
      "METEOR: 0.030814\n",
      "ROUGE_L: 0.000000\n",
      "CIDEr: 0.000000\n",
      "SkipThoughtsCosineSimilarity: 0.697586\n",
      "EmbeddingAverageCosineSimilarity: 0.569458\n",
      "EmbeddingAverageCosineSimilairty: 0.569458\n",
      "VectorExtremaCosineSimilarity: 0.484577\n",
      "GreedyMatchingScore: 0.847494\n",
      "test 500000 iter\n",
      "load save file\n",
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "from nlgeval import compute_metrics\n",
    "import pandas as pd\n",
    "\n",
    "print(corpus_name)\n",
    "# Configure models\n",
    "model_name = \"focal_loss_model\"\n",
    "attn_model = 'dot'\n",
    "# attn_model = 'general'\n",
    "# attn_model = \"concat\"\n",
    "hidden_size = 256\n",
    "encoder_n_layers = 4\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.1\n",
    "batch_size = 512\n",
    "teacher_forcing_ratio=0.2\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "\n",
    "#1000,2000,3000,4000,5000,6000,7000,8000,9000,15000,30000,52000,56000,60000,64000,68000,72000,96000,100000,102000,104000,120000,124000,128000,129000,148000,150000,151000,152000,153000,154000,155000,156000,157000,158000\n",
    "#159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000\n",
    "#174000,175000,176000,177000,178000,179000,180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000\n",
    "# checkpoint_list = [4000,15000,30000,52000,56000,68000,72000,100000,104000,120000,124000,128000,148000,150000,151000,152000,153000,155000,156000,157000,158000,\n",
    "#                    159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000,174000,175000,176000,178000,179000,\n",
    "#                    180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000]\n",
    "\n",
    "for checkpoint_iter in checkpoint_list:\n",
    "    \n",
    "    print('test',checkpoint_iter,'iter')\n",
    "\n",
    "    loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                               '{}-{}_{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size, teacher_forcing_ratio),\n",
    "                               '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "    # Load model if a loadFilename is provided\n",
    "    if loadFilename:\n",
    "        print(\"load save file\")\n",
    "        # If loading on same machine the model was trained on\n",
    "        checkpoint = torch.load(loadFilename)\n",
    "        # If loading a model trained on GPU to CPU\n",
    "        # checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "        encoder_sd = checkpoint[\"en\"]\n",
    "        decoder_sd = checkpoint[\"de\"]\n",
    "        encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "        decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "        embedding_sd = checkpoint[\"embedding\"]\n",
    "        voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "\n",
    "\n",
    "    print(\"Building encoder and decoder ...\")\n",
    "    # Initialize word embeddings\n",
    "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "    if loadFilename:\n",
    "        embedding.load_state_dict(embedding_sd)\n",
    "    # Initialize encoder & decoder models\n",
    "    encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "    decoder = AttnDecoderRNN(\n",
    "        attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout\n",
    "    )\n",
    "    if loadFilename:\n",
    "        encoder.load_state_dict(encoder_sd)\n",
    "        decoder.load_state_dict(decoder_sd)\n",
    "    # Use appropriate device\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "    print(\"Models built and ready to go!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    searcher = BeamSearchDecoder(encoder, decoder, 5, 5)\n",
    "    hyp_sentences = []\n",
    "    \n",
    "    \n",
    "    for input_sentence in input_choices:\n",
    "        input_sentence = normalizeString(input_sentence)\n",
    "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "        output_words[:] = [\n",
    "            x for x in output_words if not (x == \"EOS\" or x == \"PAD\" or x==\"SOS\")\n",
    "        ]\n",
    "        hyp_sentences.append(\"\".join(output_words))\n",
    "\n",
    "        \n",
    "    with open(eval_folder+'/hyp.txt','w') as f:\n",
    "        f.write('\\n'.join(hyp_sentences))\n",
    "        \n",
    "        \n",
    "    eval_folder = 'mecab3min'\n",
    "    metrics_dict = compute_metrics(hypothesis=eval_folder+'/hyp.txt',\n",
    "                                   references=[eval_folder+'/ref.txt'])\n",
    "    \n",
    "    df = pd.read_csv('mecab3min/to_csv_out5.csv', index_col=0)\n",
    "    nums = metrics_dict.values()\n",
    "    df.loc[checkpoint_iter] = list(map(round,nums,[6]*len(nums)))\n",
    "    df.to_csv('mecab3min/to_csv_out5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bleu_1: 0.003000\n",
    "Bleu_2: 0.000055\n",
    "Bleu_3: 0.000014\n",
    "Bleu_4: 0.000007\n",
    "METEOR: 0.036579\n",
    "ROUGE_L: 0.003000\n",
    "CIDEr: 0.007500\n",
    "SkipThoughtsCosineSimilarity: 0.736541\n",
    "EmbeddingAverageCosineSimilarity: 0.633790\n",
    "EmbeddingAverageCosineSimilairty: 0.633790\n",
    "VectorExtremaCosineSimilarity: 0.562120\n",
    "GreedyMatchingScore: 0.868408"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-mikami_saturn",
   "language": "python",
   "name": "py37-mikami_saturn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
